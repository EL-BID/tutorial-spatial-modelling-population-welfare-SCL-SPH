[
["index.html", "High-resolution spatial modelling of population welfare in El Salvador, a coding tutorial Section 1 Introduction", " High-resolution spatial modelling of population welfare in El Salvador, a coding tutorial Xavier Vollenweider, Claudio Bosco, Roberto Sánchez A, Luis Tejerina Section 1 Introduction Among the United Nations Sustainable Development Goals (SDGs), Goal 1 is to end poverty in all its forms everywhere by 2030, and Goal 4 and its Target 4.6 aim to ensure that all youth and a substantial proportion of adults, both men and women, achieve literacy and numeracy. The specification of achieving Goal 1 “everywhere” means that no one should be left behind. Therefore, national statistics need to include subnational-level populations and local heterogeneities to ensure representative monitoring and optimized intervention planning. Mapping the geographic distribution of populations in great detail—including aligning their characteristics with the goals’ indicators—is accordingly a central tool for meeting the SDGs. National household surveys are typically representative of the regional level (administrative level 1), but household conditions may vary at a much finer scale. For instance, the poverty levels within a town and rural areas in the same region can differ markedly. Similarly, poverty levels between rural towns and neighboring hamlets can exhibit large differences, while local economic activities (e.g., the presence or absence of a manufacturer) can largely determine SDG outcomes. It is therefore important to have more detailed, high-resolution information to support the efficient allocation of resources across territories and the monitoring of SDG indicators. Conventional approaches to producing high-resolution development indicators rely on small area estimation (SAE) methods that integrate household-survey and census data to estimate the proportion of households in poverty. Household surveys, conducted every one to five years, have been improved through the introduction of geolocated survey clusters that provide more fine-grained spatial data using GPS. In El Salvador, the national household survey Encuesta de Hogares de Propositos Multiples (EHPM) is conducted annually. Censuses, on the other hand, are typically undertaken irregularly, sometimes every 10 years or longer in many low-income countries. In El Salvador, the last census was conducted in 2007. The reliance on census data may weaken the reliability of SAE estimates, preventing the capability for ongoing monitoring of SDG indicators. Georeferenced national household survey data provide an opportunity to achieve more spatially detailed, accurate, and regular estimates of poverty distribution and other SDG indicators. To further improve these estimates, novel sources of spatial data are increasingly used. For example, remote sensing (RS) and geographic information system data comprise continually collected information—such as on rainfall, temperature, and vegetation—which are related to agricultural productivity, or light at night and distance to roads and cities which are related to access to markets and information and local economic dynamism. These additional data, once combined with household surveys, can help the production of poverty maps at high spatial resolution. Spatial interpolation approaches consist of overlapping such data with more traditional sources such as survey-based data to produce regularly updatable high-resolution maps of development indicators. In this report, we use spatial interpolation methods by integrating household-survey cluster data with geospatial covariates to produce high-resolution poverty, income, and literacy maps for El Salvador. This report gives a coding tutorial for creating high-resolution maps of SDG indicators. It outlines the relevant data and how they can be processed and analyzed. The emphasis is on providing reproducible codes and examples, and all the RS data are open source. The codes to go from the raw data to the high-resolution maps are presented in detail. This report is written with the Markdown (Allaire et al. 2019) and bookdown (Xie 2016) packages. The tutorial assumes that the reader is familiar with the open-source statistical computing environment R. For an introductory book on using R, refer to https://cengel.github.io/R-intro/. For an introductory book on using spatial data with R, refer to https://cengel.github.io/R-spatial/. The tutorial is structured as follows. Chapter 2 provides an overview of the analysis. Chapter 3 introduces the reader to data exploration and unsupervised predictors selection. In Chapter 4, a simple Bayesian model is fitted. Chapter 5 instructs how to do covariate selection using the jackknife approach, shows the results of the selected models, and produces a map of a socioeconomic variable. Chapter 6 concludes. Appendix 1 provides a short primer on INLA, an R package. Appendix 2 provides the codes required to preprocess the RS data, starting from the raw raster and vector layers up through the data in row and columns format used to fit the models. References "],
["overview.html", "Section 2 Overview 2.1 The First Look at the Data", " Section 2 Overview This analysis aims to create a high-resolution map of poverty, income, and literacy for El Salvador. The data on these three development indicators come from the 2017 EHPM household survey. While the survey data are available for only 1,664 segmentos, the smallest (local) administrative units, the goal is to provide a map of the three development indicators for all 12,435 segmentos in the country. This is made possible by building a statistical model exploiting the relationship between RS data (e.g., lights at night measured from a satellite, precipitation), the EHPM survey data, and the spatial correlation between SDG outcomes across space. Once the relationship between the EHPM survey data and the RS data is modeled in locations where EHPM survey data are available, RS data, which are available for the entire country, are used to predict SDG outcomes across the entire country. Furthermore, as SDG outcomes of two segmentos are more likely to be similar if both segmentos are neighbors than if they are far away from each other, the accuracy of the spatial prediction can be enhanced by explicitly into account the spatial distances and spatial relationships between the segmentos. The interpolation technique we use in this report is based on Bayesian geospatial methods implemented in the open-source statistical computing environment R (R Core Team 2018a). The core of the modeling method is implemented in the R package INLA (Rue, Martino, and Chopin 2009; Lindgren, Rue, and Lindström 2011; Martins et al. 2013; Lindgren and Rue 2015). Several other R packages are used in this tutorial. The main ones are dplyr (Wickham et al. 2019) for data handling, plotly (Sievert 2018) for interactive plots, leaflet (Cheng, Karambelkar, and Xie 2018) for interactive maps, raster (R. J. Hijmans 2019a) for raster data, rgdal (Bivand, Keitt, and Rowlingson 2019) for vector files, sp (Bivand, Pebesma, and Gomez-Rubio 2013) for coordinates systems adjustments, energy (Rizzo and Szekely 2018) for distance correlation computations, and parallel (R Core Team 2018b) for the covariates selection process. These are the five main steps of the analysis: Data preprocessing Covariate preselection Model fitting Diagnostic checks Out-of-sample spatial interpolation to create high-resolution maps A brief introduction to the INLA method is provided in Section 8. Three books are recommended for those who are interested in more information on the INLA approach: For readers with only a minimal quantitative background: Beginner’s Guide to Spatial, Temporal, and Spatial-Temporal Ecological Data Analysis with R-INLA (Zuur, Ieno, and Saveliev 2017) For those willing to delve deeper into the methodology behind INLA: Spatial and Spatio-Temporal Bayesian Models with R-INLA (Blangiardo and Cameletti 2015) For those interested in an application for public health: Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny(Moraga 2019) Bakka et al. (Bakka et al. 2018) provide a good overview of spatial modeling with INLA, and Steele et al. (Steele et al. 2017) provide a great example of the use of INLA for poverty mapping. For information about the R-INLA package, please refer to the R-INLA project website. 2.1 The First Look at the Data A total of 20,609 households were interviewed in the 2017 EHPM. The EHPM data are collected in 1,664 of El Salvador’s 12,435 segmentos. The number of households interviewed per segmento is mapped in Figure 2.1. Here are the required steps to create the map: Load the required packages: rgdal, dplyr and leaflet. Load the Segmentos shapefile with rgdal and the list of households per segmento. Count the number of households per segmento with dplyr and identify the segmentos where at least one survey recipient exists. Map the results with leaflet. To begin, we load the packages and initial data with rgdal. # Load the packages library(rgdal) library(dplyr) library(leaflet) #source(&quot;../utils.R&quot;) # Modify the directory path “dir_data” to where you stored the data root_dir=&quot;~/&quot; project_dir=&quot;data/&quot; dir_data=paste0(root_dir,project_dir) # Load the segmento shapefile map (simplified for faster rendering) segmentos_to_map=readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Segmentos_simplified.shp&quot;)) # Load the households list surveyed in the EHPM per segmento id_segmento_2017=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;), sheet = 2) We then compute the number of participants per segmento with dplyr. id_segmento_2017_df=id_segmento_2017%&gt;% rename(SEG_ID=seg_id)%&gt;% group_by(SEG_ID)%&gt;% # Group data per segmento summarize(ehpm_2017_d=1, # 1 if there is a household in the segmento ehpm_2017_n=n()) # # Count the number of households per segmento with the function n() ## `summarise()` ungrouping output (override with `.groups` argument) Before merging the results with the shapefile, we change the identifier of segmentos in the shapefile, “SEG_ID”, into character format. This is to make sure the match between the SEG_ID stored as a character in the household list id_segmento_2017_df and the shapefile map segmentos is correct. segmentos_to_map@data=segmentos_to_map@data%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% # Turn SEG_ID into character format left_join(id_segmento_2017_df, # Merge the household list with the shapefile data by=&quot;SEG_ID&quot;) We can now map all the segmentos where the EHPM data have been collected with the package leaflet. Note that when running the next command, rendering the 12,435 shapes might take a while. leaflet(segmentos_to_map) %&gt;% # Leaflet is used to render the map addProviderTiles(providers$CartoDB.Positron)%&gt;% addPolygons(color = &quot;#444444&quot;, # Color of the line of the border of each segmento weight = 1, # Thickness of the line of the border of each segmento smoothFactor = 1, # Simplify the shape to speed rendering opacity = 0, # Opacity of the segmento border lines fillOpacity = 1, # Opacity of the segmento areas fillColor = ~colorQuantile(&quot;Greens&quot;, # Define the color ramps of polygons ehpm_2017_n,na.color = &quot;transparent&quot;)(ehpm_2017_n)) Figure 2.1: Number of Participants Surveyed per Segmento Our modeling exercise obtains estimates of income, literacy, and poverty for the segmentos where no EHPM data is available, which are the empty values in Figure 2.1. References "],
["explo.html", "Section 3 Data exploration and preselection of covariates 3.1 Outcome Variables: Distribution and Outliers 3.2 Unsupervised Dimension Reduction Based on Distance Correlation 3.3 Correlations between the Predictors and the Outcome Variables", " Section 3 Data exploration and preselection of covariates The data used in the model can be separated into two main categories: The outcome variables, which are the three development indicators (income, poverty, and literacy) The covariates, which are the set of predictors derived in Section 9 that will be used to predict the outcome variables in segmentos not sampled in the EHPM survey. When exploring the outcome data, the aim is to identify possible issues such as a lack of variation or the presence of outliers. The exploration also helps in specifying the likelihood function for the outcome data (e.g., Gaussian, gamma, beta, etc.). For the covariates, we start by reducing their number to limit the effect of multicollinearity, which is the presence of highly correlated covariates in the model. Reducing the number of covariates will also reduce the risk of retaining covariates because of chance correlation in a next stepwise covariates selection process as shown in section 5. Lastly, we will investigate the correlation of the preselected covariates with the outcome variables. 3.1 Outcome Variables: Distribution and Outliers 3.1.1 Income We start by exploring the distribution of the median income per segmento. It is named ingpe in the dataset: Figure 3.1: Median income per segmento As is common with income distributions, this distribution is right skewed: the median income is between USD 95 and 105 in most segmentos and reaches values above USD 200 in a few of them. An option is to log-transform the data to make it Gaussian.1 Another option is to model income with a gamma likelihood function, which accounts for the asymmetry of the income distribution. Figure 3.2: Median Income Map Figure 3.2 shows that all the segmentos with a median income above USD 265 (blue markers) are clustered together in urban centers, while the four with an income above USD 700 (gold markers) are located in Antiguo Cuscatlan, one of the country’s wealthiest neighborhoods where several embassies are located. The national statistical office of El Salvador (DIGESTYC) classifies segmentos into rural and urban. Figure 3.3 shows the log-transformed distribution of median income for rural and urban segmentos separately. Figure 3.3: Rural-Urban Median Income Distribution Figure 3.3 shows that log transformation appears to be successful at making the distribution Gaussian. As seen in Figure 3.2, the median income is higher among urban than rural segmentos. Furthermore, the main difference between the rural and urban income distributions is a location shift: the variance or skewness of the income distribution does not appear to vary significantly between rural and urban segmentos. Figure 3.3 suggests that a single model could be used provided that a binary covariate identifying rural and urban segmentos is used in the model (instead of running separate models for rural and urban segmentos). In summary, this first exploration of the median income outcome data suggests that one could use either a gamma or a Gaussian likelihood function. In the latter case, income should be log-transformed. Furthermore, there are clear spatial patterns whereby urban areas have higher income levels than rural ones and segmentos that are close to each other tend to have similar income levels. Explicitly modeling spatial dependency is hence likely to improve the model’s goodness of fit. Lastly, we do not see any major difference between the distributions of income of rural and urban segmentos except for a location shift (all values shifted to the right among urban segmentos). A single model run both on rural and urban segmentos appears appropriate provided that a binary covariate identifying the rural or urban category is used. 3.1.2 Literacy The variable literacy_rate provides the proportion of survey participants 16 years or older who can read and write. Its domain is bounded between 0 percent to 100 percent. Figure 3.4: Rural-Urban Literacy Rates The distribution of literacy in rural and urban segmentos appears quite different. Of the 950 urban segmentos, 181 have a 100 percent literacy rate among adult EHPM respondents. In contrast, no such spike is observed in the rural areas. This finding suggests that literacy could be modeled as a two-step process for urban areas: (1) the presence/absence of illiteracy is modeled, and (2) the proportion of illiteracy is modeled. In rural areas, only the second step would be necessary. Figure next figure shows the map of literacy for the EHPM segmentos. Figure 3.5: Literacy Rate Map There is a clear spatial pattern: literacy rates are highest in urban areas, particularly around the capital city. Areas in the northwest (the departments of San Miguel, Morazan, and La Union) have low literacy levels. In summary, modeling literacy will be more complex than modeling income. First, a zero-inflated model might be appropriate, particularly for urban segmentos. Second, the distribution of literacy appears to vary between rural and urban segmentos, and not only in terms of the location. Therefore, separate or mixed models for rural and urban segmentos might be more appropriate. In both cases, a beta likelihood function might be appropriate, as literacy rates are bounded between 0 and 100 percent. Lastly, there is a clear spatial pattern suggesting that explicitly taking the spatial dependence structure into account will increase the model’s goodness of fit. 3.1.3 Moderate Poverty The variable pobreza_mod provides the proportion of survey participants 16 years or older living in moderate poverty. Its domain is bounded between 0 to 1. Figure 3.6: Rural-Urban Moderate Poverty Rate Figure 3.6 shows that more than 100 urban segmentos have no adult EHPM respondents living in poverty but no rural segmento does. Except for this important difference, the distribution of poverty of rural and urban segmentos does not appear to differ much. Figure 3.7 shows the map of moderate poverty for the EHPM segmentos. Figure 3.7: Moderate Poverty Map Interestingly, the picture is much less clear when mapping moderate poverty than income. However, the general urban-rural divide appears to hold. As poverty is measured as a proportion, we will use a beta likelihood. Given the high number of segmentos with zero respondents living in poverty, we will test a zero-inflated beta likelihood. Lastly, as the excess number of zeros is present only among urban segmentos, we will compare results of the two separate models for rural and urban segmentos with the results of when only one model is run. 3.2 Unsupervised Dimension Reduction Based on Distance Correlation We now reduce the number of covariates by removing highly correlated covariates. Before starting the process, we standardize the covariates to limit the effect of the various scales and measurement units of each covariate on the computation of the correlation. 3.2.1 Obtaining a Data Frame with Standardized Covariates We start by storing the candidate covariates in a data frame. # Join with shapefile #### segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) # Identify EHPM segmentos #### segmento_sh_data_model=segmento_sh_data ehpm_index=which(is.na(segmento_sh_data_model$n_obs)==F) ehpm17_predictors_survey=segmento_sh_data_model@data[ehpm_index,] # Remove outcome variables and administrative information #### # covariates=ehpm17_predictors_survey%&gt;% # select(-c(SEG_ID,municauto,n_obs,pobreza_extrema,pobreza_mod,literacy_rate,ingpe,DEPTO, # COD_DEP,MPIO,COD_MUN,CANTON,COD_CAN,COD_ZON_CE,COD_SEC_CE,COD_SEG_CE,LZCODE, # ndvi_17_perc_median,ndvi_17_perc_sd,ndvi_17_perc_sum, # chirps_ev_perc_norm,chirps_ev_2017, chirps_ev_perc_norm, # dist2allpubserv,lights_sd,gpw_es_TOT,gpw_es_avg_dens,dist2water_r, # soil_prod_state,soil_carb,soil_luc,soil_prod_prf,soil_prod_state,soil_prod_trj,settlements)) covariates=ehpm17_predictors_survey%&gt;% select(-c(SEG_ID,municauto,n_obs,pobreza_extrema,pobreza_mod,literacy_rate,ingpe,DEPTO, COD_DEP,MPIO,COD_MUN,CANTON,COD_CAN,COD_ZON_CE,COD_SEC_CE,COD_SEG_CE,LZCODE,OBJECTID, SHAPE_Leng,AREA_KM2,dens_bus,dens_luxury)) # Create binary variables from the livelihood zone categories covariates=covariates%&gt;% mutate(AREA_ID=ifelse(AREA_ID==&quot;U&quot;,1,0), LZ_BasicGrainLaborZone=ifelse(LZNAMEEN==&quot;Basic Grain and Labor Zone&quot;, 1,0), LZ_CentralIndusServ=ifelse(LZNAMEEN==&quot;Central Free-Trade, Services and Industrial Labor Zone&quot;, 1,0), LZ_CoffeeAgroIndus=ifelse(LZNAMEEN==&quot;Coffee, Agro-Industrial and Labor Zone&quot;, 1,0), LZ_LvstckGrainRem=ifelse(LZNAMEEN==&quot;Eastern Basic Grain, Labor, Livestock and Remittance Zone&quot;, 1,0), LZ_FishAgriTourism=ifelse(LZNAMEEN==&quot;Fishing, Aquaculture and Tourism Zone&quot;, 1,0))%&gt;% select(-LZNAMEEN) We write a function myStd to standardize the covariates by subtracting the mean from each observation and dividing them by the standard deviation. myStd=function(x){ # Standardize a given covariate x with the myStd function mu_x=mean(x,na.rm = T) sd_x=sd(x,na.rm = T) x_std=(x-mu_x)/sd_x return(x_std) } We then apply myStd to each covariate. covariates_std=apply(covariates,2,myStd) # We then apply the function myStd to each covariate The data frame covariates_std contains the standardized covariates. Before removing highly correlated covariates, we compute the variance inflation factor (VIF) to determine the presence of multicollinearity. # Calculate the VIF before selecting covariates #### covariates_std_df=as.data.frame(covariates_std) data_model=covariates_std_df%&gt;% bind_cols(ehpm17_predictors_survey%&gt;% select(ingpe)) formula_glm= reformulate(paste(names(covariates_std_df),collapse=&quot;+&quot;), &quot;ingpe&quot;) glm_0=glm(formula_glm, data=data_model) sort(car::vif(glm_0)) ## luxury bus hospital ## 1.041469 1.063869 1.105755 ## fuel mkt parking ## 1.130935 1.188140 1.208130 ## soil_prod_prf cult_educ official ## 1.242899 1.252937 1.348125 ## food fin dens_rural_track ## 1.360186 1.386133 1.403419 ## building worship dens_fast_track ## 1.420363 1.435243 1.436840 ## dens_secondary shop secondary ## 1.455963 1.464974 1.473073 ## fast_track tertiary residential ## 1.485937 1.537712 1.538448 ## dens_unclassified health_srv rural_track ## 1.541739 1.551940 1.607205 ## dens_tertiary unclassified dist2health ## 1.705620 1.733906 1.858782 ## all_roads urban_no_motor LZ_FishAgriTourism ## 1.931947 1.932650 1.989607 ## dist2hosp dist2crop_r dens_residential ## 2.181394 2.283840 2.323513 ## slope dist2pubamen dist2road ## 2.372871 2.465642 2.508210 ## soil_prod_trj dist2tree_r gpw_es_TOT ## 2.522349 2.533267 2.596605 ## dist2schools_priv Shape_Area sch_pri ## 2.686345 2.933353 3.113431 ## dist2roadInter dens_all_roads soil_luc ## 3.142500 3.152475 3.218847 ## AREA_ID LZ_LvstckGrainRem pop_dens ## 3.249108 3.327067 3.331335 ## LZ_CoffeeAgroIndus dens_urban_no_motor dist2biz ## 3.415023 3.428458 3.444821 ## chirps_ev_2017 dist2coast_r LZ_BasicGrainLaborZone ## 3.445628 3.510607 3.593043 ## dist2urban_r soil_carb dist2fin ## 3.627804 3.727184 3.768047 ## ndvi_17_perc_sum soil_prod_state LZ_CentralIndusServ ## 4.298087 4.874471 4.950485 ## chirps_ev_med soil_deg_SDG_index settlements ## 5.255571 6.850837 6.951521 ## lights_sd ndvi_17_sum lights_med ## 7.340702 7.865851 12.760513 ## lc_crop dist2schools_pub lc_tree ## 14.256064 17.284021 21.274021 ## dist2allpubserv lc_urban sch_pub ## 21.442907 22.713274 35.288954 ## dist2schools dem sch ## 36.022745 36.222480 37.125670 ## temp_median health_esp_seg health_intermed ## 39.379451 48.457934 352.252893 ## health_basic health dist2water ## 390.098294 763.726685 21007.340492 ## dist2water_r ## 21014.000919 dim(covariates_std_df) ## [1] 1664 82 vif_res=car::vif(glm_0) length(which(c(vif_res)&gt;5)) ## [1] 22 Values below 3 mean there are no multicollinearity issues. Values above 5 are indicative of multicollinearity issues. Of the 82 candidate covariates, 22 have a VIF higher than 5. Some covariate preselection is hence justified. This multicollinearity might be caused by the fact that Many predictors are an expression of the same raw data (e.g., distance to schools and distance to public services) Some predictors cover similar physical features but come from different data sources (e.g., distance to coastline and distance to water bodies) Some predictors are closely associated with each other (e.g., lights at night and percentage of the area classified as urban) 3.2.2 Dimension Reduction Based on Distance Correlation We start by computing our Distance correlation between all of the covariate pairs. Measuring correlation with this correlation better accounts for nonlinearity in the relationship between two variables than a standard Pearson correlation. This can be illustrated by computing the Pearson and Distance correlations in the case of a linear or quadratic relationship. x=seq(-10,10,length.out=1000) y=2*x+rnorm(mean=2,sd=10,1000) plotly::plot_ly(x=x, y=y, type=&quot;scatter&quot;, mode=&quot;markers&quot;)%&gt;% plotly::layout(title=paste0(&quot;Pearson corr:&quot;,format(round(cor(x,y),4),scientific=F), &quot;\\nDistance corr:&quot;,round(dcor(x,y),4))) Figure 3.8: Linear Relationship: Pearson Correlation Picks It Up x=seq(-10,10,length.out=1000) y=-x^2+rnorm(mean=2,sd=10,1000) plotly::plot_ly(x=x, y=y, type=&quot;scatter&quot;, mode=&quot;markers&quot;)%&gt;% plotly::layout(title=paste0(&quot;Pearson corr:&quot;,format(round(cor(x,y),4),scientific=F), &quot;\\nDistance corr:&quot;,round(dcor(x,y),4))) Figure 3.9: Quadratic Relationship: Only The Distance Correlation Picks It Up While the measure of association is similar for both correlation metrics when the dependence between the variables is linear, as shown in Figure 3.8, the Distance correlation is much better at capturing a nonlinear relationship between \\(X\\) and \\(Y\\), as shown by the Distance correlation of 0.46 against -0.0128 for the Pearson correlation reported in Figure 3.9. The Distance correlation for all pairs of covariates is computed as follows. # Compute the Distance correlation between all pairs of covariates tic() distMatrix=dcor.matrix(covariates_std) toc() ## 1803.381 sec elapsed diag(distMatrix)=NA # Replace diag with NA We then randomly remove one of the two most-correlated covariates and repeat the process until only 50 covariates are left, which is 5 percent of the sample size of the training. The latter figure is a generally accepted rule of thumb to decide on an appropriate number of covariates to start the backward covariate selection process, as it limits the risk of chance correlation—finding a significant correlation by chance. # Randomly remove one of the two most-correlated covariates trainig_set_n=dim(covariates_std)[1]*0.6 # Size of the training set = 60%, validation set = 20%, test set=20% target_n_cov=round(0.045*trainig_set_n) # Target number of covariates: 5% of training set= 50 n_cov=dim(covariates_std)[2] # Starting number of covariates = 58 n_to_remove=n_cov-target_n_cov i=1 distMatrix_reduced=distMatrix for(i in 1:n_to_remove){ # Continue to remove the two most-correlated covariates until covariates number only 5% of training set index_max &lt;- arrayInd(which.max(distMatrix_reduced), dim(distMatrix_reduced)) # Get index of columns and rows of the max d-corr cell set.seed(i+2) # Set seeds for replicability print(paste(colnames(distMatrix_reduced)[index_max])) print(distMatrix_reduced[index_max]) remove_cov_pair=round(runif(1)+1) # Either the row (1) or column covariate (2) remove_cov=index_max[remove_cov_pair] # Select index of row or column print(paste(&quot;cov removed&quot;,colnames(distMatrix_reduced)[remove_cov])) # Print the names of the covariate removed print(&quot;&quot;) distMatrix_reduced=distMatrix_reduced[-remove_cov,-remove_cov] # Remove the corresponding covariate from distMatrix } ## [1] &quot;dist2water_r&quot; &quot;dist2water&quot; ## [1] 0.9999477 ## [1] &quot;cov removed dist2water_r&quot; ## [1] &quot;&quot; ## [1] &quot;dist2allpubserv&quot; &quot;dist2schools&quot; ## [1] 0.9731764 ## [1] &quot;cov removed dist2schools&quot; ## [1] &quot;&quot; ## [1] &quot;temp_median&quot; &quot;dem&quot; ## [1] 0.9594395 ## [1] &quot;cov removed temp_median&quot; ## [1] &quot;&quot; ## [1] &quot;sch_pub&quot; &quot;sch&quot; ## [1] 0.9500018 ## [1] &quot;cov removed sch&quot; ## [1] &quot;&quot; ## [1] &quot;dist2allpubserv&quot; &quot;dist2schools_pub&quot; ## [1] 0.9212498 ## [1] &quot;cov removed dist2schools_pub&quot; ## [1] &quot;&quot; ## [1] &quot;lights_sd&quot; &quot;lights_med&quot; ## [1] 0.9194312 ## [1] &quot;cov removed lights_sd&quot; ## [1] &quot;&quot; ## [1] &quot;lc_urban&quot; &quot;settlements&quot; ## [1] 0.8422507 ## [1] &quot;cov removed lc_urban&quot; ## [1] &quot;&quot; ## [1] &quot;lights_med&quot; &quot;settlements&quot; ## [1] 0.8173944 ## [1] &quot;cov removed settlements&quot; ## [1] &quot;&quot; ## [1] &quot;soil_luc&quot; &quot;soil_carb&quot; ## [1] 0.8060875 ## [1] &quot;cov removed soil_luc&quot; ## [1] &quot;&quot; ## [1] &quot;lights_med&quot; &quot;ndvi_17_sum&quot; ## [1] 0.7990358 ## [1] &quot;cov removed lights_med&quot; ## [1] &quot;&quot; ## [1] &quot;soil_deg_SDG_index&quot; &quot;soil_prod_state&quot; ## [1] 0.7969524 ## [1] &quot;cov removed soil_prod_state&quot; ## [1] &quot;&quot; ## [1] &quot;dist2allpubserv&quot; &quot;Shape_Area&quot; ## [1] 0.7797326 ## [1] &quot;cov removed dist2allpubserv&quot; ## [1] &quot;&quot; ## [1] &quot;lc_tree&quot; &quot;dist2tree_r&quot; ## [1] 0.7721969 ## [1] &quot;cov removed dist2tree_r&quot; ## [1] &quot;&quot; ## [1] &quot;dens_residential&quot; &quot;dens_all_roads&quot; ## [1] 0.7399636 ## [1] &quot;cov removed dens_all_roads&quot; ## [1] &quot;&quot; ## [1] &quot;Shape_Area&quot; &quot;AREA_ID&quot; ## [1] 0.7322786 ## [1] &quot;cov removed Shape_Area&quot; ## [1] &quot;&quot; ## [1] &quot;pop_dens&quot; &quot;gpw_es_TOT&quot; ## [1] 0.7249122 ## [1] &quot;cov removed gpw_es_TOT&quot; ## [1] &quot;&quot; ## [1] &quot;dens_urban_no_motor&quot; &quot;urban_no_motor&quot; ## [1] 0.7237921 ## [1] &quot;cov removed dens_urban_no_motor&quot; ## [1] &quot;&quot; ## [1] &quot;dist2urban_r&quot; &quot;dist2schools_priv&quot; ## [1] 0.7161026 ## [1] &quot;cov removed dist2schools_priv&quot; ## [1] &quot;&quot; ## [1] &quot;dist2urban_r&quot; &quot;dist2roadInter&quot; ## [1] 0.7024571 ## [1] &quot;cov removed dist2roadInter&quot; ## [1] &quot;&quot; ## [1] &quot;dist2fin&quot; &quot;dist2biz&quot; ## [1] 0.6998672 ## [1] &quot;cov removed dist2fin&quot; ## [1] &quot;&quot; ## [1] &quot;health_basic&quot; &quot;health&quot; ## [1] 0.6977073 ## [1] &quot;cov removed health&quot; ## [1] &quot;&quot; ## [1] &quot;dist2urban_r&quot; &quot;AREA_ID&quot; ## [1] 0.6967835 ## [1] &quot;cov removed dist2urban_r&quot; ## [1] &quot;&quot; ## [1] &quot;soil_deg_SDG_index&quot; &quot;soil_prod_trj&quot; ## [1] 0.6849447 ## [1] &quot;cov removed soil_deg_SDG_index&quot; ## [1] &quot;&quot; ## [1] &quot;LZ_CentralIndusServ&quot; &quot;pop_dens&quot; ## [1] 0.6684595 ## [1] &quot;cov removed LZ_CentralIndusServ&quot; ## [1] &quot;&quot; ## [1] &quot;dens_secondary&quot; &quot;secondary&quot; ## [1] 0.6614154 ## [1] &quot;cov removed secondary&quot; ## [1] &quot;&quot; ## [1] &quot;lc_tree&quot; &quot;slope&quot; ## [1] 0.6532028 ## [1] &quot;cov removed lc_tree&quot; ## [1] &quot;&quot; ## [1] &quot;dist2biz&quot; &quot;dist2pubamen&quot; ## [1] 0.6480211 ## [1] &quot;cov removed dist2biz&quot; ## [1] &quot;&quot; ## [1] &quot;dens_rural_track&quot; &quot;rural_track&quot; ## [1] 0.6472546 ## [1] &quot;cov removed dens_rural_track&quot; ## [1] &quot;&quot; ## [1] &quot;dens_fast_track&quot; &quot;fast_track&quot; ## [1] 0.5912912 ## [1] &quot;cov removed fast_track&quot; ## [1] &quot;&quot; ## [1] &quot;ndvi_17_sum&quot; &quot;AREA_ID&quot; ## [1] 0.5816056 ## [1] &quot;cov removed AREA_ID&quot; ## [1] &quot;&quot; ## [1] &quot;chirps_ev_med&quot; &quot;chirps_ev_2017&quot; ## [1] 0.5710039 ## [1] &quot;cov removed chirps_ev_med&quot; ## [1] &quot;&quot; ## [1] &quot;lc_crop&quot; &quot;dist2crop_r&quot; ## [1] 0.5641672 ## [1] &quot;cov removed lc_crop&quot; ## [1] &quot;&quot; ## [1] &quot;dens_tertiary&quot; &quot;tertiary&quot; ## [1] 0.5551673 ## [1] &quot;cov removed tertiary&quot; ## [1] &quot;&quot; ## [1] &quot;dens_unclassified&quot; &quot;unclassified&quot; ## [1] 0.5353308 ## [1] &quot;cov removed unclassified&quot; ## [1] &quot;&quot; ## [1] &quot;dist2crop_r&quot; &quot;slope&quot; ## [1] 0.522504 ## [1] &quot;cov removed slope&quot; ## [1] &quot;&quot; ## [1] &quot;pop_dens&quot; &quot;ndvi_17_sum&quot; ## [1] 0.4994304 ## [1] &quot;cov removed pop_dens&quot; ## [1] &quot;&quot; ## [1] &quot;dist2coast_r&quot; &quot;chirps_ev_2017&quot; ## [1] 0.488211 ## [1] &quot;cov removed dist2coast_r&quot; ## [1] &quot;&quot; Next, we recompute the VIF with the remaining 50 covariates. Median lights at night (named lights_med in the dataset), the rural-urban DYGESTIC binary segmentos classification (named AREA_ID in the dataset), population density (pop_dens), and slope (slope) are removed in the process, but they need to be reintroduced because we expect them to be important predictors of the development outcomes. # RE calculate vif #### sort(colnames(distMatrix_reduced)) ## [1] &quot;all_roads&quot; &quot;building&quot; &quot;bus&quot; ## [4] &quot;chirps_ev_2017&quot; &quot;cult_educ&quot; &quot;dem&quot; ## [7] &quot;dens_fast_track&quot; &quot;dens_residential&quot; &quot;dens_secondary&quot; ## [10] &quot;dens_tertiary&quot; &quot;dens_unclassified&quot; &quot;dist2crop_r&quot; ## [13] &quot;dist2health&quot; &quot;dist2hosp&quot; &quot;dist2pubamen&quot; ## [16] &quot;dist2road&quot; &quot;dist2water&quot; &quot;fin&quot; ## [19] &quot;food&quot; &quot;fuel&quot; &quot;health_basic&quot; ## [22] &quot;health_esp_seg&quot; &quot;health_intermed&quot; &quot;health_srv&quot; ## [25] &quot;hospital&quot; &quot;luxury&quot; &quot;LZ_BasicGrainLaborZone&quot; ## [28] &quot;LZ_CoffeeAgroIndus&quot; &quot;LZ_FishAgriTourism&quot; &quot;LZ_LvstckGrainRem&quot; ## [31] &quot;mkt&quot; &quot;ndvi_17_perc_sum&quot; &quot;ndvi_17_sum&quot; ## [34] &quot;official&quot; &quot;parking&quot; &quot;residential&quot; ## [37] &quot;rural_track&quot; &quot;sch_pri&quot; &quot;sch_pub&quot; ## [40] &quot;shop&quot; &quot;soil_carb&quot; &quot;soil_prod_prf&quot; ## [43] &quot;soil_prod_trj&quot; &quot;urban_no_motor&quot; &quot;worship&quot; covariates_std_reduced=covariates_std[,c(colnames(distMatrix_reduced),&quot;lights_med&quot;,&quot;AREA_ID&quot;,&quot;pop_dens&quot;,&quot;slope&quot;)] covariates_std_reduced_df=as.data.frame(covariates_std_reduced) data_model=covariates_std_reduced_df%&gt;% bind_cols(ehpm17_predictors_survey%&gt;% select(ingpe)) formula_glm_1= reformulate(paste(names(covariates_std_reduced_df),collapse=&quot;+&quot;), &quot;ingpe&quot;) glm_1=glm(formula_glm_1, data=data_model) sort(car::vif(glm_1)) ## luxury bus dens_secondary ## 1.017735 1.045257 1.069115 ## health_esp_seg hospital health_basic ## 1.077276 1.087613 1.089489 ## health_intermed fuel soil_prod_prf ## 1.093697 1.108080 1.112309 ## dens_fast_track dens_unclassified sch_pri ## 1.119581 1.131510 1.138939 ## dens_tertiary mkt rural_track ## 1.139313 1.147971 1.164022 ## parking cult_educ urban_no_motor ## 1.193745 1.203896 1.215575 ## soil_carb building soil_prod_trj ## 1.241506 1.250470 1.283704 ## official food fin ## 1.313564 1.330757 1.335872 ## worship sch_pub shop ## 1.374467 1.407148 1.419167 ## LZ_FishAgriTourism residential dens_residential ## 1.426517 1.427602 1.448909 ## all_roads health_srv chirps_ev_2017 ## 1.450050 1.483279 1.494114 ## dist2road dist2health dist2hosp ## 1.521071 1.613757 1.659747 ## dist2pubamen pop_dens dist2crop_r ## 1.746986 1.796098 1.805095 ## dist2water LZ_CoffeeAgroIndus LZ_BasicGrainLaborZone ## 1.809049 1.830862 1.990535 ## slope LZ_LvstckGrainRem dem ## 2.057306 2.189399 2.276269 ## AREA_ID ndvi_17_perc_sum lights_med ## 2.364066 3.281148 4.858914 ## ndvi_17_sum ## 5.740155 sort(names(covariates_std_reduced_df)) ## [1] &quot;all_roads&quot; &quot;AREA_ID&quot; &quot;building&quot; ## [4] &quot;bus&quot; &quot;chirps_ev_2017&quot; &quot;cult_educ&quot; ## [7] &quot;dem&quot; &quot;dens_fast_track&quot; &quot;dens_residential&quot; ## [10] &quot;dens_secondary&quot; &quot;dens_tertiary&quot; &quot;dens_unclassified&quot; ## [13] &quot;dist2crop_r&quot; &quot;dist2health&quot; &quot;dist2hosp&quot; ## [16] &quot;dist2pubamen&quot; &quot;dist2road&quot; &quot;dist2water&quot; ## [19] &quot;fin&quot; &quot;food&quot; &quot;fuel&quot; ## [22] &quot;health_basic&quot; &quot;health_esp_seg&quot; &quot;health_intermed&quot; ## [25] &quot;health_srv&quot; &quot;hospital&quot; &quot;lights_med&quot; ## [28] &quot;luxury&quot; &quot;LZ_BasicGrainLaborZone&quot; &quot;LZ_CoffeeAgroIndus&quot; ## [31] &quot;LZ_FishAgriTourism&quot; &quot;LZ_LvstckGrainRem&quot; &quot;mkt&quot; ## [34] &quot;ndvi_17_perc_sum&quot; &quot;ndvi_17_sum&quot; &quot;official&quot; ## [37] &quot;parking&quot; &quot;pop_dens&quot; &quot;residential&quot; ## [40] &quot;rural_track&quot; &quot;sch_pri&quot; &quot;sch_pub&quot; ## [43] &quot;shop&quot; &quot;slope&quot; &quot;soil_carb&quot; ## [46] &quot;soil_prod_prf&quot; &quot;soil_prod_trj&quot; &quot;urban_no_motor&quot; ## [49] &quot;worship&quot; The VIF figures are much more acceptable now. The reduced number of covariates limits the risk of chance correlation. We write the covariates and their names in .csv files for later use. # Write the names of selected features into .csv #### write.table(names(covariates_std_reduced_df), paste0(dir_data, &quot;out/distance_corr_var/selected_all.txt&quot;), row.names = F) # Write the selected features and outcome into .csv #### covariates_std_reduced_df$SEG_ID=ehpm17_predictors_survey$SEG_ID covariates_std_reduced_df$ingpe=ehpm17_predictors_survey$ingpe covariates_std_reduced_df$literacy_rate=ehpm17_predictors_survey$literacy_rate covariates_std_reduced_df$pobreza_extrema=ehpm17_predictors_survey$pobreza_extrema covariates_std_reduced_df$pobreza_mod=ehpm17_predictors_survey$pobreza_mod covariates_std_reduced_df$n_obs=ehpm17_predictors_survey$n_obs write.csv(covariates_std_reduced_df, paste0(dir_data, &quot;out/all_covariates_and_outcomes.csv&quot;), row.names = F) 3.3 Correlations between the Predictors and the Outcome Variables We now explore the relationship between the predictors and the outcome variables. This will give us a feel of the important covariate in the model. 3.3.1 Correlation with Income We compute the distance correlation for all covariates with a loop and store these correlations in a data frame. covariates_std_reduced_df$SEG_ID=ehpm17_predictors_survey$SEG_ID covariates_std_reduced_df$ingpe=ehpm17_predictors_survey$ingpe covariates_std_reduced_df$literacy_rate=ehpm17_predictors_survey$literacy_rate covariates_std_reduced_df$pobreza_extrema=ehpm17_predictors_survey$pobreza_extrema covariates_std_reduced_df$pobreza_mod=ehpm17_predictors_survey$pobreza_mod covariates_std_reduced_df$n_obs=ehpm17_predictors_survey$n_obs corr_re=list() names_re=list() for(i in 1:50){ # Where 50 is the number of cor_test=energy::dcor(covariates_std_reduced_df$ingpe,covariates_std_reduced_df[,i]) corr_re[[i]]=c(cor_test) names_re[[i]]=names(covariates_std_reduced_df)[i] } corr_df=do.call(rbind,corr_re) names_df=do.call(rbind,names_re) corr_df=as_tibble(corr_df) ## Warning: The `x` argument of `as_tibble.matrix()` must have column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. corr_df$covariate = names_df names(corr_df)[1]=&quot;cor&quot; corr_df=corr_df %&gt;% arrange(desc(abs(cor))) Lastly, we select only the variables with an absolute distance correlation with income higher than 20 percent and plot the correlation with income. The latter selection is only for illustrative purposes: we do not want to clutter the graph. corr_df_top=corr_df%&gt;% filter(!(covariate%in%c(&quot;pobreza_mod&quot;,&quot;literacy_rate&quot;,&quot;ingpe&quot;)))%&gt;% filter(abs(cor)&gt;=.2) corr_df_top$covariate_f=factor(corr_df_top$covariate, levels = corr_df_top$covariate) plot_ly(x=corr_df_top$covariate_f, y=corr_df_top$cor, type=&quot;bar&quot;, name = corr_df_top$covariate)%&gt;% layout(title=&quot;Covariate Correlations with Income&quot;, yaxis=list(tickformat = &quot;%&quot;)) Figure 3.10: Correlation of Candidate Covariates with Income knitr::include_graphics(&quot;img/corr_ingpe.PNG&quot;) As expected, among the three most important covariates are those indicative of urban centers: light at night intensity, rural/urban classification of the segment, and distance to public amenities. To get a sense of the direction of the correlation and the associations between the variables, we plot a correlogram. covariates_std_reduced_df_selected=covariates_std_reduced_df%&gt;% dplyr::select_(.dots =c(&quot;ingpe&quot;,corr_df_top$covariate)) ## Warning: `select_()` is deprecated as of dplyr 0.7.0. ## Please use `select()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. qgraph::qgraph(cor(covariates_std_reduced_df_selected), minimum=0.20, layout=&quot;spring&quot;, # groups=gr, labels=names(covariates_std_reduced_df_selected), label.scale=F, title=&quot;Correlations: Income and Predictors&quot;) Figure 3.11: Correlogram of Income (ingpe) with Candidate Covariates The plot in Fig.3.11 clearly shows that the top predictors are highly correlated. 3.3.2 Correlation with Literacy The process described in the previous subsection is repeated for literacy. Light at night is again the best predictor of literacy, by distance, the rural-urban segmento identifier, and normalized difference vegetation index (NDVI). 3.3.3 Correlation with Poverty In examining this correlation, NDVI and distance to public amenities and hospitals, in particular, come first. We see that the level of correlation is lower than in the previous plot, suggesting that it might be harder to find good model fit for moderate poverty than for the other indicators. Figure 3.12: Correlation of Candidate Covariates with Moderate Poverty A Gaussian distribution is a normal distribution. We use the former terminology as it corresponds to the INLA one↩︎ "],
["simple-model.html", "Section 4 Fitting a simple model 4.1 Loading the Data 4.2 Area Data 4.3 Point-Referenced Data 4.4 K-fold Cross-Validation", " Section 4 Fitting a simple model The goal of this section is to fit a simple model for income to introduce the modified Besag-York-Mollie model (BYM 2) and the stochastic partial differential equation (SPDE) model. In both cases, the general process is the same: Define the spatial dependency. Split the model into training and validation sets. Specify the formula, expressing the outcome variables in terms of fixed and random spatial effects. Fit the model on the training set. Validate the model on the validation set. The fixed effects are the covariates (e.g., population density or light at night). The spatial random effects are spatially correlated random effects used to model the spatial dependency. Note that it is assumed here that the list of fixed effects is known. The next section presents a backward stepwise covariate selection process to select the list of covariates. 4.1 Loading the Data We start by loading the covariates data and matching them with the segmento spatial polygon data frame. rm(list=ls()) library(parallel) library(INLA) library(dplyr) #INLA:::inla.dynload.workaround() # Modify dir_data to where you stored the data root_dir=&quot;~/&quot; project_dir=&quot;data/&quot; dir_data=paste0(root_dir,project_dir) # Load the data #### ehpm17_predictors=read.csv(paste0(dir_data, &quot;out/all_covariates_and_outcomes.csv&quot;)) # Correct for .xls misbehavior: The SEG_ID with a leading 0 were shortened (i.e. the leading 0 was eliminated) ehpm17_predictors=ehpm17_predictors%&gt;% mutate(SEG_ID=as.character(SEG_ID), SEG_ID=ifelse(nchar(SEG_ID)==7, paste0(0,SEG_ID), SEG_ID)) # Shape segmento_sh=rgdal::readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Segmentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields # Add the survey data to shapefile segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% # dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) 4.2 Area Data 4.2.1 Spatial Dependency with Area Data To model the spatial dependency with data from the area data, we need to inform the model of neighboring segmentos. The function spdep::poly2nb creates a neighbors list from the segmento polygon. The list is created in a spatial format suitable for INLA with the function spdep::nb2INLA. It is then read as a graph object for INLA consumption. # Define neighboring structures #### segmento_sh_data@data$ID=1:length(segmento_sh_data@data$n_obs) segmento.nb=spdep::poly2nb(segmento_sh_data) spdep::nb2INLA(paste0(dir_data, &quot;out/SEG.graph&quot;), segmento.nb) SEG.adj=paste0(paste0(dir_data, &quot;out/SEG.graph&quot;), sep=&quot;&quot;) Segmento.Inla.nb &lt;- INLA::inla.read.graph(paste0(dir_data, &quot;out/SEG.graph&quot;)) Figure 4.1 illustrates the results with data at the department level. ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Departamentos.shp&quot;, layer: &quot;STPLAN_Departamentos&quot; ## with 14 features ## It has 6 fields Figure 4.1: Neighboring Structures at the Departamento Level 4.2.2 Store the Data for Modeling The data are stored in the data frame data_model. # Identify EHPM 17 segmentos to fit the model #### segmento_sh_data_model=segmento_sh_data non_na_index=which(is.na(segmento_sh_data_model$n_obs)==F) ehpm17_predictors_nona=segmento_sh_data_model@data[non_na_index,] # Covariates ##### cov_candidates_selected_df=read.table(paste0(dir_data, &quot;out/distance_corr_var/selected_all.txt&quot;), header =T) cov_candidates_selected_table=cov_candidates_selected_df names(cov_candidates_selected_table)=&quot;Candidates&quot; # knitr::kable( # cov_candidates_selected_table, # caption = &#39;Candidate covariates for income&#39;, # booktabs = TRUE # ) covariates=ehpm17_predictors_nona[,as.character(cov_candidates_selected_table$Candidates)] # Store data for the model #### data_model=data.frame(ingpe=ehpm17_predictors_nona$ingpe, intercept=array(1,dim(covariates)[1]), # intercept for INLA ID=ehpm17_predictors_nona$ID, # ID to link data to the neibhouring structure covariates) 4.2.3 Split the Sample into 80-20 Training and Validation Sets The observations in the dataset are split into 80-20 training and validation sets. The model will be fitted on the training set, and income will be predicted on the validation sets. The predicted values will be compared with the observed values of the validation set to assess the model’s goodness of fit. # Take a sample of segmentos for training, validation, and test #### set.seed(1234) spec = c(train = .8, validate = .2) g = sample(cut( seq(nrow(data_model)), nrow(data_model)*cumsum(c(0,spec)), labels = names(spec) )) index_val=which(g==&quot;validate&quot;) index_train=which(g==&quot;train&quot;) data_model$pred=data_model$ingpe data_model$pred[c(index_val)] &lt;- NA # Set the validation predictions NA: The values will be predicted by the model and compared with observed values 4.2.4 Formula Specifying the Relationship between the Outcome Variable and the Fixed and Spatial Random Effects Next, we write a formula to predict income with a subset of the covariates and chose to use the following covariates: Binary indicator for rural and urban segmentos Median lights at night Population density Average slope in the segmentos Average precipitation Distance to public services Distance to businesses Distance to roads # Write the test formula #### formula_test =pred~ AREA_ID+lights_med+pop_dens+slope+chirps_ev_2017+dist2pubamen+dist2road+ f(ID, model = &quot;bym2&quot;, graph = Segmento.Inla.nb, hyper=list( prec = list(prior = &quot;pc.prec&quot;, param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, 0.5))), scale.model = TRUE, constr = T, adjust.for.con.comp = T) The variable pred is the output variable we aim to model (e.g., income). The selected covariates are added linearly. Remember that they have been standardized in the previous section to avoid any issues linked to scale. These covariates are called fixed effects. The most complex aspect of the formula is the spatial dependence structure specified in f(). The ID is the segmento identifiers linking the graph object Segmento.Inla.nb, where the neighboring structure is defined, with the data frame where the covariates and output data are stored. The model parameter is for the specification of the spatial correlation model that should be used. The bym2 is used because it is flexible and allows for a relatively intuitive specification of spatial random effects. The flexibility comes from the fact that bym2 allows for the error term to be composed of two elements: (1) a pure random noise, and (2) a spatially correlated noise. The penalized complexity priors phi allows the importance of each effect to be influenced (if phi=0, then we have pure noise, if phi=1, we have only a spatially correlated random effect). The prec parameter is the precision parameters, or how much the spatially correlated noise is allowed to vary spatially. If the value is high, then the spatially correlated noise is allowed to vary a lot across space and vice versa. By decreasing the penalized complexity priors on the precision parameter, we decrease the risk of overfitting: less of the spatial variation left unexplained by the covariates is attributed to the spatial random effects, and, as a result, the model is better able to generalize to areas where it was not trained. Here, we have chosen to set the penalized complexity prior on precision as \\(Pr(\\sigma&gt;1)=0.0001\\) instead of the default \\(Pr(\\sigma&gt;1)=0.01\\). Indeed, we were facing overfitting issues with the default priors: the difference between the goodness of fit in the training and validation sets was very large (about 20 percent in the \\(R^{2}\\)). 4.2.5 Fit the Model on the Training Set The model is fitted with the inla command. We have to specify the likelihood distribution of the response variable. The family argument defines the type of likelihood function for the response variable. As the distribution of income is strictly positive and right skewed, a gamma distribution is a good option. The data argument specifies the data on which the model is fitted, which in our case is the data_model data frame. As we use a gamma likelihood function, the predicted values need to be back transformed to the original scale by using the exponential function. The control.predictor automatizes the process once the option list(link=1,compute=T) is specified. We use a simple integration strategy to compute the posterior marginals of the model parameters by specifying control.inla =list(int.strategy = \"eb\"), where eb stands for “empirical Bayes.” In the empirical Bayes approach, we use only one “integration point equal to the posterior mode of the hyperparameters” (Moraga 2019). This speeds up the estimation of the model. # Fit the model on the training set #### start_time=Sys.time() # Time the start of the estimation bym.res=inla(formula=formula_test, family = &quot;gamma&quot;, data = data_model, control.predictor=list(link=1,compute=T), control.inla =list(int.strategy = &quot;eb&quot;)) end_time=Sys.time() # Time at the end of the estimation duration=end_time-start_time print(duration) ## Time difference of 55.78308 secs It took 3.7 minutes to estimate the model. With the default integration strategy, the model takes 13.5 minutes to be estimated. No significant difference was found between the integration strategies. 4.2.6 Validate the Model on the Validation Set The model’s goodness of fit can now be investigated on the validation set. We start by extracting the fitted values, which are stored in the bym.res object under summary.fitted.values. The inla model yields a distribution of predicted income for each segmento. Various statistics summarizing this prediction are also available for each segmento: The mean fitted value The median fitted value The 2.5 and 97.5 percentiles of the fitted values The standard deviation of the fitted value distribution Below, we select the mean fitted value, that is, the mean prediction value for each segmento. # Extract the fitted values #### M_fit=bym.res$summary.fitted.values[,&quot;mean&quot;] We can now compute the root mean square error (RMSE) and the pseudo \\(R^{2}\\) as shown below.2 RMSE=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RMSE_val &lt;- sqrt(mean(res^2,na.rm=T)) return(RMSE_val) } pseudo_r2=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } RMSE_val=RMSE(index_val,&quot;ingpe&quot;,data_model,M_fit) RMSE_train=RMSE(index_train,&quot;ingpe&quot;,data_model,M_fit) r2_val=pseudo_r2(index_val,&quot;ingpe&quot;,data_model,M_fit) r2_train=pseudo_r2(index_train,&quot;ingpe&quot;,data_model,M_fit) a &lt;- list( x = 600, y = 100, text = paste(&quot;R2 training set:&quot;,round(r2_train*100),&quot;%&quot;, &quot;\\nR2 validation set:&quot;,round(r2_val*100),&quot;%&quot;, &quot;\\n&quot;, &quot;\\nRMSE training set:&quot;,round(RMSE_train),&quot;USD&quot;, &quot;\\nRMSE validation set:&quot;,round(RMSE_val),&quot;USD&quot;), xref = &quot;x&quot;, yref = &quot;y&quot;, showarrow = F) Figure 4.2: Observed versus Predicted Income Values, BYM 2 Model Figure 4.2 shows the predicted and observed income values for the training and validation sets. The model appears to do a relatively good job, although it has difficulty capturing the segmentos with an income above 400. An \\(R^{2}\\) of 44 percent is relatively satisfactory. It means that 44 percent of the income variation in the validation set is explained by the model fitted on the training set. However, the fact that the \\(R^{2}\\) is at 69 percent, close to twice as large as the \\(R^{2}\\) in the validation set, indicates overfitting. The RMSE is USD 66 in the validation set and USD 40 in the training set. The RMSE is the standard deviation of the residuals, the difference between the fitted and observed values. The results are stored in a list and saved in a .RData file for later consumption. income_bym2_naive=list(&quot;outcome&quot;=&quot;ingpe&quot;, &quot;spat_dep&quot;=&quot;bym2&quot;, &quot;cov_select&quot;=&quot;naive&quot;, &quot;formula&quot;=formula_test, &quot;family&quot;=&quot;gamma&quot;, &quot;data&quot;=data_model, &quot;fit&quot;=M_fit, &quot;index_val&quot;=index_val, &quot;index_train&quot;=index_train, &quot;r2_val&quot;=r2_val, &quot;r2_train&quot;=r2_train, &quot;RMSE_val&quot;=RMSE_val, &quot;RMSE_train&quot;=RMSE_train) save(income_bym2_naive, file=paste0(dir_data, &quot;out/results/income_bym2_naive.RData&quot;)) 4.3 Point-Referenced Data 4.3.1 Spatial Dependency with Point-Referenced Data The spatial dependence is modeled here with an SPDE approach. The following steps are required: 1 Create the mesh. 2 Create the SPDE. *3 Create the matrix of weights. 4.3.1.1 Create the Mesh To create the mesh, we start by identifying the segmentos for which there are EHPM data. Second, we create a spatial polygon of the country’s boundaries by dissolving the shapefile of the departamentos with the function unionSpatialPolygons. Third, we transform this spatial polygon into a list that the INLA library can process with the function inla.sp2segment. Lastly, we use the helper function inla.mesh.create.helper to create the mesh, using the boundary and coordinates of the segmentos’ centroids (i.e., middle points) as inputs. # Create the mesh #### segmento_sh_subset=subset(segmento_sh_data_model, is.na(segmento_sh_data_model@data$ingpe)==F) # Identify where there is EHPM data coords=coordinates(segmento_sh_subset) # Collect the coordinates of the segmentos with EHPM data SVborder &lt;- maptools::unionSpatialPolygons(departamentos_sh, rep(1, nrow(departamentos_sh))) # Create one polygon using the boundaries of the countries SV.bdry &lt;- inla.sp2segment(SVborder) # Create an inla boundary object SV.mesh &lt;- inla.mesh.create.helper( boundary=SV.bdry, points=coords, offset=c(2500, 25000), max.edge=c(20000, 50000), min.angle=c(25, 25), cutoff=8000, plot.delay=NULL) plot(SV.mesh,main=&quot;&quot;,asp=1) Figure 4.3: Mesh The offset parameters of inla.mesh.create.helper define the inner and outer distances. For instance, if we change the outer offset parameter to 250 kilometers instead of the original 25 kilometers, the result is plotted in Figure 4.4. SV.meshL &lt;- inla.mesh.create.helper( boundary=SV.bdry, points=coords, offset=c(2500, 250000), # we increased from 25000 to 250000 max.edge=c(20000, 50000), min.angle=c(25, 25), cutoff=8000, plot.delay=NULL) plot(SV.meshL,main=&quot;&quot;,asp=1) Figure 4.4: Mesh When choosing the outer offset parameter, the aim is to avoid the presence of boundary effects in the subsequent estimation of the SPDE, so it is best to allow for a sufficient distance as shown in Figure 4.3. However, Figure 4.4 is clearly too large and would slow the estimation process. The inner offset parameter has no effect here, as it is superseded by the max.edge, min.angle, and cutoff parameters. The max.edge parameter defines the largest-allowed triangle-edge length. The min.angle parameter defines the smallest-allowed triangle angle, and the parameter cutoff defines the minimum-allowed distance between points. Next, we illustrate the impact of modifying the max.edge from 20 kilometers to 5 kilometers. SV.meshL &lt;- inla.mesh.create.helper( boundary=SV.bdry, points=coords, offset=c(2500, 25000), max.edge=c(20000, 5000), # Decrease from 50,000 to 5,000 min.angle=c(25, 25), cutoff=8000, plot.delay=NULL) plot(SV.meshL,main=&quot;&quot;,asp=1) Figure 4.5: Mesh Figure 4.5 shows the results. The risk of such a tight mesh is that it could lead to overfitting. Furthermore, it is computationally more expensive. 4.3.1.2 Derive the SPDE Next, the SPDE is derived from the mesh using the inla.spde2.matern function, and the spatial index is derived with the function inla.spde.make.index. # Create the SPDE #### SV.spde &lt;- inla.spde2.matern(mesh=SV.mesh,alpha=2) s.index &lt;- inla.spde.make.index(name=&quot;spatial.field&quot;, n.spde=SV.spde$n.spde) The s.index allows the mesh to link to the matrix of spatial weights that will be defined below. #### Create the matrix of weights The matrix of weights is created with the INLA function inla.spde.make.A. A.train &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_train,]) A.val &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_val,]) 4.3.2 Store the Data for Modeling in a Stack We use the inla.stack function to conveniently store the data in one object. # Stack #### covariates_list=c(&quot;AREA_ID&quot;,&quot;lights_med&quot;,&quot;pop_dens&quot;,&quot;slope&quot;,&quot;chirps_ev_2017&quot;,&quot;dist2pubamen&quot;,&quot;dist2road&quot;) stack.train &lt;- inla.stack(data = list(pred=segmento_sh_subset@data$ingpe[index_train]), # This output variable (income) A = list(A.train, 1,1), effects = list(s.index, Intercept=1:length(index_train), segmento_sh_subset@data[index_train,c(covariates_list)]), tag=&quot;train&quot;) # The tag allows selected statistics for the desired sample to be extracted later stack.val &lt;- inla.stack(data = list(pred=NA), # Set it to NA A = list(A.val, 1,1), effects = list(s.index, Intercept=1:length(index_val), segmento_sh_subset@data[index_val,c(covariates_list)]), tag=&quot;val&quot;) # Combine both stacks join.stack &lt;- inla.stack(stack.train, stack.val) 4.3.3 Specifying the Relationship between the Outcome Variable and the Fixed and Spatial Random Effects We use the same set of fixed effects. # Write the test formula #### formula_test =pred~ -1+Intercept+ AREA_ID+lights_med+pop_dens+slope+chirps_ev_2017+dist2pubamen+dist2road+ f(spatial.field, model=SV.spde) The main difference from the BYM 2 specification is that the spatial random effects are specified by the f() function, which takes two arguments: spatial.field is the name of the spatial index s.index, and the model is the SPDE defined in SV.spde. 4.3.4 Fit the Model on the Training Set The same inla command used to fit the areal model is used to fit the SPDE model. The only difference is that the data argument takes the stack data defined above as an input. # Fit the model#### start_time=Sys.time() spde_res=inla(formula_test, # Formula data=inla.stack.data(join.stack), family=&quot;gamma&quot;, # Likelihood of the data control.predictor=list(A=inla.stack.A(join.stack), link=1,compute=T)) end_time=Sys.time() print(end_time-start_time) ## Time difference of 10.09208 secs The model is much faster to fit: it took only 19 seconds (against 3.5 minutes for the BYM 2 model). 4.3.5 Validate the Model on the Validation Set We can now investigate the model’s goodness of fit. To extract the fitted values for the training and validation sets from the income_spde_test object, we use the inla.stack.index function. Again, we extract only the mean predicted value for each segmento. # Extract the fitted values index_inla_train = inla.stack.index(join.stack,&quot;train&quot;)$data index_inla_val = inla.stack.index(join.stack,&quot;val&quot;)$data results.train=spde_res$summary.fitted$mean[index_inla_train] results.val=spde_res$summary.fitted$mean[index_inla_val] M_fit_spde=array(NA,length(M_fit)) M_fit_spde[index_train]=results.train M_fit_spde[index_val]=results.val r2_train_spde=pseudo_r2(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) r2_val_spde=pseudo_r2(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_train_spde=RMSE(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_val_spde=RMSE(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) a &lt;- list( x = 600, y = 100, text = paste(&quot;R2 training set:&quot;,round(r2_train_spde*100),&quot;%&quot;, &quot;\\nR2 validation set:&quot;,round(r2_val_spde*100),&quot;%&quot;, &quot;\\n&quot;, &quot;\\nRMSE training set:&quot;,round(rmse_train_spde),&quot;USD&quot;, &quot;\\nRMSE validation set:&quot;,round(rmse_val_spde),&quot;USD&quot;), xref = &quot;x&quot;, yref = &quot;y&quot;, showarrow = F) Figure 4.6: Observed versus Predicted Income Values, SPDE Model Figure 4.6 shows the observed values against the predicted values for the training and validation sets. There are no major differences in the results obtained with the modified BYM model shown in Figure 4.2 except that there does not appear to be overfitting here, as the \\(R^{2}\\) are similar in the training and validation sets. Next, we save the results for later use. income_spde_naive=list(&quot;outcome&quot;=&quot;ingpe&quot;, &quot;spat_dep&quot;=&quot;spde&quot;, &quot;cov_select&quot;=&quot;naive&quot;, &quot;formula&quot;=formula_test, &quot;family&quot;=&quot;gamma&quot;, &quot;data&quot;=join.stack, &quot;fit&quot;=M_fit_spde, &quot;index_val&quot;=index_val, &quot;index_train&quot;=index_train, &quot;r2_val&quot;=r2_val_spde, &quot;r2_train&quot;=r2_train_spde, &quot;RMSE_val&quot;=rmse_val_spde, &quot;RMSE_train&quot;=rmse_train_spde) save(income_spde_naive, file=paste0(dir_data, &quot;out/results/income_spde_naive.RData&quot;)) Lastly, Figure 4.7 shows the results when using a Gaussian likelihood and log transforming the income data. There is a minor improvement to the goodness of fit compared to the gamma model. # Write the test formula #### formula_test =log(pred)~ -1+Intercept+ AREA_ID+lights_med+pop_dens+slope+chirps_ev_2017+dist2pubamen+dist2road+ f(spatial.field, model=SV.spde) # Fit the model#### spde_res=inla(formula_test, # Formula data=inla.stack.data(join.stack), family=&quot;gaussian&quot;, # Likelihood of the data control.predictor=list(A=inla.stack.A(join.stack), link=1,compute=T)) # Extract the fitted values index_inla_train = inla.stack.index(join.stack,&quot;train&quot;)$data index_inla_val = inla.stack.index(join.stack,&quot;val&quot;)$data results.train=spde_res$summary.fitted$mean[index_inla_train] results.val=spde_res$summary.fitted$mean[index_inla_val] M_fit_spde=array(NA,length(M_fit)) M_fit_spde[index_train]=results.train M_fit_spde[index_val]=results.val # Compute the goodness-of-fit statistics segmento_sh_subset@data$ln_ingpe=log(segmento_sh_subset@data$ingpe) r2_train_spde=pseudo_r2(index_train,&quot;ln_ingpe&quot;,segmento_sh_subset@data,M_fit_spde) r2_val_spde=pseudo_r2(index_val,&quot;ln_ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_train_spde=RMSE(index_train,&quot;ln_ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_val_spde=RMSE(index_val,&quot;ln_ingpe&quot;,segmento_sh_subset@data,M_fit_spde) a &lt;- list( x = 4, y = 6, text = paste(&quot;R2 training set:&quot;,round(r2_train_spde*100),&quot;%&quot;, &quot;\\nR2 validation set:&quot;,round(r2_val_spde*100),&quot;%&quot;, &quot;\\n&quot;, &quot;\\nRMSE training set:&quot;,round(rmse_train_spde,1),&quot;log USD&quot;, &quot;\\nRMSE validation set:&quot;,round(rmse_val_spde,1),&quot;log USD&quot;), xref = &quot;x&quot;, yref = &quot;y&quot;, showarrow = F) Figure 4.7: Observed versus predicted income values, SPDE model with Gaussian likelihood and Log-transformed income values # Save the results income_spde_naive_gaussian=list(&quot;outcome&quot;=&quot;ingpe&quot;, &quot;spat_dep&quot;=&quot;spde&quot;, &quot;cov_select&quot;=&quot;naive&quot;, &quot;formula&quot;=formula_test, &quot;family&quot;=&quot;gaussian&quot;, &quot;data&quot;=join.stack, &quot;fit&quot;=M_fit_spde, &quot;index_val&quot;=index_val, &quot;index_train&quot;=index_train, &quot;r2_val&quot;=r2_val_spde, &quot;r2_train&quot;=r2_train_spde, &quot;RMSE_val&quot;=rmse_val_spde, &quot;RMSE_train&quot;=rmse_train_spde) save(income_spde_naive_gaussian, file=paste0(dir_data, &quot;out/results/income_spde_naive_gaussian.RData&quot;)) 4.4 K-fold Cross-Validation The split of the observations between the training and validation sets has an important impact on the model’s goodness of fit. Indeed, the model could fit the training set observations very well but the validations set observations so poorly as to fit them purely by chance. K-fold cross-validation is a way to assess the results’ robustness to various splits of the data. In K-fold cross-validation, the data a split in K folds, and each fold is used only once for validation purposes and \\(K-1\\) times for training purposes. At the end of the validation process, one can assess how the goodness-of-fit statistics vary and the stastistics’ average value. A typical value for K is 10. The 10 folds are created as follows: set.seed(1) spec = c(val1 = .1, val2 = .1, val3 = .1, val4 = .1, val5 = .1, val6 = .1, val7 = .1, val8 = .1, val9 = .1, val10 = .1) g = sample(cut( seq(nrow(data_model)), nrow(data_model)*cumsum(c(0,spec)), labels = names(spec) )) We then fit the model 10 times, using each fold as a validation set once. r2_train_spde_a=r2_val_spde_a=rmse_train_spde_a=rmse_val_spde_a=c() # Prepare empty array to store goodness-of-fit statistics for(k in 1:10){ # Define the index index_val=which(g==paste0(&quot;val&quot;,k)) index_train=which(g!=paste0(&quot;val&quot;,k)) # Define the weights A.train &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_train,]) A.val &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_val,]) # Build the stack covariates_list=c(&quot;AREA_ID&quot;,&quot;lights_med&quot;,&quot;pop_dens&quot;,&quot;slope&quot;,&quot;chirps_ev_2017&quot;,&quot;dist2pubamen&quot;,&quot;dist2road&quot;) stack.train &lt;- inla.stack(data = list(pred=segmento_sh_subset@data$ingpe[index_train]), # This output variable (income) A = list(A.train, 1,1), effects = list(s.index, Intercept=1:length(index_train), segmento_sh_subset@data[index_train,c(covariates_list)]), tag=&quot;train&quot;) # The tag allows selected statistics for the desired sample to be extracted later stack.val &lt;- inla.stack(data = list(pred=NA), # Set it to NA A = list(A.val, 1,1), effects = list(s.index, Intercept=1:length(index_val), segmento_sh_subset@data[index_val,c(covariates_list)]), tag=&quot;val&quot;) join.stack &lt;- inla.stack(stack.train, stack.val) # Write the test formula #### formula_test =pred~ -1+Intercept+ AREA_ID+lights_med+pop_dens+slope+chirps_ev_2017+dist2pubamen+dist2road+ f(spatial.field, model=SV.spde) # Fit the model spde_res=inla(formula_test, # formula data=inla.stack.data(join.stack), family=&quot;gamma&quot;, # likelihood of the data control.predictor=list(A=inla.stack.A(join.stack), link=1,compute=T)) end_time=Sys.time() # Extract the fitted values index_inla_train = inla.stack.index(join.stack,&quot;train&quot;)$data index_inla_val = inla.stack.index(join.stack,&quot;val&quot;)$data results.train=spde_res$summary.fitted$mean[index_inla_train] results.val=spde_res$summary.fitted$mean[index_inla_val] M_fit_spde=array(NA,length(M_fit)) M_fit_spde[index_train]=results.train M_fit_spde[index_val]=results.val # Compute goodness-of-fit statistics r2_train_spde=pseudo_r2(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) r2_val_spde=pseudo_r2(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_train_spde=RMSE(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_val_spde=RMSE(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) # Store goodness-of-fit statistics in an array r2_train_spde_a=c(r2_train_spde_a,r2_train_spde) r2_val_spde_a=c(r2_val_spde_a,r2_val_spde) rmse_train_spde_a=c(rmse_train_spde_a,rmse_train_spde) rmse_val_spde_a=c(rmse_val_spde_a,rmse_val_spde) # Print status # Print(paste(k, &quot;folds done&quot;)) } print(paste(&quot;Average R2 in the validation set:&quot;, round(mean(r2_val_spde_a)*100),&quot;%&quot;, &quot;\\nAverage R2 in the training set:&quot;, round(mean(r2_train_spde_a)*100),&quot;%&quot;)) ## [1] &quot;Average R2 in the validation set: 44 % \\nAverage R2 in the training set: 48 %&quot; The average \\(R^{2}\\) of the validation and training sets (respectively 41 percent and 47 percent) are in line with the results obtained with the original split. References "],
["stepwise.html", "Section 5 Stepwise Covariates Selection 5.1 Step 1: Split the Sample into 60-20-20 Training, Validation, and Test Sets 5.2 Steps 2 to 4: Create a Function 5.3 Steps 5 to 7: Run a Parallel Loop 5.4 Step 8: Inspect the Results and Select a Specification 5.5 Step 9: Conduct a Final Test", " Section 5 Stepwise Covariates Selection This section describes a backward stepwise covariates selection process, which reduces the number of covariates from the 50 potential ones identified in section 3 to about 10 or fewer. Here is an overview of the process: Split the sample into 60-20-20 training, validation, and test sets. Write the test formula without one of the 50 candidate covariates. Fit the model on the training set. Validate the model on the validation set and compute the goodness-of-fit statistics. Repeat steps two to four once for each covariate (50 times). Drop the covariate in the absence of which the fit is the best;. Repeat steps two to six until no covariate is left. Inspect the results to identify a parsimonious specification (typically below 10 covariates) yielding a good fit on the validation set. Test the final model on the test set. The backward stepwise covariates selection process is computationally expensive. Fitting one modified BYM model takes about 3.5 minutes on a standard PC.3 Carrying out steps 1 to 5 takes about 175 minutes. Steps 6 and 7 take about 74 hours. To speed up the process, we write a function to parallelize step 5 across CPU cores. As the number of cores might be relatively limited on a PC (typically up to eight), an option is to carry out the process on the cloud, where an instance with more cores can be rented. For instance, the computing time can be reduced to below two hours with 64 cores. The backward stepwise covariates selection is presented with the income model. 5.1 Step 1: Split the Sample into 60-20-20 Training, Validation, and Test Sets The observations in the dataset are split into 60-20-20 training, validation, and test sets. The model will be fitted on the training set, and income will be predicted on the training and validation sets. Predicted values in the validation set will be compared with the observed values to assess the model’s goodness of fit. Once the covariate selection process is complete and a given specification has been chosen (steps 1 to 8 above), predictions are made on the final test set to assess the final model’s goodness of fit. # Sample segmentos for training, validation, and testing #### set.seed(1234) spec = c(train = .6, test = .2, validate = .2) g = sample(cut( seq(nrow(data_model)), nrow(data_model)*cumsum(c(0,spec)), labels = names(spec) )) index_val=which(g==&quot;validate&quot;) index_train=which(g==&quot;train&quot;) index_test=which(g==&quot;test&quot;) data_model$pred=data_model$ingpe data_model$pred[c(index_val,index_test)] &lt;- NA # Set the validation prediction to NA: The values will be predicted by the model and compared with observed values mod_data_jack = data_model # Store the data into a new data frame for model fitting mod_data_jack=mod_data_jack[c(index_val,index_train),] # Select only the training and validation sets; keep test set of final validation index_val_jack=which(is.na(mod_data_jack$pred)) index_train_jack=which(is.na(mod_data_jack$pred)==F) 5.2 Steps 2 to 4: Create a Function To make reading the code easier, we create a function INLA_steps_2_4 that implements steps 2 to 4 of the covariate selection process. The steps in INLA_steps_2_4 are Write the test formula without one of the candidate covariates. Fit the model on the training set. Validate the model on the validation set. Compute the goodness-of-fit statistics. The INLA_steps_2_4 accepts the following arguments: covariates formulation, index of the covariate to be removed, index of the validation set, index of the training set, data frame for the model, a string defining the outcome variable (e.g., “ingpe”), and a string defining the likelihood function (e.g., a Gaussian, gamma, or beta distribution). # INLA_set2_4_function ##### INLA_steps_2_4=function(covariates_formulation, # The formulation of each covariate ix, # The index of the covariate to be removed index_val, # The index of the validation set index_train, # The index of the training set mod_data_jack, # The data for the model outcome, # The string defining the outcome variable, such as ingpe family){ # The string defining the likelihood # Formula candidate_covariates &lt;- covariates_formulation formula_test &lt;- reformulate(c(&quot;-1&quot;, &quot;intercept&quot;, paste(candidate_covariates[-ix],collapse=&quot;+&quot;), &#39;f(ID,model = &quot;bym2&quot;, hyper=list( prec = list(prior = &quot;pc.prec&quot;, param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, 0.5))), graph = Segmento.Inla.nb, scale.model = TRUE, constr = T, adjust.for.con.comp = T)&#39;), &quot;pred&quot;) # Rescale to allow the Newton-Raphson optimizer to converge if(family==&quot;gaussian&quot;){ formula_test &lt;- reformulate(c(&quot;-1&quot;, &quot;intercept&quot;, paste(candidate_covariates[-ix],collapse=&quot;+&quot;), &#39;f(ID,model = &quot;bym2&quot;, hyper=list( prec = list(prior = &quot;pc.prec&quot;, param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, 0.5))), graph = Segmento.Inla.nb, scale.model = TRUE, constr = T, adjust.for.con.comp = T)&#39;), &quot;pred/10&quot;) # Rescale to allow the Newton-Raphson optimizer to converge } # Fit the model if(family%in%c(&quot;beta&quot;,&quot;zeroinflatedbinomial1&quot;)){ bym.res=INLA::inla(formula=formula_test, family = family, data = mod_data_jack, Ntrials = mod_data_jack$n_obs, control.predictor=list(link=1,compute=T), control.compute=list(dic=T, cpo=F), control.inla =list(int.strategy = &quot;eb&quot;) ) }else{ bym.res=INLA::inla(formula=formula_test, family = family, data = mod_data_jack, control.predictor=list(link=1,compute=T), control.compute=list(dic=T, cpo=F), control.inla =list(int.strategy = &quot;eb&quot;) ) } # Extract the fitted values M_fit=bym.res$summary.fitted.values[,&quot;mean&quot;] if(family%in%c(&quot;gamma&quot;)){ M_fit=exp(M_fit) # Back transform to level } if(family%in%c(&quot;gaussian&quot;)){ M_fit=M_fit*10 # Back transform to level } # The function for RMSE RMSE=function(set,outcome,data){ res = data[set,outcome]-M_fit[set] RMSE_val &lt;- sqrt(mean(res^2,na.rm=T)) return(RMSE_val) } # The function for pseudo_r2 pseudo_r2=function(set,outcome,data){ res = data[set,outcome]-M_fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(M_fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } # RMSE: RMSE function defined above RMSE_val=RMSE(index_val,outcome,mod_data_jack) RMSE_train=RMSE(index_train,outcome,mod_data_jack) # R2: Pseudo_r2 function defined above r2_val=pseudo_r2(index_val,outcome,mod_data_jack) r2_train=pseudo_r2(index_train,outcome,mod_data_jack) # Store the results results_list=list(&quot;cov_i&quot;=ix, &quot;cov_name&quot;=candidate_covariates[ix], &quot;formula&quot;=formula_test, # &quot;fitted_values&quot;=M_fit, # &quot;index_val&quot;=index_val, # &quot;index_train&quot;=index_train, # &quot;index_train_val&quot;=index_train_val, &quot;RMSE_val&quot;=RMSE_val, &quot;RMSE_train&quot;=RMSE_train, &quot;r2_val&quot;=r2_val, &quot;r2_train&quot;=r2_train, &quot;outcome&quot;=outcome, &quot;family&quot;=family) rm(bym.res) return(results_list) } We can now test the function. # Test the function ##### test_fct=INLA_steps_2_4(covariates_formulation, # The formulation of each covariate 1, # The index of the covariate to be removed index_val_jack, # The index of the validation set index_train_jack, # The index of the training set mod_data_jack, # The data for the model &quot;ingpe&quot;, # The outcome &quot;gaussian&quot;) # The likelihood The results can then be accessed using the $ sign. For instance, we can look at the \\(R^2\\): print(paste(&quot;R-squared in the validation set:&quot;,test_fct$r2_val, &quot;R-squared in the training set:&quot;,test_fct$r2_train)) ## [1] &quot;R-squared in the validation set: 0.412553293719591 R-squared in the training set: 0.511180089748186&quot; 5.3 Steps 5 to 7: Run a Parallel Loop We now launch the entire backward selection process with a loop. To speed up the process, we parallelize the estimation of every single model across the cores: each core is in charge of estimating one model. In the first iteration of the loop, we start with a list of \\(50\\) candidate covariates, and \\(50\\) models are estimated. Each model contains all covariates minus one \\(j\\) covariate, where \\(j\\) is different in each model and \\(j=1, ..., 50\\). Once the \\(50\\) models have been estimated, the results are collected, and the best-performing model is identified, say, for example, model k. The corresponding \\(k\\) covariate is removed from the list of candidate covariate and the loop proceeds to the next iteration with a list of \\(50-1=49\\) candidate covariates. # Loop all covariates until only one is left ##### candidate_covariates=covariates_formulation # Start again with the full list of candidate covariate n2drop=length(covariates_formulation)-1 start_t=Sys.time() results_jacknife=list() # Create a list to store all the results of the jacknife selectio process for(n_cov_to_drop in 1:n2drop){ # Repeat the step n2drop=12 times until having only 8 covariates set.seed(101) INLA_steps_2_4_par=function(cov_n){ # Fit the model by dropping one covariate after the other model_fct=INLA_steps_2_4(candidate_covariates, # The formulation of each covariate cov_n, # The index of the covariate to be removed index_val_jack, # The index of the validation set index_train_jack, # The index of the validation set mod_data_jack, # The data for the model &quot;ingpe&quot;, # The outcome &quot;gaussian&quot;) # The likelihood cat(&quot;\\014&quot;) # Clean the consol # end_t=Sys.time() # I think uration=end_t-start_t # Print(duration) return(model_fct) } cov_n&lt;-1:length(candidate_covariates) n_cores&lt;-detectCores() cl=makeCluster(n_cores) start_T&lt;-Sys.time() clusterExport(cl=cl, varlist=c(&quot;candidate_covariates&quot;, &quot;INLA_steps_2_4&quot;, &quot;index_val_jack&quot;, &quot;index_train_jack&quot;, &quot;mod_data_jack&quot;, &quot;Segmento.Inla.nb&quot;)) results_list&lt;-clusterApply(cl,cov_n,INLA_steps_2_4_par) end_T&lt;-Sys.time() cat(&quot;\\014&quot;) # Clean the consol print(end_T-start_T) stopCluster(cl) gc() # Store all the results for a quality check results_jacknife[[n_cov_to_drop]]=results_list # Drop the covariate that least affects the goodness of fit rmse_val=lapply(results_list,function(x) unlist(x$RMSE_val)) rmse_val_min=which.min(rmse_val) covariates_to_be_removed=results_list[[rmse_val_min]]$cov_name candidate_covariates=candidate_covariates[-which(candidate_covariates==covariates_to_be_removed)] rm(results_list) print(paste(length(candidate_covariates),&quot;covariates remaining&quot;)) } end_t=Sys.time() duration=end_t-start_t print(duration) # 1.84 hours The process took slightly less than 2 hours using 64 cores on the cloud. 5.4 Step 8: Inspect the Results and Select a Specification We now inspect the results of the selection process. Look at the goodness of fit of the best-performing models at each covariate selection step using the R-squared and the RMSE as goodness-of-fit statistics. # INCOME BYM-2 #### load(paste0(dir_data,&quot;workspace/income_bym2_gaussian.RData&quot;)) # Visualize the R-squared #### # Extract the R-squared r2_val_max=r2_train_max=r2_val_min=r2_train_min=c() for(k in 1:length(results_jacknife)){ r2_val=lapply(results_jacknife[[k]],function(x) unlist(x$r2_val)) r2_train=lapply(results_jacknife[[k]],function(x) unlist(x$r2_train)) # Get the maximum r2 r2_val_max_index=which.max(r2_val) r2_val_max_i=r2_val[[r2_val_max_index]] r2_train_max_i=r2_train[[r2_val_max_index]] r2_val_max=c(r2_val_max,r2_val_max_i) r2_train_max=c(r2_train_max,r2_train_max_i) # Get the minimum r2 r2_val_min_index=which.min(r2_val) r2_val_min_i=r2_val[[r2_val_min_index]] r2_train_min_i=r2_train[[r2_val_min_index]] r2_val_min=c(r2_val_min,r2_val_min_i) r2_train_min=c(r2_train_min,r2_train_min_i) } # Visualize the r2 data2plot=data.frame(set=c(rep(&quot;val&quot;,length(r2_val_max)),rep(&quot;train&quot;,length(r2_val_max))), index=c(rev(1:length(r2_val_max)),rev(1:length(r2_val_max))), index_jack=c(1:length(r2_val_max),1:length(r2_val_max)), r2_max=c(unlist(r2_val_max),unlist(r2_train_max)), r2_min=c(unlist(r2_val_min),unlist(r2_train_min))) plotly::plot_ly(data=data2plot, x=~index_jack, y=~r2_max, name = ~set, type=&quot;scatter&quot;, mode=&quot;line&quot;) # Visualize the RMSE #### # Extract the RMSE RMSE_val_max=RMSE_train_max=RMSE_val_min=RMSE_train_min=c() for(k in 1:length(results_jacknife)){ RMSE_val=lapply(results_jacknife[[k]],function(x) unlist(x$RMSE_val)) RMSE_train=lapply(results_jacknife[[k]],function(x) unlist(x$RMSE_train)) # Get the maximum RMSE RMSE_val_max_index=which.max(RMSE_val) RMSE_val_max_i=RMSE_val[[RMSE_val_max_index]] RMSE_train_max_i=RMSE_train[[RMSE_val_max_index]] RMSE_val_max=c(RMSE_val_max,RMSE_val_max_i) RMSE_train_max=c(RMSE_train_max,RMSE_train_max_i) # Get the minimum RMSE RMSE_val_min_index=which.min(RMSE_val) RMSE_val_min_i=RMSE_val[[RMSE_val_min_index]] RMSE_train_min_i=RMSE_train[[RMSE_val_min_index]] RMSE_val_min=c(RMSE_val_min,RMSE_val_min_i) RMSE_train_min=c(RMSE_train_min,RMSE_train_min_i) } # Visualize the RMSE data2plot=data.frame(set=c(rep(&quot;val&quot;,length(RMSE_val_max)),rep(&quot;train&quot;,length(RMSE_val_max))), index=c(rev(1:length(RMSE_val_max)),rev(1:length(RMSE_val_max))), RMSE_max=c(unlist(RMSE_val_max),unlist(RMSE_train_max)), RMSE_min=c(unlist(RMSE_val_min),unlist(RMSE_train_min))) plotly::plot_ly(data=data2plot, x=~index, y=~RMSE_min, name = ~set, type=&quot;scatter&quot;, mode=&quot;line&quot;) Based on the results above, the 31st iteration appears to provide a good balance between the sparsity of the formulation and the goodness of fit. # Identify the specification #### step_chosen=31 r2_val=lapply(results_jacknife[[step_chosen]],function(x) unlist(x$r2_val)) r2_val_max_index=which.max(r2_val) formula_selected=results_jacknife[[step_chosen]][[r2_val_max_index]]$formula formula_selected ## pred/10 ~ -1 + intercept + chirps_ev_med + lights_med + dist2coast_r + ## pop_dens + lc_tree + dens_all_roads + dens_secondary + dens_bus + ## LZ_CentralIndusServ + f(ID, model = &quot;bym2&quot;, hyper = list(prec = list(prior = &quot;pc.prec&quot;, ## param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, ## 0.5))), graph = Segmento.Inla.nb, scale.model = TRUE, constr = T, ## adjust.for.con.comp = T) ## &lt;environment: 0x555a51da26c0&gt; Here are the covariates that were selected: chirps_ev_med: median precipitation lights_med: median lights over the year dist2coast_r: distance to coast pop_dens: population density lc_tree: percentage of trees dens_all_roads: road density (all road categories) dens_secondary: secondary road density dens_bus : bus-lanes density LZ_CentralIndusServ: Central Livelihood zones on industry and services 5.5 Step 9: Conduct a Final Test Lastly, we perform the final goodness-of-fit test on the test set. As a reminder, the test set contains 20 percent of the data we set aside before starting the selection process, meaning that the model has not yet seen them. We start by building the data frame for estimating the final model with the training and test sets only. # Build a data frame for the final test #### data_test=data_model data_test$pred=data_test$ingpe # Create prediction variables equal to ingpe, the outcome variable # data_test$pred[index_test] &lt;- NA # Set the test prediction to NA: the variables will be predicted by the model and compared with observed data_test$pred[c(index_test, index_val)] &lt;- NA # Set the test prediction to NA: the variables will be predicted by the model and compared with observed data_test=data_test[c(index_test,index_train),] index_train_bis=which(is.na(data_test$pred)==F) index_test_bis=which(is.na(data_test$pred)) a &lt;- list( x = 600, y = 100, text = paste(&quot;R2 training set:&quot;,round(r2_train*100),&quot;%&quot;, &quot;\\nR2 test set:&quot;,round(r2_test*100),&quot;%&quot;, &quot;\\n&quot;, &quot;\\nRMSE training set:&quot;,round(RMSE_train),&quot;USD&quot;, &quot;\\nRMSE test set:&quot;,round(RMSE_test),&quot;USD&quot;), xref = &quot;x&quot;, yref = &quot;y&quot;, showarrow = F) The estimation is done with the usual inla function: Figure 5.1: Observed versus Predicted Income Values, BYM 2 Model with Covariates Selected with the Backward Stepwise Selection Process Processor: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz, 16.0 GB RAM.↩︎ "],
["summary-of-resuls.html", "Section 6 Results summary 6.1 Goodness of Fit at the Segmento Level 6.2 Segmento-Level Map 6.3 Goodness of Fit at the Municipio Level", " Section 6 Results summary This section presents the results obtained with the models for the three development indicators: median income, literacy, and poverty rates. Goodness-of-fit statistics are presented both at the segmento level and at the municipio (municipal) level. In the latter case, the results are presented with models fitted on either the entire datasets or the 50 representative municipios only. Lastly, we present the map of predicted income, literacy, and poverty rates as well as the uncertainty maps. 6.1 Goodness of Fit at the Segmento Level Various models were tested for the three development indicators. Spatial dependence was modeled either with a BYM 2 or an SPDE model. The backward selection process was adopted, starting from the same set of covariates. For the income model, we tested both Gaussian and gamma likelihood functions. Both literacy and poverty are ratio data, bounded between 0 percent and 100 percent, so we used a beta model. As beta distributions cannot include 0 and 1, we added and subtracted \\(0.00001\\) to observations with a 0 or 1, respectively. We also tested both models for the goodness of fit of a Gaussian model. Additionally, we investigated the use of zero-inflated and zero-altered beta models for literacy and poverty. For poverty, we also tested two separate models for rural and urban segmentos, which did not yield any noticeable improvement in the model’s goodness of fit. We, therefore, do not present the codes and results for these more complex models. The model summaries follow: # Load the results #### dir_result=paste0(root_dir,project_dir,&quot;out/results/&quot;) results_list=dir(dir_result) for(i in 1:length(results_list)){ load(paste0(dir_result, results_list[i])) } results_names=gsub(&quot;.RData&quot;,&quot;&quot;,results_list) lines_r=list() i=1 for(i in 1:length(results_names)){ lines_r[[i]]=list(get(results_names[i])[[c(&quot;outcome&quot;)]], get(results_names[i])[[c(&quot;spat_dep&quot;)]], get(results_names[i])[[c(&quot;family&quot;)]], get(results_names[i])[[c(&quot;cov_select&quot;)]], get(results_names[i])[[c(&quot;r2_train&quot;)]], get(results_names[i])[[c(&quot;r2_val&quot;)]], get(results_names[i])[[c(&quot;r2_test&quot;)]], get(results_names[i])[[c(&quot;RMSE_train&quot;)]], get(results_names[i])[[c(&quot;RMSE_val&quot;)]], get(results_names[i])[[c(&quot;RMSE_test&quot;)]]) if(is.null(lines_r[[i]][[7]])){lines_r[[i]][[7]]=NA} if(is.null(lines_r[[i]][[10]])){lines_r[[i]][[10]]=NA} # print(length(lines_r[[i]])) } lines_df=do.call(rbind.data.frame, lines_r) names(lines_df)=c(&quot;Outcome&quot;,&quot;spatial dependency model&quot;,&quot;likelihood&quot;,&quot;covariate selection method&quot;, &quot;r2 training set&quot;,&quot;r2 validation set&quot;,&quot;r2 test set&quot;, &quot;RMSE training set&quot;,&quot;RMSE validation set&quot;,&quot;RMSE test set&quot;) Table 8.1 summarizes the RMSE and r-squared for the 10 models. lines_df[,which(unlist(lapply(lines_df,is.numeric)))]= round(lines_df[,which(unlist(lapply(lines_df,is.numeric)))],digit=2) caption_table=&#39;Summary of Goodness of Fit at the Segmento Level&#39; knitr::kable( lines_df, format=&quot;markdown&quot;, caption = caption_table, booktabs = TRUE, row.names = FALSE) Outcome spatial dependency model likelihood covariate selection method r2 training set r2 validation set r2 test set RMSE training set RMSE validation set RMSE test set illiteracy bym2 beta naive 0.65 0.43 NA 0.06 0.08 NA illiteracy_rate spde beta naive 0.53 0.42 NA 0.07 0.08 NA illiteracy_rate spde beta stepwise 0.55 0.46 0.43 0.07 0.07 0.08 ingpe bym2 gamma naive 0.66 0.47 NA 42.25 63.76 NA ingpe bym2 gaussian stepwise 0.45 0.47 0.40 53.58 63.87 57.01 ingpe spde gaussian naive 0.54 0.53 NA 0.32 0.33 NA ingpe spde gamma naive 0.48 0.48 NA 51.95 63.27 NA ingpe spde gamma stepwise 0.50 0.51 0.49 50.30 62.06 52.79 pobreza_mod bym2 beta naive 0.62 0.25 NA 0.13 0.17 NA pobreza_mod spde beta naive 0.31 0.18 NA 0.17 0.18 NA pobreza_mod spde gaussian stepwise 0.31 0.30 0.24 0.17 0.18 0.19 pobreza_mod spde beta stepwise 0.29 0.28 0.20 0.17 0.18 0.19 For income, the best model is the SPDE with a gamma likelihood function where stepwise covariate selection has been applied. The goodness of fit is relatively high (50 percent) and holds in the testing set. For literacy, the three tested models yield results that are very close in terms of the goodness of fit. We select the SPDE with stepwise covariate selection, with a goodness of fit of 43 percent, as the final model. However, the goodness of fit on the model on moderate poverty is only 23 percent. We select the Gaussian with SPDE and stepwise selection as the final model. ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields Figures 6.1, 6.2, and 6.3 show the predicted and observed values along with the \\(R^{2}\\) and the RMSE. plotly::plot_ly(y=income_spde_stepwise$fit[income_spde_stepwise$index_train], x=ehpm17_predictors_nona$ingpe[income_spde_stepwise$index_train], type=&quot;scatter&quot;, mode=&quot;markers&quot;, name=&quot;training set&quot;, marker = list(color = &#39;black&#39;, opacity = 0.3))%&gt;% plotly::add_trace(y=income_spde_stepwise$fit[income_spde_stepwise$index_test], x=ehpm17_predictors_nona$ingpe[income_spde_stepwise$index_test], mode=&quot;markers&quot;, name=&quot;test set&quot;, marker = list(color = &#39;red&#39;, opacity = 0.3))%&gt;% plotly::add_trace(y=c(0,880), x=c(0,880), type=&quot;scatter&quot;, mode=&quot;lines&quot;, name=&quot;1:1&quot;)%&gt;% plotly::layout(yaxis=list(range=c(0,510),title=&quot;Predicted (USD)&quot;), xaxis=list(range=c(0,880),title=&quot;Observed (USD)&quot;), annotations=a, title=&quot;Goodness of Fit: Income at the Segmento Level&quot;) ## A marker object has been specified, but markers is not in the mode ## Adding markers to the mode... Figure 6.1: Predicted versus Observed Income (USD), SPDE Model Stepwise Selection Figure 6.2: Predicted versus Observed Median Income (USD), SPDE Model Stepwise Selection Figure 6.3: Predicted versus Observed Moderate Poverty (%), SPDE Model Stepwise Selection 6.2 Segmento-Level Map The detailed process to produce the maps is shown for income with the SPDE model. Codes are not shown for literacy and poverty. So far, predictions have been made on only the segmentos for which EHPM data are available, which is 1,664 segmentos. To complete the map, predictions have to be made on all of El Salvador’s 12,435 segmentos. We prepare two data stacks: one covers the 1,664 EHPM segmentos and the other, the remaining 10,771 with no available EHPM data. For the latter segmentos, we do have the covariates data. The model will be trained on the EHPM segmentos, and out-of-sample prediction will be made on the non-EHPM segmentos. load(paste0(dir_data,&quot;workspace/income_spde_gamma.RData&quot;)) data_model=segmento_sh_data_model@data index_ehpm=which(is.na(data_model$ingpe)==F) index_NON_ehpm=which(is.na(data_model$ingpe)) coords.ehpm=coordinates(segmento_sh_subset) segmento_non_ehpm=subset(segmento_sh_data, is.na(segmento_sh_data$ingpe)) coords.non.ehpm=coordinates(segmento_non_ehpm) A.ehpm &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords.ehpm) A.non.ehpm &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords.non.ehpm) stack.ehpm= inla.stack(data = list(pred=data_model$ingpe[index_ehpm]), A = list(A.ehpm, 1,1), effects = list(s.index, Intercept=rep(1,length(index_ehpm)), data_model[index_ehpm, as.character(cov_candidates_selected_table$Candidates)]), tag=&quot;ehpm&quot;) stack.non.ehpm= inla.stack(data = list(pred=data_model$ingpe[index_NON_ehpm]), A = list(A.non.ehpm, 1,1), effects = list(s.index, Intercept=rep(1,length(index_NON_ehpm)), data_model[index_NON_ehpm, as.character(cov_candidates_selected_table$Candidates)]), tag=&quot;non_ehpm&quot;) join.stack &lt;- inla.stack(stack.ehpm, stack.non.ehpm) The model is now fitted with the usual inla command. start_time=Sys.time() M_map =inla(formula_selected, data=inla.stack.data(join.stack, spde=SV.spde), family=&quot;gamma&quot;, control.predictor=list(A=inla.stack.A(join.stack), link=1,compute=T), control.compute=list(cpo=F, dic=F)) end_time=Sys.time() print(end_time-start_time) ## Time difference of 15.65245 secs The fitted values are accessed using the inla.stack.index introduced in previous sections. We also extract the standard deviation of the prediction to map the uncertainty of the prediction. The latter is summarized by the coefficient of variation. For any given segmento \\(s\\), the coefficient of variation is given as \\[ \\begin{aligned} coef var_{s}=\\hat{\\sigma_{s}}/\\hat{\\mu_{s}} \\end{aligned} \\] where \\(\\hat{\\sigma_{s}}\\) is the standard deviation of the prediction for segmento \\(s\\) and \\(\\hat{\\mu_{s}}\\) is the average predicted income for segmento \\(s\\). The coefficient of variation is the standard deviation of the prediction expressed as a percentage of the prediction. index_inla_ehpm = inla.stack.index(join.stack,&quot;ehpm&quot;)$data index_inla_non_ehpm = inla.stack.index(join.stack,&quot;non_ehpm&quot;)$data M_map_fit=M_map$summary.fitted.values[,&quot;mean&quot;] M_map_fit_ehpm=M_map$summary.fitted.values[index_inla_ehpm,&quot;mean&quot;] M_map_fit_non_ehpm=M_map$summary.fitted.values[index_inla_non_ehpm,&quot;mean&quot;] M_map_sd_ehpm=M_map$summary.fitted.values[index_inla_ehpm,&quot;sd&quot;] M_map_sd_non_ehpm=M_map$summary.fitted.values[index_inla_non_ehpm,&quot;sd&quot;] The fitted values, standard deviations, and coefficient of variations are now stored in the polygon data frame segmento_sh_data_model. segmento_sh_data_model@data$fit_income=NA segmento_sh_data_model@data$fit_income[index_ehpm]=M_map_fit_ehpm segmento_sh_data_model@data$fit_income[index_NON_ehpm]=M_map_fit_non_ehpm segmento_sh_data_model@data$sd_income=NA segmento_sh_data_model@data$sd_income[index_ehpm]=M_map_sd_ehpm segmento_sh_data_model@data$sd_income[index_NON_ehpm]=M_map_sd_non_ehpm segmento_sh_data_model@data$coefvar_income=segmento_sh_data_model@data$sd_income/segmento_sh_data_model@data$fit_income We can then save the spatial polygon data frame into a .geojson file for later inspection. segmento_wgs_income=sp::spTransform(segmento_sh_data_model, &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) segmento_wgs_income@data=segmento_wgs_income@data%&gt;% select(SEG_ID,AREA_ID,fit_income,ingpe,sd_income,coefvar_income,lights_med,pop_dens,slope,n_obs) rgdal::writeOGR(segmento_wgs_income_sp, paste0(dir_data, &quot;out/map/segmentos_spde_income2.geojson&quot;), driver = &quot;GeoJSON&quot;, layer = 1, overwrite_layer=T) Figure 6.4 shows the map of the predicted income with a continuous color scale as well as the map of prediction uncertainty.4 Figure 6.4: Median Income (USD): Continuous Color Scale The prediction uncertainty is relatively low. In most cases, the coefficient of variation indicates that the standard deviation is smaller than 10 percent of the estimates. Figure 6.5 shows the map of the predicted income with a quantile color scale, highlighting the rural-urban difference. Figure 6.5: Median Income (USD): Quantile Color Scale Figure 6.6 shows predicted illiteracy with a continuous color scale and uncertainty, while Figure 6.7 shows predicted illiteracy with a quantile color scale. Figure 6.6: Illiteracy (%): Continuous Color Scale Figure 6.7: Illiteracy (%): Quantile Color Scale Lastly, Figure 6.8 and Figure 6.9 show the results for moderate poverty. Figure 6.8: Moderate Poverty (%): Continuous Color Scale Figure 6.9: Moderate Poverty (%): Quantile Color Scale 6.3 Goodness of Fit at the Municipio Level We now adopt two methods to investigate the goodness of fit at the municipio level. First, we aggregate the results at the municipio level and compare them with municipio-level statistics. An issue with this approach is that in each municipio, some segmentos are used in the training and validation sets. Furthermore, EHPM data are representative at the municipio level only for a subset of municipios. Therefore, the difference between the fitted and observed values will be affected by both modeling and sampling errors, and it will be hard to gauge the external validity of the model because the training and validation sets are mixed. To remedy this, we adopt a second approach. We select the segmentos from representative municipios only and split this remaining dataset into training and validation sets, making sure that all segmentos from any given municipio fall in either the training or the validation set. We can then obtain goodness-of-fit statistics at the municipio level for observations that have not been used to train the model. 6.3.1 Option 1: Aggregate the Results at the Municipio Level Using All Segmentos First, we compute the average median income level per segmento on the entire sample at the municipio level. We then compute the average prediction of the test set at the municipio level. Finally, we compare the observed average income against the predicted average income at the municipio level, for both the representative and non-representative municipios. # Reload the shape source(&quot;../utils.R&quot;) segmento_sh=rgdal::readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Segmentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields ehpm17_predictors=read.csv(paste0(dir_data, &quot;out/all_covariates_and_outcomes.csv&quot;)) ehpm17_predictors=ehpm17_predictors%&gt;% mutate(SEG_ID=as.character(SEG_ID), SEG_ID=ifelse(nchar(SEG_ID)==7, paste0(0,SEG_ID), SEG_ID)) # Extract the municipio identifier from the shapefile segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% # dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) # Identify instances of EHPM data segmento_sh_subset=subset(segmento_sh_data, is.na(segmento_sh_data@data$ingpe)==F) segmento_sh_subset$fit=NA segmento_sh_subset$fit[income_spde_stepwise$index_train]=income_spde_stepwise$fit[income_spde_stepwise$index_train] segmento_sh_subset$fit[income_spde_stepwise$index_val]=income_spde_stepwise$fit[income_spde_stepwise$index_val] segmento_sh_subset$fit[income_spde_stepwise$index_test]=income_spde_stepwise$fit[income_spde_stepwise$index_test] data_municipio=segmento_sh_subset@data%&gt;% select(SEG_ID,fit,ingpe)%&gt;% left_join(segmento_sh_data@data%&gt;% select(SEG_ID,DEPTO,MPIO)%&gt;% mutate(DEPTO_MPIO=paste0(DEPTO,MPIO))%&gt;% select(-c(DEPTO,MPIO)), by=&quot;SEG_ID&quot;) # Extract the representative information from the EHPM ehpm=read.csv(paste0(dir_data, &quot;tables/ehpm-2017.csv&quot;)) segID=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;), sheet=&quot;2017&quot;) ehpm_municauto=ehpm%&gt;% select(r005,autorrepresentado,idboleta)%&gt;% left_join(segID, by=&quot;idboleta&quot;)%&gt;% rename(&quot;SEG_ID&quot;=&quot;seg_id&quot;)%&gt;% group_by(SEG_ID)%&gt;% summarise(autorrepresentado=mean(autorrepresentado)) # Add representativity indicator to results data frame data_municipio_rep=data_municipio%&gt;% left_join(ehpm_municauto, by=&quot;SEG_ID&quot;) # Aggregate the results at the municipio level #### # All data_municipio_all=data_municipio_rep%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(obs=mean(ingpe,na.rm=T), fit=mean(fit,na.rm=T))%&gt;% mutate(representative=&quot;all&quot;) # Non-representative data_municipio_NONREP=data_municipio_rep%&gt;% filter(autorrepresentado==0)%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(obs=mean(ingpe,na.rm=T), fit=mean(fit,na.rm=T))%&gt;% mutate(representative=&quot;no&quot;) # Representative data_municipio_REP=data_municipio_rep%&gt;% filter(autorrepresentado==1)%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(obs=mean(ingpe,na.rm=T), fit=mean(fit,na.rm=T))%&gt;% mutate(representative=&quot;yes&quot;) # Bind the data frame data_municipio=data_municipio_REP%&gt;% bind_rows(data_municipio_NONREP)%&gt;% bind_rows(data_municipio_all) index_rep=which(data_municipio$representative==&quot;yes&quot;) index_non_rep=which(data_municipio$representative==&quot;no&quot;) index_all=which(data_municipio$representative==&quot;all&quot;) # Fit RMSE=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RMSE_val &lt;- sqrt(mean(res[,&quot;obs&quot;]^2,na.rm=T)) return(RMSE_val) } pseudo_r2=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } r2_all=pseudo_r2(index_all,&quot;obs&quot;,data_municipio,data_municipio$fit) r2_rep=pseudo_r2(index_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) r2_non_rep=pseudo_r2(index_non_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) RMSE_all=RMSE(index_all,&quot;obs&quot;,data_municipio,data_municipio$fit) RMSE_rep=RMSE(index_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) RMSE_non_rep=RMSE(index_non_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) The results are plotted in Figure 6.10. Figure 6.10: Predicted versus Observed Median Income: Municipio Level, Entire Sample The results for literacy and poverty are shown in Figure 6.11 and Figure 6.12, respectively. Figure 6.11: Predicted versus Observed Illiteracy Rate: Municipio Level, Entire Sample Figure 6.12: Predicted versus Observed Moderate Poverty Rate: Municipio Level, Entire Sample The goodness of fit is much better for the representative municipios (in green) than the non-representative ones (in red). In the latter ones, the goodness of fit is affected by both the model and sampling errors. Furthermore, it is hard to take the goodness-of-fit statistics for the representative segmentos at face value, as they are based on a mix of training and validation data. We accordingly rerun the model below with only the segmentos in representative municipios. 6.3.2 Option 2: Aggregate the Results at the Municipio Level Using Segmentos in Representative Municipios Only We select only the segmentos in representative municipios and allocate the municipios to the training or validation set. This ensures that all segmentos from each municipio are in either the training or the validation set. There are 50 representative municipios in the EHPM totaling 1,114 segmentos. The only change compared to the code shown in the earlier sections is the creation of the training and validation sets. The process is Allocate the representative municipios into training and validation sets. Select the data for the segmentos in the representative municipios. Add the municipio training and validation set indexes to the segment-level data frame. # 1. Allocate the representative municipios into training and validation sets #### list_municipio=data_municipio_REP%&gt;% distinct(DEPTO_MPIO) set.seed(12345) spec = c(train = .8, validate = .2) g = sample(cut( seq(nrow(list_municipio)), nrow(list_municipio)*cumsum(c(0,spec)), labels = names(spec) )) list_municipio$index=g # 2. Select the data for the segmentos in the representative municipios #### segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% # dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) segmento_sh_data@data=segmento_sh_data@data%&gt;% mutate(DEPTO_MPIO=paste0(DEPTO,MPIO)) segmento_sh_subset_muni=subset(segmento_sh_data, is.na(segmento_sh_data@data$ingpe)==F&amp; segmento_sh_data@data$DEPTO_MPIO%in%list_municipio$DEPTO_MPIO) # identify where there is EHPM data and representative municipios # 3. Add the municipio training and validation set indexes to the segment-level data frame segmento_sh_subset_muni@data=segmento_sh_subset_muni@data%&gt;% left_join(list_municipio, by=&quot;DEPTO_MPIO&quot;) # Identify the indexes #### index_train=which(segmento_sh_subset_muni$index==&quot;train&quot;) index_val=which(segmento_sh_subset_muni$index==&quot;validate&quot;) The rest of the process is the same: we create the mesh, the SPDE, the matrix of weights, the stack, the formula (we used the naive one), fit the model, and extract the fitted values (code not shown). We then add the fitted values to the shapefile, aggregate the values at the municipio level, and compute separate goodness-of-fit statistics for the municipios in the training and validation sets. # Add fitted values into the shapefile #### segmento_sh_subset_muni@data$fit_income=NA segmento_sh_subset_muni@data$fit_income[index_train]=results.train segmento_sh_subset_muni@data$fit_income[index_val]=results.val # Aggregate data at the municipio level #### data_muni=segmento_sh_subset_muni@data%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(fit=mean(fit_income), obs=mean(ingpe), set=unique(index)) ## `summarise()` ungrouping output (override with `.groups` argument) # Goodness of fit #### RMSE=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RMSE_val &lt;- sqrt(mean(res[,&quot;obs&quot;]^2,na.rm=T)) return(RMSE_val) } pseudo_r2=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } index_train_muni=which(data_muni$set==&quot;train&quot;) index_val_muni=which(data_muni$set==&quot;validate&quot;) r2_train_income=r2_train=pseudo_r2(index_train_muni,&quot;obs&quot;,data_muni,data_muni$fit) r2_val_income=r2_val=pseudo_r2(index_val_muni,&quot;obs&quot;,data_muni,data_muni$fit) RMSE_train_income=RMSE_train=RMSE(index_train_muni,&quot;obs&quot;,data_muni,data_muni$fit) RMSE_val_income=RMSE_val=RMSE(index_val_muni,&quot;obs&quot;,data_muni,data_muni$fit) The results are plotted in Figure 6.13. Figure 6.13: Predicted versus Observed Median Income: Municipio Level, Representative Municipios Only The results for literacy and poverty are shown in Figure 6.14 and Figure @ref(fig: pobreza_mod-fit-municipio-2-chunk) respectively. Figure 6.14: Predicted versus Observed Illiteracy: Municipio Level, Representative Municipios Only (#fig:pobreza_mod-fit-municipio-2-chunk)Predicted versus Observed Moderate Poverty: Municipio Level, Representative Municipios Only The three models’ goodness of fit at the municipio level is very high, as summarized in Table 8.2. Outcome R-squared training (%) R-squared validation (%) RMSE training (USD) RMSE validation (USD) Median Income (USD) 87.34 83.59 19.20 15.99 Illiteracy (% of adults) 72.90 77.60 0.03 0.03 Moderate Poverty (% of adults) 58.64 60.44 0.05 0.06 An option would have been to create an interactive map with the package leaflet, but we chose to edit the map in QGIS and display it as an image to ease the rendering of the maps.↩︎ "],
["discussion.html", "Section 7 Discussion", " Section 7 Discussion This report introduces high-resolution mapping based on Bayesian geospatial techniques implemented in the open source statistical computing environment R (R Core Team 2018a). The core of the modeling method is implemented in the R package INLA (Rue, Martino, and Chopin 2009; Lindgren, Rue, and Lindström 2011; Martins et al. 2013; Lindgren and Rue 2015). The emphasis is on providing and explaining the R codes required to go from the raw data to the final maps. Three development indicators are mapped for El Salvador: median income, adult literacy, and poverty. The data on these development indicators come from the EHPM, conducted in 20,645 households among 1,664 segmentos in 2017. A suite of more than 20 RS and GIS open source data sets are used to derive a list of 86 potential predictors, hereafter covariates. After having standardized the covariates, we apply an unsupervised dimension-reduction process to reduce this number to 50. This limits the effect of multicollinearity in subsequent modeling and limits the risk of retaining covariates because of chance correlation in the subsequent covariates selection. Next, we apply a backward stepwise covariates selection process to select a parsimonious model specification. We tested several likelihood functions (Gaussian, gamma, and beta) and spatial dependence structures (the SPDE and BYM 2 models). The best results are obtained with the SPDE approach and, at the segmento level, yield a \\(R^2\\) on the final test set of 50 percent for income, 43 percent for illiteracy, and only 23 percent for poverty. Once the results are aggregated at the municipio level, the \\(R^2\\) increases to 84 percent for income, 80 percent for literacy, and 61 percent for poverty. We spent limited time exploring interaction terms and nonlinear relationships between covariates and outcome variables. Another avenue for amelioration would be to develop a spatiotemporal model that takes into account the time dimension. This is possible, as the EHPM is collected annually. A spatiotemporal model would permit the monitoring of trends in the socioeconomic and SDG indicators of interest. Lastly, other modeling architectures such as random forests or convolutional neural networks could take advantage of nonlinear effects, potentially increasing the predictive accuracy of the models. Another direction for potentially improving the granularity of socioeconomic maps lies in the use of mobile phone call-detail records. Covariate data derived from mobile phone call-detail records have been shown to correlate well with poverty indicators both at the household and aggregate level (e.g., Blumenstock, Cadamuro, and On 2015; Steele et al. 2017). These call records also have the advantage of being highly granular in terms of both space and time, presenting the possibility to potentially monitor variation in population well-being at a higher frequency. While the technical demands and requirements to negotiate access to call-detail-record data and conduct meaningful analyses with them are high, there are many potential applications for these data (e.g., mobility analysis, disaster response, and preparedness) and the marginal cost of each additional project is low once the agreements and the IT system are in place. A lot of the data preprocessing work and some of the analytical steps could be packaged and automatized in software. This could further streamline the process of creating these high-resolution maps of development indicators. High-resolution maps of development indicators may allow more accurate targeting of government interventions aimed at reducing poverty or increasing literacy rates among adults. Second, the methods presented here are portable to other socioeconomic and SDG indicators of interest. They could play a key role in monitoring and reporting on SDG achievements. Third, other than the survey’s segmento-level data, the methods described here are conducted using freely available data sources from freely available software. Fourth, the method presented here does not rely on the use of census data as do traditional small-area-estimation methods, so it can be applied independently of any census round. Fifth, the methods can be readily deployed by analysts with masters-level statistical training. These five points make the method very suitable for incorporation in routine and standard practices of national statistical offices (NSOs). The next possible step is to present these methods to NSOs to generate discourse on the value of these methods, potential improvements to increase their operational relevance, and implications for their integration into decision-making frameworks. We hope that the relative ease of these methods, the open-source nature of the software and covariate data, as well as the large number of code examples in this report (and on the web in general) will encourage their adoption by NSOs. Experts from the Flowminder Foundation are available to provide technical training and support to strengthen in-house capacity at NSOs to use these methods. This report could constitute the backbone of a short course of up to five days to equip a first cohort of statisticians with the know-how for confidently using these methods, data, and the R software for high-resolution mapping of development indicators. References "],
["theory.html", "Section 8 Appendix 1: Primer on INLA 8.1 Area Data versus Point-Referenced Data 8.2 Model Formulation", " Section 8 Appendix 1: Primer on INLA This section introduces high-resolution mapping based on Bayesian geostatistical techniques to allow for an intuitive understanding of the models—it does not offer an in-depth derivation and explanation of the theory underpinning them. This section is largely based on the books by Zuur et al. (Zuur, Ieno, and Saveliev 2017) and Blangiardo and Cameletti (Blangiardo and Cameletti 2015). 8.1 Area Data versus Point-Referenced Data There are three types of spatial data (Blangiardo and Cameletti 2015): Area or lattice data: the outcome variable to be modeled is a “random aggregate value over an areal unit”(Blangiardo and Cameletti 2015). The difference between area and lattice data is that the latter is aggregated over a regular grid while the former is aggregated over a set of irregular polygons such as administrative boundaries. For instance, income data aggregated at the segmento level are area data. Point-referenced (or geostatistical) data: the outcome variable to be modeled is “a random outcome at a specific location” (Blangiardo and Cameletti 2015), such as households’ income measured at the household location georeferenced with latitude and longitude coordinates. Spatial point patterns: the outcome variable to be modeled represents the occurrence or not of an event at a given location. While point-referenced data are random outcomes at a specific location, the location of the occurrence or not is itself random in spatial point patterns. For instance, road traffic accidents could be modeled as spatial point patterns. The EHPM data were collected at the household level but aggregated at the segmento level, so they are area data. Nevertheless, we will also explore a model assuming they are point-referenced data, using the centroids of the segmentos as the location. 8.1.1 Bayesian Approach In the frequentist approach, the interest lies in estimating the probability of the data, such as on income, given the parameters \\(P(data | \\beta)\\), that is, one assumes the parameters exist. In a Bayesian approach, the interest lies in estimating the probability of the parameters given the data, \\(P(\\beta | data)\\). The Bayes theorem states that: \\[ \\begin{aligned} P(A&amp;B)=P(A|B)P(B) \\end{aligned} \\] \\[ \\begin{aligned} P(A|B)=\\frac{P(B|A)P(A)}{P(B)} \\end{aligned} \\] Replacing \\(A\\) and \\(B\\) with the quantity of interest gives: \\[ \\begin{aligned} P(\\beta|data)=\\frac{P(data|\\beta)P(\\beta)}{P(data)} \\end{aligned} \\] where \\(P(\\beta|data)\\) is the posterior distribution of the parameter \\(\\beta\\), \\(P(data|\\beta)\\) is the likelihood function of the data (e.g., a gamma distribution for income), \\(P(\\beta)\\) is the prior distribution of the \\(\\beta\\) parameter. This can be expressed as: \\[ \\begin{aligned} P(\\beta|data)\\propto P(data|\\beta)P(\\beta) \\end{aligned} \\] which reads as the posterior distribution of \\(\\beta\\) is proportional to \\(P(data|\\beta)P(\\beta)\\). There are various strategies for getting \\(P(data|\\beta)P(\\beta)\\), such as a simulation with a Monte Carlo or a Markov Chain Monte Carlo algorithm, or a deterministic approach via Integrated Nested Laplace Approximation (INLA) implemented in the INLA package. 8.2 Model Formulation This section models the relationship between the outcome variable \\(y\\), for instance, segmento median income, and a set of covariates \\(\\mathbf{X}\\), with the bold notation denoting that \\(\\mathbf{X}\\) is a matrix with each column corresponding to a covariate. 8.2.1 A Simple Model That Does Not Account for Spatial Dependence Ignoring, for now, the spatial dimension, the relationship could be modeled as: \\[ \\begin{aligned} \\mathbf{y}=\\beta_{0}+\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\epsilon} \\end{aligned} \\] \\[ \\begin{aligned} \\mathbf{\\epsilon}\\sim N(0,\\mathbf\\Omega) \\end{aligned} \\] where \\(\\mathbf{y}\\) is an income vector of size \\(n\\) for the \\(n\\) segmentos. With each element \\(y_{i}\\) of \\(i=1, ... ,n\\) is the median income level in segmemto \\(i\\), \\(\\mathbf{X}\\) is the matrix of covariates with \\(k\\) columns for the \\(k\\) covariates and \\(n\\) rows for the \\(n\\) segmentos, \\(\\mathbf{\\beta}\\) is the vector of \\(k\\) parameters indicating the effect of each covariate on \\(y\\), and \\(\\mathbf{\\epsilon}\\) is the vector of error term of size \\(n\\), distributed normally with mean \\(0\\), and variance-covariance defined by the matrix \\(\\mathbf{\\Omega}\\). \\[ \\begin{aligned} \\mathbf\\Omega = \\left[\\begin{array} {rrrr} \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\\\ 0 &amp; \\sigma^2 &amp; \\dots &amp; \\vdots \\\\\\ \\vdots &amp; &amp;\\ddots &amp; 0 \\\\\\ 0 &amp; \\dots &amp; 0 &amp; \\sigma^2 \\end{array}\\right]=\\sigma^2\\mathbf{I} \\end{aligned} \\] Each off-diagonal element of \\(\\mathbf\\Omega\\) represents the covariance of \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\), here set to \\(0\\). The variance is \\(\\sigma^2\\), which can be more concisely written as \\(\\sigma^2\\mathbf{I}\\), where \\(\\mathbf{I}\\) is an identity matrix where the diagonal elements are \\(1\\) and the off-diagonal elements are \\(0\\). The strong assumption of this model is that the term \\(\\epsilon_{i}\\) is independently and identically distributed, often abbreviated as iid. Let us unpack this expression. Independence. The independence assumption implies that \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\) for any two pairs of segmento \\(i\\) and \\(j\\) will be independent, even if \\(i\\) and \\(j\\) are neighboring segmentos. This is represented by the covariance between \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\) for \\(i\\neq j\\) being set to zero in \\(\\Omega\\). We can expect that it does not hold in the case of average income per segmento. For instance, imagine that a large company is recruiting its workforce in segmentos \\(i\\) and \\(j\\) and no data is available in \\(\\mathbf{X}\\) to take this into account. Therefore, \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\) will be highly correlated, violating the independence assumption. Identical distribution. There is no symbol \\(i\\) in the normal distribution \\(N(\\mu,\\sigma^2)\\), meaning that \\(\\epsilon_{i}\\) of all segmentos are assumed to have a mean \\(\\mu\\) and a variance \\(\\sigma^2\\). However, both the mean and the variance might vary across locations. For instance, one can expect that in areas where most of the workforce is employed in agriculture, weather shocks might imply a larger deviation from model predictions than in areas where most of the workforce is employed in the service sector, leading to different variances. Similarly, the model might systematically overpredict or underpredict in some areas, leading to \\(\\epsilon_{i}\\) with \\(\\mu\\) higher or under zero. The violation of these assumptions leads to incorrect estimation of the precision of the \\(\\mathbf{\\beta}\\) parameters. Furthermore, explicitly modeling the spatial dependence increases the predictive performance of the model; not taking the location into account implies throwing away an important piece of information. 8.2.2 A Spatial Model To explicitly model the spatial dependence, an additional term is added to the equation. \\[ \\begin{aligned} \\mathbf{y}=\\beta_{0}+\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\epsilon}+\\mathbf{\\upsilon}, \\end{aligned} \\] where \\(\\mathbf{\\epsilon}\\) is distributed according to the equation presented in the previous subsection and \\(\\mathbf{\\upsilon}\\) is a spatially correlated random effect distributed as follows: \\[ \\begin{aligned} \\mathbf{\\upsilon}\\sim N(0,\\mathbf\\Sigma) \\end{aligned} \\] \\(\\Sigma\\) is a non-diagonal matrix \\[ \\begin{aligned} \\mathbf\\Sigma = \\left[\\begin{array} {rrrr} \\sigma_{u}^2 &amp;\\phi_{1,2}^2 &amp; \\dots &amp; \\phi_{1,n}^2 \\\\\\ \\vdots &amp; \\sigma_{u}^2 &amp; \\dots &amp; \\vdots \\\\\\ \\vdots &amp; &amp;\\ddots &amp; \\phi_{n-1,n-1}^2 \\\\\\ \\dots &amp; \\dots &amp; \\dots &amp; \\sigma_{u}^2 \\end{array}\\right] \\end{aligned} \\] with \\(\\sigma_{u}^2\\) as the variance of the \\(\\upsilon\\) and \\(\\phi_{i,j}=corr(\\upsilon_{i},\\upsilon_{j})\\neq0\\) is the correlation between the spatial random effect at locations \\(i\\) and \\(j\\). The challenge now is to estimate the \\(\\phi_{i,j}\\) parameters. In the present case, the matrix \\(\\mathbf\\Sigma\\) has a dimension of \\(1664*1664\\), that is, 2,768,896 parameters to estimate. 8.2.3 Estimating the Model with Areal Data When working with areal data, quantifying proximity with a Euclidean distance is not adequate. First, one would need to figure out which point of the area should be considered when measuring the distance. One option could be to take some measure of the center of each segmento; however, larger segmentos would then appear more remote than smaller ones. The preferred approach with areal data is hence slightly different: proximity is defined in terms of which segmentos share a border. For simplicity’s sake, let us assume for the moment that the link function is a simple identity function, such that \\(g(\\mu(s_{i}))=\\mu(s_{i})\\). Therefore, \\(\\mu(s_{i})\\) can be expressed as \\[ \\begin{aligned} \\mu(s_{i})=\\eta_{i}=\\mathbf{X(s_{i})}\\mathbf{\\beta}+\\upsilon_{i}. \\end{aligned} \\] A conditional autoregressive model (CAR) correlation function is used to model the spatial random effect \\(\\upsilon_{i}\\) (when dealing with point-referenced data, a Matern correlation and its SDPE expression are used instead, see the next subsection). The spatial dependency is assumed to be Markovian in nature, that is, the spatial correlation can be summarized by the spatial correlation between direct neighbors. The distribution of each \\(\\upsilon_{i}\\) conditional on all other \\(\\upsilon\\) is expressed as \\[ \\begin{aligned} \\upsilon_{i}|\\upsilon_{-i} \\sim N(\\sum_{j\\neq i}^{N}c_{i,j}\\upsilon_{j}, \\sigma_{i}^{2}), \\end{aligned} \\] where \\({-i}\\) means all except \\(i\\). In the simplest model, called the intrinsic CAR, \\(c_{i,j}\\) is \\(1\\) if \\(j\\) is a neighbor of \\(i\\) and \\(0\\) otherwise. The conditional mean of each \\(\\upsilon_{i}\\) is hence an average of its direct neighbor. The joint distribution of all \\(\\upsilon_{i}\\) is also Gaussian and written as \\[ \\begin{aligned} \\mathbf{\\upsilon} \\sim N(0, (\\mathbf{I-C})^{-1}\\mathbf{M}), \\end{aligned} \\] where \\(\\mathbf{I}\\) is a matrix of \\(1\\) and \\(0\\), \\(\\mathbf{C}\\) contains the \\(c_{i,j}\\), and \\(\\mathbf{M}\\) is the covariance matrix. Various CAR models specify various function for \\(\\mathbf{C}\\) and \\(\\mathbf{M}\\). The homogeneous CAR model set is \\[ \\begin{aligned} \\mathbf{C}=\\phi\\mathbf{A} \\space \\space \\space \\space \\space \\mathbf{M}=\\mathbf{I}\\sigma^{2}_{CAR}, \\end{aligned} \\] where \\(\\mathbf{A}\\) is \\(1\\) if the areas are neighboring and \\(0\\) otherwise, and \\(\\phi\\) needs to be estimated: the larger it is, the more the random effects are correlated. The intrinsic CAR models set \\(\\phi\\) to 1. One of the issues with CAR models is that they lead to overfitting, that is, models perform well on the training data set but poorly on out-of-sample predictions on the test set for validation purposes. A solution is to smooth the pattern of the spatial random effects. With a lower \\(\\sigma^{2}_{CAR}\\), the \\(\\upsilon\\) will vary less from neighbor to neighbor, leading to less overfit. To push the \\(\\sigma^{2}_{CAR}\\) lower, it is possible to work on the priors. The standard way of doing this is by using penalized complexity priors whereby one specifies the probability that \\(\\sigma^{2}_{CAR}\\) will be larger than a given number. To further limit the risk of overfitting, decompose the spatial random effects into one part that is spatially correlated and another that is pure noise. This is called the modified Besag-York-Mollie (Simpson et al. 2017), which we will be using in the next section. 8.2.4 Estimating the Model with Point-Referenced Data We now assume that the segmentos’ median incomes are point-referenced data. We reconsider \\(\\mathbf{y}\\) as a random variable distributed at \\(n\\) locations \\(s_{1}, ...,s_{n}\\). The income data can be considered as a sample of \\(y(s_{1}), ..., y(s_{n})\\) from the random process \\(\\mathbf{y(s)}\\). Assuming that the \\(y(s_{i})\\) are normally distributed, we have \\[ \\begin{aligned} y(s_{i})\\sim N(\\mu(s_{i}),\\sigma^2), \\end{aligned} \\] where \\(\\mu(s_{i})\\) is expressed as a function of a structured additive predictor \\(\\eta_{i}\\), such that \\(g(\\mu(s_{i}))=\\eta_{i}\\). For the sake of simplicity, let us assume for the moment that the link function is a simple identity function such that \\(g(\\mu(s_{i}))=\\mu(s_{i})\\). \\(\\mu(s_{i})\\) can be expressed as \\[ \\begin{aligned} \\mu(s_{i})=\\eta_{i}=\\mathbf{X(s_{i})}\\mathbf{\\beta}+\\upsilon_{i}+\\epsilon_{i}. \\end{aligned} \\] Assuming that the \\(\\upsilon_{i}\\) is normally distributed, it makes it a Gaussian field (\\(GF\\)): \\[ \\begin{aligned} \\upsilon_{i} \\sim GF(\\mathbf{0},\\mathbf{\\Sigma}). \\end{aligned} \\] Assuming that the covariance is Markovian, meaning that only the spatial correlation can be summarized by the spatial correlation between direct neighbors, then the \\(\\upsilon_{i}\\) is distributed according to a Gaussian Markov random field (GMRF). \\[ \\begin{aligned} \\upsilon_{i} \\sim GRMF(\\mathbf{0},\\mathbf{\\Sigma}). \\end{aligned} \\] As \\(\\mathbf{\\Sigma}\\) can be large (a matrix with more than 2 million elements in the case of the segmentos), a deterministic structure is imposed on it to speed the estimation. When working with point-referenced data, the variance-covariance \\(\\mathbf{\\Sigma}\\) is expressed in terms of the Matern correlation function. \\[ \\begin{aligned} \\mathbf{\\Sigma}=\\sigma_{\\upsilon}^2 cor_{Matern}(\\upsilon(s_{i}),\\upsilon(s_{j})). \\end{aligned} \\] The advantage of the Matern correlation function is that only a few parameters need to be estimated. To further simplify the computation, the Matern correlation can be re-expressed with an SPDE. Once the SPDE equation is solved, the SPDE parameters can be used to solve the Matern correlation parameters. To solve the SPDE, a spatial mesh is created on the point-referenced data. For each node of the mesh, we get a value \\(w_{k}\\) via the finite element approach. The \\(w_{k}\\) also form a GMRF. Once we have the \\(w_{k}\\), we can calculate the \\(\\upsilon_{i}\\) as a weighted sum of \\(w_{k}\\) time \\(a_{i,k}\\)s, where \\(a_{i,k}\\) is the distance of \\(s{i}\\) to node \\(k\\). This allows us to obtain the posterior distribution of the \\(\\upsilon_{i}\\)s. References "],
["data.html", "Section 9 Appendix 2: Data Pre-processing 9.1 Data Sources 9.2 Loading the Survey 9.3 Loading the Vectors 9.4 Loading the Rasters 9.5 Rasters Calculation 9.6 Distance Calculation 9.7 Spatial Statistics at the Segmento Level 9.8 Merging the Covariates with the EHPM Data 9.9 Summary", " Section 9 Appendix 2: Data Pre-processing Data preprocessing can be defined as the work required to turn raw data into the data used for modeling. The raw data used in this analysis are remote sensing and other geographic information system data, hereafter “RS data,” such as digitized maps of elevation, population density, or public service locations. These data need to be aggregated and linked with the survey containing the data on the development indicators we plan to map. Data preprocessing is arguably one of the most time-consuming tasks of any data science project. This section documents the path required to convert the raw data into data usable for high-resolution mapping. Here are the main steps: Load the RS layers. Create summaries for the RS layers that come with a time dimension (e.g., average precipitation over the last 30 years). Perform additional calculations as required (e.g., compute the soil degradation index based on soil degradation sub-indicator). Compute the distance metrics to public services, businesses, and some natural features. Align the coordinate reference systems (CRSs) across layers (geographic or projected). Aggregate the RS layers to the map of interest (e.g., the segmento map) with a chosen set of spatial statistics (e.g., average precipitation per segmento). Match the RS aggregates with the outcome data (poverty, income, literacy). Write the data frame to a .csv file for later use. Readers interested in only the modeling instructions can skip this section entirely. 9.1 Data Sources The 2017 EHPM household survey, collected by DIGESTYC, is used to compute the three main development indicators: median income, poverty, and literacy. The EHPM survey data are stored in the file ehpm-2017.csv on the publicly available DIGESTYC website here. DIGESTYC provided the research team with the segmento identifiers, allowing the RS data to be linked with the EPHM data at the segmento level rather than the canton level, as would have been necessary if using only the publicly available data. The segmento identifier is stored in the file Identificador de segmento.xlsx. A suite of RS data is used to determine potential predictors of the three main development indicators. The data sources are listed on the next table (see web-book version of this tutorial for the links). Names Type Source Population count raster CIESIN Precipitation raster Climate Hazards Center Lights at night raster NOAA Integrated Food Security Phase Classification vector FEWS NET Livelihood zones vector FEWS NET Altitude raster NASA Shuttle Radar Topography Mission (SRTMv003) Soil degradation raster Trend.Earth Buildings vector OpenStreetMap Points of interest points vector OpenStreetMap Roads network vector OpenStreetMap Vegetation greenness (NDVI) raster U.S. Geological Survey (USGS) Schools vector La Direccion General de Estadistica y Censos (DIGESTYC) Health centers csv La Direccion General de Estadistica y Censos (DIGESTYC) Hospitals csv La Direccion General de Estadistica y Censos (DIGESTYC) Temperature raster WorldClim Version2 (Fick and Hijmans 2017) Slope raster WorldPop Archive global gridded spatial datasets Distance to OSM major roads raster WorldPop and CIESIN Distance to OSM major roads intersections raster WorldPop and CIESIN Distance to OSM major waterway raster WorldPop and CIESIN Built settlement raster WorldPop and CIESIN Land cover raster European Space Agency 9.2 Loading the Survey We start by loading the segmento shapefile and the EHPM survey data: ehpm: This is the EHPM survey data. segmento_hh_id: This is the segmento ID matched to the household ID, which allows us to map households at the segmento level. segmento shapefile: This is the map of all segmentos. To create a more easily readable directory path, we create an object with the path to the folder where the data are stored: # Modify dir_data to where you stored the data root_dir=&quot;~/&quot; project_dir=&quot;data/&quot; dir_data=paste0(root_dir,project_dir) ehpm=read.csv(paste0(dir_data, &quot;tables/ehpm-2017.csv&quot;)) segmento_hh_id=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;)) 9.3 Loading the Vectors Vector data are polygons (e.g., administrative areas), lines (e.g., roads network), or points (e.g., coordinates of schools). They can be stored in various formats, with the most common being ESRI shapefiles and geojson files. 9.3.1 Loading the Administrative Boundaries First, we load the administrative maps of the segmentos and departamentos with the command readOGR from the rgdal package. Shapefiles are loaded as a SpatialPolygonDataframe, which is a georeferenced set of polygons to which a data frame is attached. dir_shape=&quot;spatial/shape/&quot; segmento_sh=readOGR(paste0(dir_data, dir_shape, &quot;admin/STPLAN_Segmentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields departamentos_sh=readOGR(paste0(dir_data, dir_shape, &quot;admin/STPLAN_Departamentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Departamentos.shp&quot;, layer: &quot;STPLAN_Departamentos&quot; ## with 14 features ## It has 6 fields Thesegmento_sh shape file loaded above, for example, consists of a data frame with 17 fields (variables). The names of each field can be accessed by running this command: names(segmento_sh) ## [1] &quot;OBJECTID&quot; &quot;DEPTO&quot; &quot;COD_DEP&quot; &quot;MPIO&quot; &quot;COD_MUN&quot; ## [6] &quot;CANTON&quot; &quot;COD_CAN&quot; &quot;COD_ZON_CE&quot; &quot;COD_SEC_CE&quot; &quot;COD_SEG_CE&quot; ## [11] &quot;AREA_ID&quot; &quot;SEG_ID&quot; &quot;AREA_KM2&quot; &quot;SHAPE_Leng&quot; &quot;Shape_Le_1&quot; ## [16] &quot;Shape_Area&quot; &quot;demmean&quot; The data frame attached to the polygon map can be inspected using the following functions: head(segmento_sh@data) # Provide the 5 first rows of the data frame segmento_sh@data ## OBJECTID DEPTO COD_DEP MPIO COD_MUN CANTON COD_CAN COD_ZON_CE ## 0 1 AHUACHAPAN 01 ATIQUIZAYA 03 AREA URBANA 00 00 ## 1 2 AHUACHAPAN 01 GUAYMANGO 06 EL ESCALON 04 00 ## 2 3 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 01 ## 3 4 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 01 ## 4 5 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 02 ## 5 6 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 02 ## COD_SEC_CE COD_SEG_CE AREA_ID SEG_ID AREA_KM2 SHAPE_Leng Shape_Le_1 ## 0 002 1015 U 01031015 0.04352 1261.0632 1261.0217 ## 1 005 0016 R 01060016 1.65778 8536.1704 8535.8890 ## 2 003 1015 U 01011015 0.07500 1165.9750 1165.9366 ## 3 003 1014 U 01011014 0.05010 988.8543 988.8216 ## 4 009 1019 U 01011019 0.06910 1135.7582 1135.7209 ## 5 009 1042 U 01011042 0.04690 857.8403 857.8122 ## Shape_Area demmean ## 0 43526.87 NA ## 1 1657675.21 NA ## 2 74995.98 NA ## 3 50104.03 NA ## 4 69104.02 NA ## 5 46899.53 NA summary(segmento_sh@data) # Summarize each field of the data frame segmento_sh@data ## OBJECTID DEPTO COD_DEP MPIO ## Min. : 1 Length:12435 Length:12435 Length:12435 ## 1st Qu.: 3110 Class :character Class :character Class :character ## Median : 6219 Mode :character Mode :character Mode :character ## Mean : 6219 ## 3rd Qu.: 9328 ## Max. :12437 ## ## COD_MUN CANTON COD_CAN COD_ZON_CE ## Length:12435 Length:12435 Length:12435 Length:12435 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## COD_SEC_CE COD_SEG_CE AREA_ID SEG_ID ## Length:12435 Length:12435 Length:12435 Length:12435 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## AREA_KM2 SHAPE_Leng Shape_Le_1 Shape_Area ## Min. : 0.00000 Min. : 281.4 Min. : 281.4 Min. : 3903 ## 1st Qu.: 0.06062 1st Qu.: 1256.3 1st Qu.: 1256.3 1st Qu.: 60567 ## Median : 0.27345 Median : 2859.6 Median : 2859.5 Median : 273177 ## Mean : 1.67100 Mean : 5494.0 Mean : 5493.8 Mean : 1672961 ## 3rd Qu.: 2.08214 3rd Qu.: 8492.5 3rd Qu.: 8492.3 3rd Qu.: 2082009 ## Max. :48.36518 Max. :54684.4 Max. :54682.6 Max. :48194678 ## ## demmean ## Min. : NA ## 1st Qu.: NA ## Median : NA ## Mean :NaN ## 3rd Qu.: NA ## Max. : NA ## NA&#39;s :12435 We see that Shape_Le_1 and SHAPE_Leng are duplicates, while demmean comprises only null values. These fields are removed using the dplyr function select. segmento_sh@data=segmento_sh@data%&gt;% select(-c(Shape_Le_1,demmean)) To get a country mask, which is the boundary of the country, we dissolve the multiple polygons of the departamentos shapefile into one overall map of the country with the function maptools::unionSpatialPolygons. SLV_adm0_proj=maptools::unionSpatialPolygons(departamentos_sh, rep(1, nrow(departamentos_sh))) The country mask is shown in Figure 9.1. plot(SLV_adm0_proj) Figure 9.1: El Salvador country mask 9.3.1.1 Adjusting the Coordinates System There are two families of coordinates systems: Geographic coordinate systems (GCS): “A system that uses a three-dimensional spherical surface to define locations on the earth.”5. Projected coordinate systems (PCS): “A system defined on a flat, two-dimensional surface. Unlike a geographic coordinate system, a projected coordinate system has constant lengths, angles, and areas across the two dimensions.”6 The shapefiles for the segmentos, departmentos, and the country mask have a projected coordinate system. SLV_adm0_proj@proj4string ## CRS arguments: ## +proj=lcc +lat_0=13.783333 +lon_0=-89 +lat_1=13.316666 +lat_2=14.25 ## +x_0=500000 +y_0=295809.184 +k_0=0.99996704 +datum=NAD27 +units=m ## +no_defs The coordinates are expressed in meters: sp::bbox(SLV_adm0_proj) ## min max ## x 377579.6 642687.1 ## y 226506.1 369595.5 Since most spatial RS data are based on a GCS, we create a second version of the country mask with one. This is done with the function spTransform from the sp library. The use of :: allows us to call the function without loading the sp library in the environment. proj_WGS_84=&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; segmento_sh_wgs=sp::spTransform(segmento_sh, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved SLV_adm0=sp::spTransform(SLV_adm0_proj, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved sp::bbox(SLV_adm0) ## min max ## x -90.13200 -87.68380 ## y 13.15541 14.45093 The coordinates are now expressed in decimal degrees. 9.3.2 Loading the Shapefiles Containing the Fields Used as Covariates We now load the shapefiles containing the fields used as covariates, which are the containing variables that will be used as predictors in the model we will fit in the next sections. We start with the Livelihood zone maps accessed from the Famine Early Warning Systems Network (FEWS NET). LZ_sh=readOGR(paste0(dir_data, dir_shape, &quot;LZ/SV_LHZ_2010.shp&quot;)) The livelihood zone map is mapped on 9.2 for inspection. leaflet(LZ_sh)%&gt;% addTiles()%&gt;% addPolygons(weight=1, color = &quot;#444444&quot;, smoothFactor = 1, fillOpacity = 1, fillColor = ~colorFactor(&quot;Accent&quot;, LZNAMEEN)(LZNAMEEN), popup = ~LZNAMEEN)%&gt;% addLegend(&quot;bottomright&quot;, colors = ~colorFactor(&quot;Accent&quot;, LZNAMEEN)(LZNAMEEN), labels = ~LZNAMEEN, opacity = 1) Figure 9.2: Livelihood zones Next, we load all the other Vectors data. school_sh=readOGR(paste0(dir_data, dir_shape, &quot;schools/MINED.shp&quot;),use_iconv=TRUE, encoding = &quot;UTF-8&quot;) buildings_poly_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_buildings_polygons.shp&quot;), use_iconv=TRUE) poi_points_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_points_of_interest_points.shp&quot;)) poi_poly_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_points_of_interest_polygons.shp&quot;)) roads_lines_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_roads_lines.shp&quot;)) roads_poly_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_roads_polygons.shp&quot;)) coastline=readOGR(paste0(dir_data, dir_shape, &quot;coastline/coastline.shp&quot;)) waterbodies=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_waterways_polygons.shp&quot;)) proj_lcc=&quot; +proj=lcc +lat_1=13.316666 +lat_2=14.25 +lat_0=13.783333 +lon_0=-89 +x_0=500000 +y_0=295809.184 +datum=NAD27 +units=m +no_defs +ellps=clrk66 +nadgrids=@conus,@alaska,@ntv2_0.gsb,@ntv1_can.dat&quot; # waterbodies_proj=sp::spTransform(waterbodies, # proj_lcc) waterbodies_proj= waterbodies rivers=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_waterways_lines.shp&quot;)) # rivers_proj=sp::spTransform(rivers, # proj_lcc) rivers_proj=rivers 9.3.3 Loading Point-Referenced Data from Tables Two datasets are provided as R data frames in a .Rda file. They are loaded with the function load: load(paste0(dir_data, dir_shape, &quot;hospitales/hospitales.Rda&quot;)) load(paste0(dir_data, dir_shape, &quot;UCSF/UCSF.Rda&quot;)) The package sp is then used to turn the data frames into SpatialPointsDataframe objects. # In the .Rda, there is an error in the naming of the coordinates: latitudes are actually longitudes and vice versa hospitales=hospitales%&gt;% rename(lon=latitude, lat=longitude) hospitals_sp=as.data.frame(hospitales) # Features &quot;lon&quot; and &quot;lat&quot; are declared the the spatial coordinates of the data frame &quot;hospitals_sp&quot; # The data frame &quot;hospitals_sp&quot; becomes an object of the type SpatialPointsDataframe sp::coordinates(hospitals_sp)=~lon+lat # A spatial coordinate system is assigned to the SpatialPointsDataframe &quot;hospitals_sp&quot; sp::proj4string(hospitals_sp)=sp::CRS(proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved The same is done with the health center (not shown). The resulting map is plotted on 9.3 for reference.7 leaflet(SLV_adm0)%&gt;% addPolygons()%&gt;% addTiles()%&gt;% addMarkers(lng=coordinates(hospitals_sp)[,1], lat=coordinates(hospitals_sp)[,2], popup=hospitals_sp$name)%&gt;% addCircleMarkers(lng=coordinates(health_centres_sp)[,1], lat=coordinates(health_centres_sp)[,2], radius = 4, popup=paste0(health_centres_sp$name,&quot;\\n(&quot;,health_centres_sp$tipo,&quot;)&quot;), stroke = F, fillOpacity = 1, fillColor = colorFactor(&quot;Accent&quot;, health_centres_sp$tipo)(health_centres_sp$tipo)) Figure 9.3: Health Centers 9.4 Loading the Rasters We now turn to the raster files. Raster files can be described as georeferenced pixel data. For instance, a raster of population density consists of a map of pixels. The value of each pixel is the density of people in this pixel. The two main formats for raster data are zipped .tif and .ncdf files. 9.4.1 Lights at Night Lights at night data products are available by calendar month and year. In this tutorial, annual data are used for 1983 to 2016. The yearly aggregate data for 2017 had not yet been released when we began data preprocessing, so monthly aggregated data were used. The first step is to unzip the files with the untar command. For the sake of convenience, the data are already provided as extracted .tif in the data folder data/spatial/raster/l@n/, so the code snippet below is just shown for illustrative purposes and does not need to be run. # Get the list of all files in the l@n directory with the command -dir-, which takes as argument # The path to the directory list_of_files=dir(paste0(dir_data, &quot;spatial/raster/l@n/&quot;)) # Identify the index of the 2017 monthly files index_of_2017_files=grep(&quot;npp_2017&quot;, dir(paste0(dir_data, &quot;spatial/raster/l@n/&quot;))) # Selecting in the list of all files the 2017 monthly files thanks to the index files_2017=list_of_files[index_of_2017_files] # for(monthly_file in files_2017){ # untar(paste0(dir_data, # &quot;spatial/raster/l@n/&quot;, # monthly_file), # The path to the file of interest # compressed=&quot;gzip&quot;, # Specify it was zipped with gzip # exdir=paste0(dir_data, # &quot;spatial/raster/l@n/&quot;)) # Specify the directory where the files need to be unzipped exdir # } The 12 monthly lights at night .tif files are loaded with the raster::raster function, cropped to El Salvador’s boundaries with raster::crop, and stacked in one object with raster::stack. Note the use of the for loop function to loop the 12 monthly files. # Get the list of the files in the directory list_of_files=dir(paste0(dir_data, &quot;spatial/raster/l@n/&quot;)) # Get the list of lights at night files labelled &quot;avg_rade9h&quot; monthly_file_light=list_of_files[grepl(&quot;avg_rade9h&quot;, list_of_files, fixed=T)] # Create a vector of character to name the raster as file_1, file_2, etc. names_file=paste0(&quot;file_&quot;,1:12) # Loop over the 12 monthly files in order to: for(i in 1:length(monthly_file_light)){ # Load each i monthly layer r=raster::raster(paste0(dir_data, &quot;spatial/raster/l@n/&quot;, monthly_file_light[i])) # Crop each i monthly layer r=raster::crop(r, SLV_adm0) # Assign each i cropped monthly layer to a name assign(names_file[i], # assign it to an object name r) } # Stack all the layers lights_all=raster::stack(lapply(names_file,FUN=get)) Let us have a look at the object lights_all: class(lights_all)[1] ## [1] &quot;RasterStack&quot; dim(lights_all) ## [1] 311 587 12 raster::res(lights_all) ## [1] 0.004166667 0.004166667 raster::extent(lights_all) ## class : Extent ## xmin : -90.13125 ## xmax : -87.68542 ## ymin : 13.15625 ## ymax : 14.45208 raster::crs(lights_all) ## CRS arguments: +proj=longlat +datum=WGS84 +no_defs This a RasterStack object, which is a stack of rasters where each layer is aligned above another, with 310 rows, 587 columns, and 12 layers (1 per month) for a total of 181,970 cells. The resolution is provided in decimal degrees: 0.004166667 degrees by 0.004166667 degrees pixels. Next, we plot the 12 monthly layers for inspection. raster::plot(lights_all, main=c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;,&quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) Figure 9.4: 2017 monthly average lights at night (average DNB radiance) The maps in 9.4 show there are variations over the year. 9.4.2 Precipitation The CHIRPS precipitation data are provided as an .nc file, which is a collection of raster files similar to a raster stack. Data are loaded as a brick using the raster::brick function, similar to the raster stack function.8 chirps=raster::brick(paste0(dir_data, &quot;spatial/raster/chirps/chirps-v2.0.annual.nc&quot;), band=1:38) print(chirps) ## File /home/rstudio/data/spatial/raster/chirps/chirps-v2.0.annual.nc (NC_FORMAT_NETCDF4): ## ## 1 variables (excluding dimension variables): ## float precip[longitude,latitude,time] (Chunking: [800,223,4]) (Compression: level 5) ## units: mm/year ## standard_name: convective precipitation rate ## long_name: Climate Hazards group InfraRed Precipitation with Stations ## time_step: year ## missing_value: -9999 ## _FillValue: -9999 ## geostatial_lat_min: -50 ## geostatial_lat_max: 50 ## geostatial_lon_min: -180 ## geostatial_lon_max: 180 ## ## 3 dimensions: ## longitude Size:7200 ## units: degrees_east ## standard_name: longitude ## long_name: longitude ## axis: X ## latitude Size:2000 ## units: degrees_north ## standard_name: latitude ## long_name: latitude ## axis: Y ## time Size:38 ## units: days since 1980-1-1 0:0:0 ## standard_name: time ## calendar: gregorian ## axis: T ## ## 15 global attributes: ## Conventions: CF-1.6 ## title: CHIRPS Version 2.0 ## history: created by Climate Hazards Group ## version: Version 2.0 ## date_created: 2019-01-30 ## creator_name: Pete Peterson ## creator_email: pete@geog.ucsb.edu ## institution: Climate Hazards Group. University of California at Santa Barbara ## documentation: http://pubs.usgs.gov/ds/832/ ## reference: Funk, C.C., Peterson, P.J., Landsfeld, M.F., Pedreros, D.H., Verdin, J.P., Rowland, J.D., Romero, B.E., Husak, G.J., Michaelsen, J.C., and Verdin, A.P., 2014, A quasi-global precipitation time series for drought monitoring: U.S. Geological Survey Data Series 832, 4 p., http://dx.doi.org/110.3133/ds832. ## comments: time variable denotes the first day of the given year. ## acknowledgements: The Climate Hazards Group InfraRed Precipitation with Stations development process was carried out through U.S. Geological Survey (USGS) cooperative agreement #G09AC000001 &quot;Monitoring and Forecasting Climate, Water and Land Use for Food Production in the Developing World&quot; with funding from: U.S. Agency for International Development Office of Food for Peace, award #AID-FFP-P-10-00002 for &quot;Famine Early Warning Systems Network Support,&quot; the National Aeronautics and Space Administration Applied Sciences Program, Decisions award #NN10AN26I for &quot;A Land Data Assimilation System for Famine Early Warning,&quot; SERVIR award #NNH12AU22I for &quot;A Long Time-Series Indicator of Agricultural Drought for the Greater Horn of Africa,&quot; The National Oceanic and Atmospheric Administration award NA11OAR4310151 for &quot;A Global Standardized Precipitation Index supporting the US Drought Portal and the Famine Early Warning System Network,&quot; and the USGS Land Change Science Program. ## ftp_url: ftp://chg-ftpout.geog.ucsb.edu/pub/org/chg/products/CHIRPS-latest/ ## website: http://chg.geog.ucsb.edu/data/chirps/index.html ## faq: http://chg-wiki.geog.ucsb.edu/wiki/CHIRPS_FAQ The raster chirps provides global annual cumulative precipitation over 38 years (38 bands), from 1981 to 2018, at a resolution of 0.05 longitude/latitude decimal degrees. Next, the data are cropped to El Salvador’s borders, and all years until 2017 are selected (data are provided up to January 2018 in the file). # Crop the data chirps_ev=raster::crop(chirps, SLV_adm0) time = raster::getZ(chirps_ev) # get the time index index_of_1981_2017 = which(dplyr::between(time, as.Date(&quot;1981-01-01&quot;), as.Date(&quot;2017-01-01&quot;))) index_of_2017 = which(time==as.Date(&quot;2017-01-01&quot;)) # Subset the CHIRPS to 1981–2017 (take out 2018) chirps_ev_1981_2017=raster::subset(chirps_ev, index_of_1981_2017) 9.4.3 Soil Degradation The soil degradation data were obtained via the QGIS plugin tool TRENDS.EARTH.9 The three main soil-degradation indexes provided in the TRENDS.EARTH are soil carbon stock degradation, the soil land-use change degradation, and soil productivity degradation, which has subindexes called trajectory, performance, and state. Data for each of the above are provided in a series of multilayer .tif files. The .json file provides information about each layer. soil_deg_carb_path=&quot;spatial/raster/soil_deg_carb/soil_deg_carb.json&quot; soil_deg_carb_json=RJSONIO::fromJSON(paste0(dir_data, soil_deg_carb_path)) grep(&quot;degradation&quot;, lapply(lapply(soil_deg_carb_json$bands, unlist), cbind)) soil_deg_luc_json_path=&quot;spatial/raster/soil_deg_luc/soil_deg_luc.json&quot; soil_deg_luc_json=RJSONIO::fromJSON(paste0(dir_data, soil_deg_luc_json_path)) lapply(lapply(soil_deg_luc_json$bands, unlist), cbind) grep(&quot;degradation&quot;, lapply(lapply(soil_deg_luc_json$bands, unlist), cbind)) soil_deg_prod_json_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.json&quot; soil_deg_prod_json=RJSONIO::fromJSON(paste0(dir_data, soil_deg_prod_json_path)) grep(&quot;degradation&quot;, lapply(lapply(soil_deg_prod_json$bands, unlist), cbind)) For the carbon and land-use change files, band 1 provides the soil degradation index. For the productivity file, the index values on the trajectory, performance, and state are stored in bands 2, 4, and 7. soil_deg_carb_path=&quot;spatial/raster/soil_deg_carb/soil_deg_carb.tif&quot; soil_deg_carb=raster::raster(paste0(dir_data, soil_deg_carb_path), band=1) # Soil degradation, change in the carbon method soil_deg_luc_path=&quot;spatial/raster/soil_deg_luc/soil_deg_luc.tif&quot; soil_deg_luc=raster::raster(paste0(dir_data, soil_deg_luc_path), band=1) # Soil degradation, land use change soil_deg_prod_trj_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.tif&quot; soil_deg_prod_trj=raster::raster(paste0(dir_data, soil_deg_prod_trj_path), band=2) # Soil degradation, productivity trajectory deg soil_deg_prod_prf_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.tif&quot; soil_deg_prod_prf=raster::raster(paste0(dir_data, soil_deg_prod_prf_path), band=4) # Soil degradation, productivity performance deg soil_deg_prod_stt_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.tif&quot; soil_deg_prod_stt=raster::raster(paste0(dir_data, soil_deg_prod_stt_path), band=7) # Soil degradation, productivity state deg 9.4.4 Vegetation Greenness Vegetation greenness data, as measured by normalized difference vegetation index, are provided on a dekadal (every 10 days) basis. Two data products are available: the smoothed dekadal NDVI value for 2017, and the median dekadal NDVI value over the period 2003–2017. The first step is to unzip the data with the following code snippet. For the sake of convenience, the data are provided as unzipped .tif in the data pack, but the code to unzip the files is also provided below. # Get the list of all files in the ndvi directory with the command -dir- list_of_files=dir(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;)) # Identify the index of the 2017 monthly files and norma index_of_2017_std_files=grep(&quot;17|stm&quot;, dir(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;))) # Select in the list of all file the 2017 monthly files thanks to the index files_of_interest=list_of_files[index_of_2017_std_files] # for(file in files_of_interest){ # untar(paste0(dir_data, # &quot;spatial/raster/ndvi/&quot;, # file), # The path to the file of interest # compressed=&quot;gzip&quot;, # exdir=paste0(dir_data, # &quot;spatial/raster/ndvi/&quot;)) # } The data are then loaded. Note the use of the function grep and the regular expressions to flexibly select the appropriate set of files: \"ca17[[:digit:]]{2}.tif\" means: “ca17”, followed by a suite of 2 digits, followed by “.tif” -&gt; these are the 2017 files \"ca[[:digit:]]{2}stm.tif\" means: “ca”, followed by a suite of 2 digits, followed by “stm.tif” -&gt; these are the median files # Create a vector of character to name the raster as ndvi_1, ndvi_2, etc. names_ndvi_17=paste0(&quot;ndvi_&quot;,1:36) names_ndvi_norm=paste0(&quot;ndvi_norm_&quot;,1:36) # List of the 36 dekadal file for 2017 thanks to the command -grep- and the regular expression list_of_files=dir(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;)) files_17=list_of_files[grep(&quot;ca17[[:digit:]]{2}.tif&quot;, list_of_files)] # The list of 36 ones for the 2003–2017 median files_norm=list_of_files[grep(&quot;ca[[:digit:]]{2}stm.tif&quot;, list_of_files)] # Loop the 36 dekadal 2017 and normal files to load and crop them for(i in 1:length(files_17)){ # 1) The 2017 data: # Load each i dekadal layer r=raster::raster(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, files_17[i])) # Crop each i monthly layer r=raster::crop(r, SLV_adm0) # Assign each i cropped monthly layer to a name assign(names_ndvi_17[i], # assign it to an object name r) # 2) The median data # Load each i dekadal layer r=raster::raster(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, files_norm[i])) # Crop each i monthly layer r=raster::crop(r, SLV_adm0) # Assign each i cropped monthly layer to a name assign(names_ndvi_norm[i], # Assign it to an object name r) } # Stack all the layers ndvi_17=raster::stack(lapply(names_ndvi_17, FUN=get)) ndvi_norm=raster::stack(lapply(names_ndvi_norm, FUN=get)) The rasters are then resampled to perfectly align both sets: ndvi_17_re=raster::resample(ndvi_17, ndvi_norm) 9.4.5 Temperature The 12 monthly average temperature data rasters are cropped and stacked. files_temp=dir(paste0(dir_data, &quot;spatial/raster/temperature/&quot;))[grep(&quot;wc2.0_30s_tavg_&quot;, dir(paste0(dir_data, &quot;spatial/raster/temperature/&quot;)))] month=1 for(temp_m in files_temp){ temp=raster::raster(paste0(dir_data, &quot;spatial/raster/temperature/&quot;,temp_m)) temp=raster::crop(temp, SLV_adm0) assign(paste0(&quot;temp_&quot;,month), temp) month=month+1 } temp_months=raster::stack(lapply(paste0(&quot;temp_&quot;, 1:12), FUN=get)) 9.4.6 Slope The slope data are obtained from the WorldPop Dataverse Repository as a suite of raster files, each of which covers a portion of El Salvador’s territory. The rasters are combined into a single raster to ease preprocessing work. slope_files=paste0(dir_data, &quot;spatial/raster/slope/&quot;, dir(paste0(dir_data, &quot;spatial/raster/slope/&quot;))) i=1 raster_list=list() for(file in slope_files){ raster_f=raster::raster(file) raster_list[[i]]=raster_f i=i+1 } slope &lt;- do.call(raster::merge, raster_list) slope=raster::crop(slope, SLV_adm0) raster::plot(slope, main=&quot;Slope (degrees)&quot;) sp::plot(SLV_adm0, add=T) 9.4.7 Other Rasters The other rasters are loaded with the function raster: dem=raster::raster(paste0(dir_data, &quot;spatial/raster/dem/srtm_dem.tif&quot;)) dist2road_path=&quot;spatial/raster/distance2roads/slv_osm_dst_road_100m_2016.tif&quot; dist2road=raster::raster(paste0(dir_data, dist2road_path)) dist2roadInter_path=&quot;spatial/raster/distance2intersections/slv_osm_dst_roadintersec_100m_2016.tif&quot; dist2roadInter=raster::raster(paste0(dir_data, dist2roadInter_path)) settlements_path=&quot;spatial/raster/settlements/slv_bsgme_v0a_100m_2017.tif&quot; settlements=raster::raster(paste0(dir_data, settlements_path)) dist2water_path=&quot;spatial/raster/distance2waterway/slv_osm_dst_waterway_100m_2016.tif&quot; dist2water=raster::raster(paste0(dir_data, dist2water_path)) gpw_path=&quot;spatial/raster/GPW/gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2015_30_sec.tif&quot; gpw=raster::raster(paste0(dir_data,gpw_path)) # Population count for 2015 gpw_es=raster::crop(gpw, SLV_adm0) lc_path=&quot;spatial/raster/CCILC/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7_sv.tif&quot; lc=raster::raster(paste0(dir_data,lc_path)) # Land class We can plot these rasters for quality controls. Figure 9.5 shows the digital elevation model, that is, altitudes across the country. raster::plot(dem) sp::plot(SLV_adm0, add=T) Figure 9.5: Digital Elevation Model Figure 9.6 shows the population density data. By examining the GPW data, it is possible to identify some geometric forms corresponding to the enumeration areas of the census. raster::plot(gpw_es) sp::plot(SLV_adm0, add=T) Figure 9.6: Population density data (GPW) 9.5 Rasters Calculation The lights at night raster is expected to be a good predictor of economic activity (Henderson, Storeygard, and Weil 2012). However, it is not clear which summary statistics will best predict the three development indicators. For example, we cannot establish with certainty whether annual average lights at night or annual minimum average monthly lights at night is a better predictor of poverty. This holds for the other raster data as well. 9.5.1 Lights at Night The median and standard deviation of the lights at night data are computed for each map pixel: lights_med=raster::calc(lights_all, fun = median, na.rm = T) lights_sd=raster::calc(lights_all, fun = sd, na.rm = T) lights_summaries=raster::stack(lights_med, lights_sd) names(lights_summaries)=c(&quot;lights_med&quot;, &quot;lights_sd&quot;) The results are plotted in Figure 9.7. raster::plot(lights_summaries, main=c(&quot;Median&quot;, &quot;Standard Deviation&quot;)) sp::plot(SLV_adm0, add=T) Figure 9.7: 2017 monthly average lights at night (average DNB radiance) 9.5.2 Precipitation Again, as it is not clear a priori which summary statistics of precipitation are best at predicting the development indicators, a series of summary statistics is computed for each pixel of the map: Two precipitation summaries related to short-term weather: Annual cumulative precipitation for 2017 Annual cumulative precipitation for 2017 as a percentage of normal Two precipitation summaries related to climate: Median annual cumulative precipitation over the 38-year period Standard deviation in precipitation between years # Compute summaries chirps_ev_2017=raster::subset(chirps_ev, index_of_2017) # 2017 chirps_ev_med=raster::calc(chirps_ev_1981_2017, fun = median, na.rm = T) #medi chirps_summaries=raster::stack(chirps_ev_2017, chirps_ev_med) names(chirps_summaries)=c(&quot;chirps_ev_2017&quot;, &quot;chirps_ev_med&quot;) For quality control, we plot the respective summaries. raster::plot(chirps_summaries, main=c(&quot;2017 precipitation&quot;, &quot;2017 precipitation as percentage of normal&quot;, &quot;1981-2017 median&quot;, &quot;1981-2017 standard deviation&quot;)) 9.5.3 Vegetation Greenness Various statistics are computed on the dekadal NDVI and its deviation from the 2003–2017 normal (median pixel value over the period). ndvi_17_perc=(ndvi_17_re/ndvi_norm)*100 # annual NDVI values ndvi_17_sum=raster::calc(ndvi_17_re, fun = sum, na.rm = T) ndvi_17_summaries=raster::stack(ndvi_17_sum) names(ndvi_17_summaries)=c(&quot;ndvi_17_sum&quot;) # NDVI as a percentage of the 2003–2017 norm ndvi_17_perc_sum=raster::calc(ndvi_17_perc, fun = sum, na.rm = T) ndvi_17_perc_summaries=raster::stack(ndvi_17_perc_sum) names(ndvi_17_perc_summaries)=c(&quot;ndvi_17_perc_sum&quot;) 9.5.4 Soil Degradation The missing data are stored in the raw rasters as -32768 and turned into NA. Next, the soil degradation data are simplified by creating one binary raster for the three soil degradation proxies: the soil is degraded or not. # Soil carbon stock degradation # Replace the NA soil_deg_carb_val=raster::getValues(soil_deg_carb) index_na=which(soil_deg_carb_val==-32768) soil_deg_carb_val[index_na]=NA #set to NA the cell identified with original value of -32768 # Binarize for degradation index_deg=which(soil_deg_carb_val&lt;0) soil_deg_carb_deg=soil_deg_carb_val soil_deg_carb_deg[index_deg]=1 soil_deg_carb_deg[-index_deg]=0 soil_deg_carb_deg[index_na]=NA soil_deg_carb_deg=raster::setValues(soil_deg_carb, soil_deg_carb_deg) As the code is quite lengthy, it is shown only for the carbon stock soil degradation process. # Soil Land use change degradation soil_deg_luc_val=raster::getValues(soil_deg_luc) index_deg=which(soil_deg_luc_val&lt;0) soil_deg_luc_deg=soil_deg_luc_val soil_deg_luc_deg[index_deg]=1 soil_deg_luc_deg[-index_deg]=0 soil_deg_luc_deg=raster::setValues(soil_deg_luc, soil_deg_luc_deg) # Soil Productivity degradation # Trajectory soil_deg_prod_trj_val=raster::getValues(soil_deg_prod_trj) index_na=which(soil_deg_prod_trj_val==-32768) index_deg=which(soil_deg_prod_trj_val&lt;0) soil_deg_prod_trj_val[index_na]=NA soil_deg_prod_trj_deg=soil_deg_prod_trj_val soil_deg_prod_trj_deg[index_deg]=1 soil_deg_prod_trj_deg[-index_deg]=0 soil_deg_prod_trj_deg[index_na]=NA soil_deg_prod_trj_deg=raster::setValues(soil_deg_prod_trj, soil_deg_prod_trj_deg) # Performance soil_deg_prod_prf_val=raster::getValues(soil_deg_prod_prf) index_na=which(soil_deg_prod_prf_val==-32768) index_deg=which(soil_deg_prod_prf_val&lt;0) soil_deg_prod_prf_val[index_na]=NA soil_deg_prod_prf_deg=soil_deg_prod_prf_val soil_deg_prod_prf_deg[index_deg]=1 soil_deg_prod_prf_deg[-index_deg]=0 soil_deg_prod_prf_deg[index_na]=NA soil_deg_prod_prf_deg=raster::setValues(soil_deg_prod_prf, soil_deg_prod_prf_deg) # State soil_deg_prod_stt_val=raster::getValues(soil_deg_prod_stt) index_na=which(soil_deg_prod_stt_val==-32768) index_deg=which(soil_deg_prod_stt_val&lt;0) soil_deg_prod_stt_val[index_na]=NA soil_deg_prod_stt_deg=soil_deg_prod_stt_val soil_deg_prod_stt_deg[index_deg]=1 soil_deg_prod_stt_deg[-index_deg]=0 soil_deg_prod_stt_deg[index_na]=NA soil_deg_prod_stt_deg=raster::setValues(soil_deg_prod_stt, soil_deg_prod_stt_deg) Finally, the SDG Indicator (15.3) for soil degradation indicator is computed. It is based on the one-out-all-out principle that is, if a pixel is considered as soil degraded in any of the three soil degradation layers, then it is considered degraded in the SDG indicator. The first step consists of aligning the raster to make sure the pixels of each layer are perfectly overlaid. # Check that the rasters&#39; extents are the same all(raster::extent(soil_deg_luc_deg)==raster::extent(soil_deg_prod_stt_deg), raster::extent(soil_deg_carb_deg)==raster::extent(soil_deg_prod_stt_deg)) ## [1] FALSE As some of the layers have a slightly different extent, the values of each layer are resampled in the same raster grid. # Resample the raster &quot;soil_deg_luc_deg&quot; to the same resolution and extent than the raster &quot;soil_deg_prod_stt_deg&quot; soil_deg_luc_deg_re=raster::resample(soil_deg_luc_deg, soil_deg_prod_stt_deg) # Round the value, as the resampling takes averages while we want 0/1 soil_deg_luc_deg_re_val=raster::getValues(soil_deg_luc_deg_re) soil_deg_luc_deg_re_val_r=round(soil_deg_luc_deg_re_val) soil_deg_luc_deg_re_r=raster::setValues(soil_deg_luc_deg_re, soil_deg_luc_deg_re_val_r) raster::plot(soil_deg_luc_deg_re_r) sp::plot(SLV_adm0, add=T) The same is done with the raster soil_deg_carb_deg (not shown). The resulting rasters are stacked, and the SDG index is computed. # Stack the rasters #### soil_deg=raster::stack(soil_deg_carb_deg_re_r, soil_deg_luc_deg_re_r, soil_deg_prod_stt_deg, soil_deg_prod_prf_deg, soil_deg_prod_trj_deg) names(soil_deg)=c(&quot;soil_carb&quot;, &quot;soil_luc&quot;, &quot;soil_prod_state&quot;, &quot;soil_prod_prf&quot;, &quot;soil_prod_trj&quot;) # SDG 15.3.1 soil degradation index (one out all out principle) #### soil_deg_SDG_index=raster::calc(soil_deg, sum, na.rm=T) soil_deg_SDG_index=raster::calc(soil_deg_SDG_index, fun=function(x) {x[x&gt;0] = 1; return(x)} ) And here is the map of soil degradation according to SDG Indicator 15.3: raster::plot(soil_deg_SDG_index) sp::plot(SLV_adm0, add=T) Figure 9.8: Soil Degradation, SDG indicator 15.3 9.5.5 Temperature Next, we calculate the median and standard deviation of the temperature variations for the year 2017. temp_median=raster::calc(temp_months, fun = median, na.rm = T) temp_summaries=raster::stack(temp_median) names(temp_summaries)=c(&quot;temp_median&quot;) raster::plot(temp_median) sp::plot(SLV_adm0, add=T) 9.6 Distance Calculation Distance to public services and businesses may be a good indicator of poverty and other development indicators: remote areas tend to be poorer and have lower levels of literacy. Furthermore, the distance to some natural features, such as the distance to the coast or the distance to forested areas, may be correlated with local economic development. We compute below the distance to: Public services Hospitals (source: IDB) Health centers (source: IDB) Private, public, and other schools (source: IDB) Government offices (education and health excluded) (source: OSM) Any of the above Businesses Banks or ATMs (source: OSM) Any businesses, Bank or ATM included (source: OSM) Natural features Water bodies (land class from ESA) Cropland (land class from ESA) Forested areas (land class from ESA) Urban areas (land class from ESA) 9.6.1 Distance to Public Services The first step is to create a raster to use as a canvas to compute distances. The distances will be computed from each raster cell’s center to the closest point of interest (e.g., the closest school). To have relatively good precision with the distance estimates, we will create a raster with a resolution of 100 meters. Note the use of the projected coordinates system from the segmento_sh shapefile. # Create a raster for the computation of distances raster_dist=raster::raster(raster::extent(segmento_sh), crs=segmento_sh@proj4string, res=c(100,100)) print(segmento_sh@proj4string) ## CRS arguments: ## +proj=lcc +lat_0=13.783333 +lon_0=-89 +lat_1=13.316666 +lat_2=14.25 ## +x_0=500000 +y_0=295809.184 +k_0=0.99996704 +datum=NAD27 +units=m ## +no_defs We transform the CRS of the hospitals_sp shapefile to the corresponding projected CRS: # Project the hospital shapefile #### hospitals_proj=sp::spTransform(hospitals_sp, segmento_sh@proj4string) We can now compute the distance from each cell center of the raster, raster_dist, to the projected shapefile of hospitals, hospitals_proj, with the function raster::distanceFromPoints.10 dist2hosp=raster::distanceFromPoints(raster_dist, hospitals_proj) The distance map is plotted for inspection. raster::plot(dist2hosp) sp::plot(SLV_adm0_proj, add=T) The color scale indicates the distance to hospitals. The same process can be repeated for the other public services (not shown). For the OSM data, two additional steps are required: The data is filtered to select only the public amenities defined as: Townhall Post office Courthouse Police station Prison Fire station Community center Public building (a loose OSM definition) The polygon data is rasterized to allow for distance calculation with the function raster::distance A buffer of 25 meters needs to be included as the function raster::rasterize considers a polygon to cross a cell only if it covers the cell center The function raster::distance computes the distance from all pixels with an NA value to the nearest pixel that is not NA. # Distance to public amenities (health and education excluded) #### amenities=c(&quot;townhall&quot;,&quot;post_office&quot;,&quot;courthouse&quot;,&quot;police&quot;, &quot;prison&quot;,&quot;public_building&quot;,&quot;fire_station&quot;,&quot;community_centre&quot;) # Polygons poi_poly_pubserv=as(subset(poi_poly_sh, amenity%in%amenities), &quot;SpatialPolygons&quot;) poi_poly_pubserv_proj=sp::spTransform(poi_poly_pubserv, segmento_sh@proj4string) poi_poly_pubserv_proj_buffer=rgeos::gBuffer(poi_poly_pubserv_proj,width=50) poly_pubserv_proj_r=raster::rasterize(poi_poly_pubserv_proj_buffer, y=raster_dist) dist2poly_pubserv=raster::distance(poly_pubserv_proj_r) # Points poi_points_pubserv=as(subset(poi_points_sh, amenity%in%amenities), &quot;SpatialPoints&quot;) poi_points_pubserv_proj=sp::spTransform(poi_points_pubserv, segmento_sh@proj4string) dist2points_pubserv=raster::distanceFromPoints(raster_dist, poi_points_pubserv_proj) dist2pubamen=min(dist2points_pubserv, dist2poly_pubserv) Lastly, we compute the distance to any public services for each pixel by taking the minimum distance of the four distance maps: # Distance to all public services ##### dist2allpubserv=min(dist2pubamen, dist2schools, dist2health, dist2hosp) 9.6.2 Distance to Businesses The same approach is adopted for the distance to businesses. We start with the distance to financial-services access points, defined as banks and ATMs. # Distance to FAPS #### amenities=c(&quot;bank&quot;,&quot;atm&quot;) poi_poly_fin=as(subset(poi_poly_sh, amenity%in%amenities), &quot;SpatialPolygons&quot;) poi_poly_fin_proj=sp::spTransform(poi_poly_fin, segmento_sh@proj4string) poi_poly_fin_proj_buffer=rgeos::gBuffer(poi_poly_fin_proj,width=50) poly_fin_proj_r=raster::rasterize(poi_poly_fin_proj_buffer, y=raster_dist) dist2poly_fin=raster::distance(poly_fin_proj_r) poi_points_fin=as(subset(poi_points_sh, amenity%in%amenities), &quot;SpatialPoints&quot;) poi_points_fin_proj=sp::spTransform(poi_points_fin, segmento_sh@proj4string) dist2points_fin=raster::distanceFromPoints(raster_dist, poi_points_fin_proj) dist2fin=min(dist2points_fin, dist2poly_fin) We proceed with the other businesses. These are the OpenStreetMap business categories considered in addition to banks and ATMs: * Market * Foodcourt * Restaurant * Fast food vendor * Cafe * Bar * Ice cream shop * Pharmacy * Internet cafe * Cinema * Fuel vendor # Businesses in amenities amenities=c(&quot;marketplace&quot;,&quot;pharmacy&quot;,&quot;restaurant&quot;,&quot;fast_food&quot;,&quot;cafe&quot;,&quot;bar&quot;,&quot;ice_cream&quot;,&quot;food_court&quot;, &quot;internet_cafe&quot;, &quot;cinema&quot;,&quot;fuel&quot;,&quot;atm&quot;,&quot;bank&quot;) # Poly poi_poly_eco=as(subset(poi_poly_sh, amenity%in%amenities), &quot;SpatialPolygons&quot;) poi_poly_eco_proj=sp::spTransform(poi_poly_eco, segmento_sh@proj4string) poi_poly_eco_proj_buffer=rgeos::gBuffer(poi_poly_eco_proj,width=50) poly_eco_proj_r=raster::rasterize(poi_poly_eco_proj_buffer, y=raster_dist) dist2poly_eco=raster::distance(poly_eco_proj_r) # Points poi_points_eco=as(subset(poi_points_sh, amenity%in%amenities), &quot;SpatialPoints&quot;) poi_points_eco_proj=sp::spTransform(poi_points_eco, segmento_sh@proj4string) dist2points_eco=raster::distanceFromPoints(raster_dist, poi_points_eco_proj) In addition to being listed in the amenity field of the OSM data, shop locations are also tagged separately. Given that shops are businesses, they are accounted for in the distance calculations. # Businesses as shop # Poly poi_poly_shop=as(subset(poi_poly_sh, is.na(shop)==F), &quot;SpatialPolygons&quot;) poi_poly_shop_proj=sp::spTransform(poi_poly_shop, segmento_sh@proj4string) poi_poly_shop_proj_buffer=rgeos::gBuffer(poi_poly_shop_proj,width=50) poly_shop_proj_r=raster::rasterize(poi_poly_shop_proj_buffer, y=raster_dist) dist2poly_shop=raster::distance(poly_shop_proj_r) # Points poi_points_shop=as(subset(poi_points_sh, is.na(shop)==F), &quot;SpatialPoints&quot;) poi_points_shop_proj=sp::spTransform(poi_points_shop, segmento_sh@proj4string) dist2points_shop=raster::distanceFromPoints(raster_dist, poi_points_shop_proj) Finally, we gather all business-specific distance maps into one general “Distance to businesses” map. # Collect all businesses dist2biz=min(dist2points_shop, dist2poly_shop, dist2points_eco, dist2poly_eco) 9.6.3 Distance to Natural Features Next, we compute the distance to natural features such as the coastline, urban areas, forested areas, and croplands. We start by cropping the world’s coastlines to El Salvador’s boundaries. Once this operation is complete, we project the map and create the necessary buffer. # Distance to coastline coastline_c=raster::crop(coastline, sp::bbox(SLV_adm0)) ## Warning in proj4string(x): CRS object has comment, which is lost in output ## Warning in proj4string(x): CRS object has comment, which is lost in output coastline_proj=sp::spTransform(coastline_c, segmento_sh@proj4string) coastline_proj_buffer=rgeos::gBuffer(coastline_proj,width=50) coastline_proj_r=raster::rasterize(coastline_proj_buffer, y=raster_dist) dist2coast=raster::distance(coastline_proj_r) To determine the distance to the urban areas, forested areas, croplands, and water bodies, we use NA for pixels that are not urban, not tree covered, not cropland, or not water bodies, respectively. We then use the function raster::distance to compute the distance from each NA pixel to the closest non-NA pixel. # Distance to urban area #### lc_urban=lc lc_urban=raster::calc(lc_urban, fun=function(x) {ifelse(x==190,1,NA)}) lc_urban_proj=raster::projectRaster(lc_urban, crs=segmento_sh@proj4string) ## Warning in rgdal::rawTransform(projfrom, projto, nrow(xy), xy[, 1], xy[, : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projection(obj), crs, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projto_int, projfrom, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings dist2urban=raster::distance(lc_urban_proj) # Distance to forest #### lc_tree=lc lc_tree=raster::calc(lc_tree, fun=function(x) {ifelse(x%in%c(50,60),1,NA)}) lc_tree_proj=raster::projectRaster(lc_tree, crs=segmento_sh@proj4string) ## Warning in rgdal::rawTransform(projfrom, projto, nrow(xy), xy[, 1], xy[, : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projection(obj), crs, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projto_int, projfrom, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings dist2tree=raster::distance(lc_tree_proj) # Distance to cropland #### lc_crop=lc lc_crop=raster::calc(lc_crop, fun=function(x) {ifelse(x%in%c(10,20,30,40),1,NA)}) lc_crop_proj=raster::projectRaster(lc_crop, crs=segmento_sh@proj4string) ## Warning in rgdal::rawTransform(projfrom, projto, nrow(xy), xy[, 1], xy[, : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projection(obj), crs, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projto_int, projfrom, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings dist2crop=raster::distance(lc_crop_proj) # Distance to water #### lc_water=lc lc_water=raster::calc(lc_water, fun=function(x) {ifelse(x%in%c(210),1,NA)}) lc_water_proj=raster::projectRaster(lc_water, crs=segmento_sh@proj4string) ## Warning in rgdal::rawTransform(projfrom, projto, nrow(xy), xy[, 1], xy[, : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projection(obj), crs, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings ## Warning in rgdal::rawTransform(projto_int, projfrom, nrow(xy), xy[, 1], : Using ## PROJ not WKT2 strings dist2water=raster::distance(lc_water_proj) 9.6.4 Stack the Distance Maps and Inspect the Results Once the function raster::resample aligns the rasters on extent and resolution, the distance maps are stacked with the function raster::stack. dist2coast_r=raster::resample(dist2coast, dist2fin) dist2water_r=raster::resample(dist2water, dist2fin) dist2crop_r=raster::resample(dist2crop, dist2fin) dist2tree_r=raster::resample(dist2tree, dist2fin) dist2urban_r=raster::resample(dist2urban, dist2fin) dist_stack=raster::stack(dist2schools_priv, # dist2schools_pub, dist2schools, dist2health, dist2hosp, dist2pubamen, dist2allpubserv, dist2biz, dist2fin, dist2coast_r, dist2water_r, dist2crop_r, dist2tree_r, dist2urban_r) names(dist_stack)=c(&quot;dist2schools_priv&quot;, # &quot;dist2schools_pub&quot;, &quot;dist2schools&quot;, &quot;dist2health&quot;, &quot;dist2hosp&quot;, &quot;dist2pubamen&quot;, &quot;dist2allpubserv&quot;, &quot;dist2biz&quot;, &quot;dist2fin&quot;, &quot;dist2coast_r&quot;, &quot;dist2water_r&quot;, &quot;dist2crop_r&quot;, &quot;dist2tree_r&quot;, &quot;dist2urban_r&quot;) We can now examine the results. The next maps show the distances to financial-service access points, businesses, private schools, and public schools (only the code for distance to financial-service access points is shown). SLV_adm0_proj=sp::spTransform(SLV_adm0, segmento_sh@proj4string) raster::plot(dist_stack$dist2fin, main=&quot;Distance to financial access points, source: OSM&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Figure 9.9: Distance to Financial Access Points raster::plot(dist_stack$dist2biz, main=&quot;Distance to businesses, source: OSM&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Figure 9.10: Distance to Financial Access Points raster::plot(dist_stack$dist2schools_priv, main=&quot;Distance to private school, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) # raster::plot(dist_stack$dist2schools_pub, main=&quot;Distance to public schools, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Figure 9.11: Distance to Financial Access Points raster::plot(dist_stack$dist2urban_r, main=&quot;Distance to urban areas, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Figure 9.12: Distance to Financial Access Points raster::plot(dist_stack$dist2hosp, main=&quot;Distance to hospital, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Figure 9.13: Distance to Financial Access Points raster::plot(dist_stack$dist2pubamen, main=&quot;Distance to public amenities, source: OSM&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Figure 9.14: Distance to Financial Access Points 9.7 Spatial Statistics at the Segmento Level The raster and vector layers built above need to be matched with the EHPM survey data for them to be used in the analysis. This can be done by either (1) identifying the centroid of each segmento and extracting the value of each covariate layer at the centroid of each segmento or (2) computing the spatial average, minimum, or maximum (or other) of each covariate for each segmento. Although the first approach is simpler to implement, the second is preferred: data on poverty, income, and literacy are available at the segmento level because they are area data rather than point-referenced data. Therefore, it is best to provide spatial summary statistics at the same spatial-aggregation level rather than extracting data at the centroid of the segmento, which is an arbitrary location that might not be representative of the prevailing conditions in the segmento. A prerequisite for merging data is to have all data defined by the same CRS. 9.7.1 Adjusting the CRS for Raster Extraction to Segmento To make sure all the preprocessed RS layers are set to the same CRS, the following process is adopted: 1. Create a list of layers 2. Identify the layer with a different CRS 3. Re-project the layer with the different CRS # Create the list of rasters rasters_list=list(chirps_summaries, soil_deg,soil_deg_SDG_index, ndvi_17_summaries,ndvi_17_perc_summaries, dem, gpw_es, temp_summaries, dist2road, dist2roadInter, slope, settlements, dist2water, lights_summaries, lc, dist_stack) sum(unlist(lapply(rasters_list, raster::nlayers))) # Number of layers ## [1] 34 names(rasters_list)=c(&quot;chirps_summaries&quot;, &quot;soil_deg&quot;,&quot;soil_deg_SDG_index&quot;, &quot;ndvi_17_summaries&quot;,&quot;ndvi_17_perc_summaries&quot;, &quot;dem&quot;, &quot;gpw_es&quot;, &quot;temp_summaries&quot;, &quot;dist2road&quot;, &quot;dist2roadInter&quot;, &quot;slope&quot;, &quot;settlements&quot;, &quot;dist2water&quot;, &quot;lights_summaries&quot;, &quot;lc&quot;, &quot;dist_stack&quot;) # Identify the rasters with a different projection different_proj=which(lapply(rasters_list, FUN=function(x) sp::proj4string(x)==sp::proj4string(segmento_sh_wgs))==F) # Reproject these rasters to the same geographic coordinates system as the segmento rasters_list[different_proj]=lapply(rasters_list[different_proj], FUN=function(x) raster::projectRaster(x, crs=sp::proj4string(segmento_sh_wgs))) # Turn the list items into object by replacing the original raster objects with # he new raster objects with the correct coordinate system for(i in 1:length(rasters_list)) { assign(names(rasters_list)[i], rasters_list[[i]]) } The same is done for the list of vector files (not shown), with the only difference being that the function raster::projectRaster is replaced by the function sp::spTransform because a vector is used. 9.7.2 Rasters For each raster layer, we now need to get summary statistics for each segmento area. We will start by getting segmento area average value for each raster layer,11 except for the population, where we will also get the \\(sum\\), that is, total population count per segmento, and land class, where we will get the percentage of segmento area for a subset of classes. The package velox rather than the standard extract function from the raster package is used, as it is much faster.12 rasters_extraction_list=c(&quot;chirps_summaries&quot;,&quot;soil_deg&quot;,&quot;soil_deg_SDG_index&quot;,&quot;ndvi_17_summaries&quot;,&quot;ndvi_17_perc_summaries&quot;, &quot;dem&quot;,&quot;temp_summaries&quot;,&quot;dist2road&quot;,&quot;dist2roadInter&quot;,&quot;slope&quot;,&quot;settlements&quot;,&quot;dist2water&quot;,&quot;lights_summaries&quot;,&quot;dist_stack&quot;) seg_r_s=c() # Create an empty vector to store the extracted data START_e=Sys.time() # Record the start time of the loop for(r in rasters_extraction_list){ # Loop over the list of rasters # Create a velox object from the raster r_velox=velox::velox(get(r)) # The function `get(x)` interprets the character “x” as an # object. In the code snippet below, it allows us to loop through the list of raster names. ex.mat &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) mean(x,na.rm=T), small=T) # Get the mean seg_r_s=cbind(seg_r_s,ex.mat) # Add the extracted data as a new column } ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition, ## but +towgs84= values preserved end_e=Sys.time() print(end_e-START_e) ## Time difference of 2.959754 mins colnames(seg_r_s)=c(names(chirps_summaries), names(soil_deg), &quot;soil_deg_SDG_index&quot;, names(ndvi_17_summaries), names(ndvi_17_perc_summaries), &quot;dem&quot;,names(temp_summaries), &quot;dist2road&quot;,&quot;dist2roadInter&quot;,&quot;slope&quot;,&quot;settlements&quot;,&quot;dist2water&quot;, names(lights_summaries), names(dist_stack)) For the population raster, we get the \\(sum\\) per segment. # Population r_velox=velox::velox(gpw_es) ex.mat_pop &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x,na.rm=T), small=T) # get the sum colnames(ex.mat_pop)=&quot;gpw_es_TOT&quot; For land cover, we extract the share of each segmento area classified as Urban areas Land class: 190 Tree-cover area13 Land class: 50, tree cover broadleaved evergreen closed to open (&gt;15 percent of pixel area) Land class: 60, tree cover broadleaved deciduous closed to open (&gt;15 percent of pixel area) Cropland Land class: 10, cropland rainfed Land class: 20, cropland irrigated or post-flooding Land class: 30, mosaic cropland (&gt;50 percent of pixel area)/natural vegetation (tree/shrub/ herbaceous cover) (&lt;50 percent of pixel area) Land class: 40, mosaic natural vegetation (tree/shrub/herbaceous cover) (&gt;50% of pixel area)/cropland (&lt;50 percent of pixel area) # Land class #### r_velox=velox::velox(lc) # Urban land class ex.mat_urb &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x==190,na.rm=T)/(sum(x==190,na.rm=T)+sum(x!=190,na.rm=T)), small=T) # Get the sum # Tree cover # 50;Tree cover broadleaved evergreen closed to open (&gt;15%);0;100;0 # 60;Tree cover broadleaved deciduous closed to open (&gt;15%);0;160;0 ex.mat_tree &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x%in%c(50,60, 100,110),na.rm=T)/(sum(x%in%c(50,60,100,110),na.rm=T)+sum(!(x%in%c(50,60,100,110)),na.rm=T)), small=T) # Get the sum # crop land # 10;Cropland rainfed;255;255;100 # 20;Cropland irrigated or post-flooding;170;240;240 # 30;Mosaic cropland (&gt;50%) / natural vegetation (tree shrub herbaceous cover) (&lt;50%);220;240;100 # 40;Mosaic natural vegetation (tree shrub herbaceous cover) (&gt;50%) / cropland (&lt;50%) ;200;200;100 ex.mat_crop &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x%in%c(10,20,30,40),na.rm=T)/(sum(x%in%c(10,20,30,40),na.rm=T)+sum(!(x%in%c(10,20,30,40)),na.rm=T)), small=T) # Get the sum colnames(ex.mat_urb)=&quot;lc_urban&quot; colnames(ex.mat_tree)=&quot;lc_tree&quot; colnames(ex.mat_crop)=&quot;lc_crop&quot; Lastly, the extracted data are added to a data frame before merging it with the segmento SpatialPolygonDataframe by binding its columns with the function cbind. seg_r=cbind(seg_r_s, ex.mat_pop, ex.mat_urb, ex.mat_tree, ex.mat_crop) # Compile the extracted data seg_r_df=as.data.frame(seg_r) segmento_sh_wgs@data=cbind(segmento_sh_wgs@data, seg_r_df) # ogrDrivers() # writeOGR(segmento_sh_wgs, # paste0(dir_data, # &quot;spatial/shape/out/STPLAN_Segmentos_WGS84_wdata2.shp&quot;), # driver = &quot;ESRI Shapefile&quot;, # layer=1) # 9.7.2.1 Quality Check We now check that the extraction worked well and did not produce any missing values, which could be caused by the raster layers not providing any value for small segmentos on islands. missing_r=apply(segmento_sh_wgs@data,2,FUN=function(x) which(is.na(x))) missing_r=unique(c(unlist(missing_r))) missing_SEG=segmento_sh_wgs%&gt;% subset(SEG_ID%in%segmento_sh_wgs$SEG_ID[missing_r]) missing_SEG_coords=sp::coordinates(missing_SEG) leaflet(SLV_adm0)%&gt;% addPolygons()%&gt;% addMarkers(missing_SEG_coords[,1],missing_SEG_coords[,2]) Figure 9.15: Observations with No Data We observe in Figure 9.15 that most of the missing values are in border areas. Two main reasons explain these NA: A lot of raster maps produced by WorldPop and HDX used boundaries produced by the open source Global Administrative Boundaries project. Unfortunately, El Salvador’s boundaries in the GADM are incorrect. Most preprocessed rasters are clipped to not overpass the boundaries, coastline, and bodies of water. Thus, some small segmentos at the border, on the coast, or on the shore of Lago Ilopango are not covered. To identify the raster with missing values, we used the code below. missing_r=apply(segmento_sh_wgs@data,2,FUN=function(x) which(is.na(x))) data.frame(n_miss=unlist(lapply(missing_r,length)), var_miss=names(missing_r))%&gt;% arrange(desc(n_miss))%&gt;% filter(n_miss&gt;0) ## n_miss var_miss ## dist2road 23 dist2road ## dist2roadInter 23 dist2roadInter ## settlements 23 settlements ## temp_median 3 temp_median ## soil_carb 2 soil_carb ## soil_prod_prf 2 soil_prod_prf ## chirps_ev_2017 1 chirps_ev_2017 ## chirps_ev_med 1 chirps_ev_med ## soil_luc 1 soil_luc ## soil_prod_state 1 soil_prod_state ## soil_prod_trj 1 soil_prod_trj ## soil_deg_SDG_index 1 soil_deg_SDG_index ## lc_urban 1 lc_urban As their number is relatively small (a max of 22 missing values for 13 covariates), we decided to simply replace the missing value with the cantonal average. For the one segmento where this did not work, we used the municipio average (code not shown). 9.7.3 Vectors For the vectors, a choice has to be made on the type of statistics desired from each vector layer. Here are the spatial summary statistics we will compute: Livelihood Zone Map: the livelihood type of a segmento Schools: private and public schools, including the sum of each category and the total number Health facilities: hospitals and three categories of health centers, including the sum of each category and the sum of categories OpenStreetMap point of interests Buildings, including the total number Points of interest, including the total number Shops, including the total number Amenities, as classified into 12 categories, including the sums within the 12 categories and the total number OpenStreetMap roads, streets, or paths Length of roads, streets, or paths classified in nine categories and their sums Density of roads, streets, or paths classified in nine categories (length of roads/segmento area) Share of rural and urban roads in total roads, streets, or paths of the segmento 9.7.3.1 Livelihood Zones As there is only one categorical value per area, the process is simple: the function over from the package sp is used to extract for each segmento the corresponding value of the “segmento_sh_wgs”’ SpatialPolygonDataFrame. The result of the function over is a data frame with one row per segmento of the segmento_sh_wgs. The livelihood zone code (“LZCODE”) and livelihood zone name (“LZNAMEEN”) are then added into the original data frame of segmento_sh_wgs. # For each segment, extract the corresponding value of the segmento_sh_wgs&#39;s SpatialPolygonDataFrame proj_WGS_84=&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; segmento_sh_wgs=sp::spTransform(segmento_sh_wgs, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved LZ_sh=sp::spTransform(LZ_sh, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved LZ_seg=sp::over(segmento_sh_wgs, LZ_sh) LZ_seg=LZ_seg[,c(&quot;LZCODE&quot;,&quot;LZNAMEEN&quot;)] # Add the livelihood zone data to segmento shape segmento_sh_wgs@data$LZCODE=LZ_seg$LZCODE segmento_sh_wgs@data$LZNAMEEN=LZ_seg$LZNAMEEN Check for any missing values. # Check there are no NA summary(is.na(segmento_sh_wgs@data$LZNAMEEN)) ## Mode FALSE TRUE ## logical 12421 14 We see that 10 segmentos have no assigned livelihood zones. They are mapped in Figure 9.16. missing_SEG=segmento_sh_wgs%&gt;% subset(SEG_ID%in%segmento_sh_wgs$SEG_ID[which(is.na(segmento_sh_wgs@data$LZNAMEEN))]) missing_SEG_coords=sp::coordinates(missing_SEG) leaflet(SLV_adm0)%&gt;% addPolygons()%&gt;% addMarkers(missing_SEG_coords[,1],missing_SEG_coords[,2]) Figure 9.16: Observations with No Data All the missing values are in border areas: as the livelihood zone shapefile is not perfectly aligned to the segmento one, some segmentos fall outside the livelihood zone shapefile. A simple solution is to replace it manually after inspecting the map. missing_canton=which(is.na(segmento_sh_wgs@data$LZNAMEEN)&amp; segmento_sh_wgs@data$CANTON%in%c(&quot;AREA URBANA&quot;,&quot;GUISQUIL&quot;)) segmento_sh_wgs@data$LZNAMEEN[missing_canton]=&quot;Fishing, Aquaculture and Tourism Zone&quot; segmento_sh_wgs@data$LZCODE[missing_canton]=&quot;SV06&quot; missing_canton=which(is.na(segmento_sh_wgs@data$LZNAMEEN)&amp; segmento_sh_wgs@data$CANTON%in%c(&quot;ISLA ZACATILLO&quot;,&quot;ISLA MEANGUERITA&quot;)) segmento_sh_wgs@data$LZNAMEEN[missing_canton]=&quot;Basic Grain and Labor Zone&quot; segmento_sh_wgs@data$LZCODE[missing_canton]=&quot;SV01&quot; missing_canton=which(is.na(segmento_sh_wgs@data$LZNAMEEN)&amp; segmento_sh_wgs@data$CANTON==&quot;EL JOCOTILLO&quot;) segmento_sh_wgs@data$LZNAMEEN[missing_canton]=&quot;Sugarcane, Agro-Industry and Labor Zone&quot; segmento_sh_wgs@data$LZCODE[missing_canton]=&quot;SV03&quot; summary(is.na(segmento_sh_wgs@data$LZNAMEEN)) ## Mode FALSE ## logical 12435 summary(is.na(segmento_sh_wgs@data$LZCODE)) ## Mode FALSE ## logical 12435 9.7.3.2 Schools The process of extracting the schools’ and health facilities’ data follows the same principle. The difference is that the data are point data instead of polygon data. The schools and health facilities datasets are the SpatialPointsDataFrame. The SpatialPointsDataFrame is first transformed into a SpatialPoints object using the function as. The function over is then run, with an additional parameter specified: fn=sum. This tells the function over to compute the sum of SpatialPoints per segmento, that is, the number of schools per segmento. For segmentos where there are no schools, the function over yields an NA value. NA values a replaced by zeros. # Schools # Total sum of schools sch_seg=sp::over(segmento_sh_wgs, as(school_sh, &quot;SpatialPoints&quot;), fn=sum,na.rm=T) # Replace the NA value sch_seg[which(is.na(sch_seg))]=0 The same process is done for public and private schools separately. The function subset is used to subset only one category. # N public schools sch_pub_seg=sp::over(segmento_sh_wgs, as(subset(school_sh, Sector!=&quot;PRIVADO&quot;), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) sch_pub_seg[which(is.na(sch_pub_seg))]=0 # N private schools sch_pri_seg=sp::over(segmento_sh_wgs, as(subset(school_sh, Sector==&quot;PRIVADO&quot;), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) sch_pri_seg[which(is.na(sch_pri_seg))]=0 Lastly, the newly created covariates are added to the SpatialPolyongsDataFrame segmento_sh_wgs. # Add the school data to segmento shape segmento_sh_wgs@data$sch=sch_seg segmento_sh_wgs@data$sch_pub=sch_pub_seg segmento_sh_wgs@data$sch_pri=sch_pri_seg The process is repeated for health facilities before adding the covariate to segmento_sh_wgs (not shown). 9.7.3.3 OpenStreetMap Data The OpenStreetMap data are provided in three sets of files: Buildings polygons (hotosm_slv_buildings_polygons.shp) Points-of-interest points (hotosm_slv_points_of_interest_points.shp) Points-of-interest polygons (hotosm_slv_points_of_interest_polygons.shp) For the buildings data, the sum of buildings per segmento is computed. This indicates how urban a segmento is. proj_WGS_84=&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; buildings_poly_sh=sp::spTransform(buildings_poly_sh, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved build_seg=sp::over(segmento_sh_wgs, as(buildings_poly_sh, &quot;SpatialPolygons&quot;), fn=sum,na.rm=T) build_seg[which(is.na(build_seg))]=0 # Add the building data to segmento shape segmento_sh_wgs@data$building=build_seg For the points of interest vectors (points and poly), the number of locations per segmento identified as a “shop” in OSM, that is, a place selling retail products or services, is calculated. This indicates how urban a segmento is and can shed some light on local economic dynamism. proj_WGS_84=&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; poi_poly_sh=sp::spTransform(poi_poly_sh, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved # Shop shop_poly_seg=sp::over(segmento_sh_wgs, as(subset(poi_poly_sh, is.na(shop)==F), &quot;SpatialPolygons&quot;), fn=sum,na.rm=T) shop_poly_seg[which(is.na(shop_poly_seg))]=0 shop_points_seg=sp::over(segmento_sh_wgs, as(subset(poi_points_sh, is.na(shop)==F), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) shop_points_seg[which(is.na(shop_points_seg))]=0 shop_n=shop_points_seg+shop_poly_seg # Add the shop data to segmento shape segmento_sh_wgs@data$shop=shop_n Next, the number of amenities according to 11 categories is computed. ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify format ## in kable. kableExtra can customize either HTML or LaTeX outputs. See https:// ## haozhu233.github.io/kableExtra/ for details. Category Feature’s name in the data OSM amenity categories Financial access points fin bank, atm Marketplace mkt marketplace Bus Station bus bus_station Parking parking parking Place of Worship worship place_of_worship Health service health_srv pharmacy, clinic, dentist, doctor, hospital Food and drink food restaurant, fast_food, cafe, bar, ice_cream, food_court Culture and Education cult_educ school, college, university, community_centre, theatre, arts_centre, internet_cafe, cinema Fuel: Petrol station fuel fuel Official buildings official townhall, post_office, courthouse, police, prison, public_building, fire_station The for loop below executes the extraction. for(i in 1: length(list_amenities)){ extracted_poly_seg=sp::over(segmento_sh_wgs, as(subset(poi_poly_sh, amenity%in%list_amenities[[i]]), &quot;SpatialPolygons&quot;), fn=sum,na.rm=T) extracted_poly_seg[which(is.na(extracted_poly_seg))]=0 extracted_point_seg=sp::over(segmento_sh_wgs, as(subset(poi_points_sh, amenity%in%list_amenities[[i]]), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) extracted_point_seg[which(is.na(extracted_point_seg))]=0 extracted_seg=extracted_point_seg+extracted_poly_seg assign(list_amenities_name[[i]], extracted_seg) } # Add the shop data to segmento shape unlist(list_amenities_name) ## [1] &quot;fin&quot; &quot;mkt&quot; &quot;bus&quot; &quot;parking&quot; &quot;worship&quot; ## [6] &quot;health_srv&quot; &quot;food&quot; &quot;cult_educ&quot; &quot;fuel&quot; &quot;official&quot; segmento_sh_wgs@data$fin=fin segmento_sh_wgs@data$mkt=mkt segmento_sh_wgs@data$bus=bus segmento_sh_wgs@data$parking=parking segmento_sh_wgs@data$worship=worship segmento_sh_wgs@data$health_srv=health_srv segmento_sh_wgs@data$food=food segmento_sh_wgs@data$cult_educ=cult_educ segmento_sh_wgs@data$fuel=fuel segmento_sh_wgs@data$official=official Lastly, a series of covariates is computed on the roads data. As the goal is to measure length, the first step is to change the CRS of the roads vector into a projected coordinates system with units expressed in meters. The CRS of the original segmento data is used. roads_lines_pr=sp::spTransform(roads_lines_sh, sp::proj4string(segmento_sh)) ## Warning in sp::proj4string(segmento_sh): CRS object has comment, which is lost ## in output The OSM ‘roads’ are then regrouped into nine categories according to their highway class. ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify format ## in kable. kableExtra can customize either HTML or LaTeX outputs. See https:// ## haozhu233.github.io/kableExtra/ for details. Category Feature’s name OSM highway class Fast track fast_track motorway_link, motorway, trunk_link, trunk, primary_link, primary Secondary secondary secondary, secondary_link Tertiary tertiary tertiary_link, tertiary Residential residential residential, corridor Rural track rural_track track Pedestrian and bike line urban_no_motor living_street, cycleway, steps, path, footway, pedestrian, service Bus lane bus bus_stop, bus_guideway Luxury luxury raceway, bridleway Unclassified unclassified no construction, unclassified, road The length of roads’ segments per segmento and according to the roads’ categories is computed using the function lineLength applied via over. The results are expressed in meters, as the unit of the CRS is meters. # Length of roads proj_WGS_84=&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; segmento_sh=sp::spTransform(segmento_sh, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved roads_lines_pr=sp::spTransform(roads_lines_sh, proj_WGS_84) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved length_all_seg=sp::over(segmento_sh, as(roads_lines_pr, &quot;SpatialLines&quot;), fn=lineLength, na.rm=T) length_all_seg[is.na(length_all_seg)]=0 for(i in 1:length(list_road_types)){ roads_type_sub=subset(roads_lines_pr, highway %in% list_road_types[[i]]) length_seg=sp::over(segmento_sh, as(roads_type_sub, &quot;SpatialLines&quot;), fn=lineLength, na.rm=T) length_seg[is.na(length_seg)]=0 assign(list_road_names[[i]], length_seg) } # Add the road length data to the shapefile segmento_sh_wgs@data$all_roads=length_all_seg segmento_sh_wgs@data$fast_track=fast_track segmento_sh_wgs@data$secondary=secondary segmento_sh_wgs@data$tertiary=tertiary segmento_sh_wgs@data$residential=residential segmento_sh_wgs@data$rural_track=rural_track segmento_sh_wgs@data$urban_no_motor=urban_no_motor segmento_sh_wgs@data$bus=bus segmento_sh_wgs@data$luxury=luxury segmento_sh_wgs@data$unclassified=unclassified 9.7.3.4 Normalizing the Road Data The road network is normalized by the area of segmento: we obtain the kilometers of roads per square kilometer. The density of roads is computed as the length of roads divided by the area of segmento. The latter is already computed in the shapefile and stored in the feature Shape_Area. # Density: length km/area road_density_all=length_all_seg/segmento_sh@data$Shape_Area for(i in 1:length(list_road_types)){ density_type_i=get(list_road_names[[i]])/segmento_sh_wgs@data$Shape_Area assign(paste0(&quot;dens_&quot;, list_road_names[[i]]), density_type_i) } # Add the road density data to the shapefile segmento_sh_wgs@data$dens_all_roads=road_density_all segmento_sh_wgs@data$dens_fast_track=dens_fast_track segmento_sh_wgs@data$dens_secondary=dens_secondary segmento_sh_wgs@data$dens_tertiary=dens_tertiary segmento_sh_wgs@data$dens_residential=dens_residential segmento_sh_wgs@data$dens_rural_track=dens_rural_track segmento_sh_wgs@data$dens_urban_no_motor=dens_urban_no_motor segmento_sh_wgs@data$dens_bus=dens_bus segmento_sh_wgs@data$dens_luxury=dens_luxury segmento_sh_wgs@data$dens_unclassified=dens_unclassified 9.7.4 Population Density We compute the average population density by dividing the segmento total population by its areas. segmento_sh_wgs@data$pop_dens=segmento_sh_wgs@data$gpw_es_TOT/segmento_sh_wgs@data$Shape_Area/10000 9.8 Merging the Covariates with the EHPM Data The data from SpatialPolygonDataframe are saved to a standard data frame. Once the EHPM survey data tables/ehpm-2017.csv are loaded, select only the development indicators and the household id “idboleta.” The idboleta is used to match the EHPM with the segmento ID in tables/Identificador de segmento.xlsx. Lastly, the SEG_ID is used to match the survey data with the predictor. # Save the data stored in segmento_sh_wgs@data in a standard data frame # Select a subset of the covariates predictors=segmento_sh_wgs@data # Survey data ehpm17=read.csv(paste0(dir_data, &quot;tables/ehpm-2017.csv&quot;)) # Read the files with the segmento ID and the idboleta segID=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;), sheet=&quot;2017&quot;) # Select only the development indicators ehpm17_dev_indic=ehpm17%&gt;% dplyr::select(municauto, idboleta, pobreza, # Poverty ingpe, # Household income r202a, # Sabe leer y escribir (knows how t oread and write) r106 # Age ) %&gt;% dplyr::rename(&quot;lit&quot;=&quot;r202a&quot;, # Rename these 2 variables &quot;age&quot;=&quot;r106&quot;)%&gt;% dplyr::mutate(over_15=ifelse(age&gt;=15,1,0), literate=ifelse(lit==1,1,0), literate_over_15=over_15*literate) # Literate and over 15 years old # Match the survey data with the segID ehpm17_seg=ehpm17_dev_indic%&gt;% left_join(segID, by=&quot;idboleta&quot;)%&gt;% rename(&quot;SEG_ID&quot;=&quot;seg_id&quot;) Below, we filter out all the survey participants who are less than 16 years old and compute development indicators for the population that is 16 years of age and above. # Aggregate the survey data at the segmento level ehpm17_seg_agg=ehpm17_seg%&gt;% filter(over_15==1)%&gt;% group_by(SEG_ID)%&gt;% summarise(municauto=unique(municauto), n_obs=n(), pobreza_extrema=sum(pobreza==1,na.rm = T)/n(), pobreza_mod=sum(pobreza%in%c(1,2),na.rm = T)/n(), literacy_rate=sum(literate_over_15,na.rm = T)/n(), ingpe=median(ingpe,na.rm = T)) ## `summarise()` ungrouping output (override with `.groups` argument) # Merge the aggregated survey data with the predictor data ehpm17_predictors=ehpm17_seg_agg%&gt;% right_join(predictors%&gt;% mutate(SEG_ID=as.character(SEG_ID)), by=&quot;SEG_ID&quot;) # Write out the file write.csv(ehpm17_predictors, paste0(dir_data, &quot;out/ehpm17_predictors2.csv&quot;), row.names = F) print(paste(ncol(ehpm17_predictors)-21, &quot;predictors&quot;)) ## [1] &quot;80 predictors&quot; The .csv file out/ehpm17_predictors.csv contains the 79 predictors derived above plus the following development indicators: ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify format ## in kable. kableExtra can customize either HTML or LaTeX outputs. See https:// ## haozhu233.github.io/kableExtra/ for details. Variable name Definition pobreza_extrema Proportion of individuals 15 years in extreme poverty: income below pobreza_mod Proportion of individuals 15 years in moderate poverty: income below $107.26 in urban areas or $66.90 in rural areas literacy_rate Proportion of individuals 15 years or older who can read and write ingpe Median income of individuals 15 years or older (USD) 9.9 Summary The process of going from raw RS layers to a SpatialPolygonDataframe ready for analysis can be summarized as follows: Load the RS layers. Create summaries for the RS layers that come with a time dimension (e.g., average precipitation over the last 30 years). Perform additional raster calculation as required (e.g., compute the Soil degradation index based on a soil degradation sub-indicator). Compute the distance metrics to public services, businesses, and some natural features. Align the CRSs across layers (geographic or projected). Aggregate the RS layers to the map of interest (e.g., the segmento map) with a chosen set of spatial statistics. Match the RS aggregates with the outcome data (poverty, income, literacy). Write the data frame to a .csv file for later use. The 22 main data sources are summarized in the SpatialPolygonDataframe “segmento_sh_wgs” into 79 segmento-level spatial statistics listed on the next table. These are the candidate covariates to predict average income, poverty, and literacy measured in the 2017 EHPM survey. ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify format ## in kable. kableExtra can customize either HTML or LaTeX outputs. See https:// ## haozhu233.github.io/kableExtra/ for details. Category Features Precipitation chirps_ev_2017, chirps_ev_med Soil degradation soil_carb, soil_luc, soil_prod_state, soil_prod_prf, soil_prod_trj, soil_deg_SDG_index Vegetation Greenness (NDVI) ndvi_17_median, ndvi_17_sd, ndvi_17_sum, ndvi_17_perc_median, ndvi_17_perc_sd, ndvi_17_perc_sum Altitude (DEM) dem Population density gpw_es, pop_dens, gpw_es_TOT Temperature temp_median Livelihood Zones LZCODE,LZNAMEEN Education dist2schools_priv, dist2schools_pub,dist2schools Health services dist2health, dist2hosp Buildings people_building Shops See distances Other points of interest See distances Roads dens_all_roads, dens_fast_track, dens_secondary, dens_tertiary, dens_residential, dens_rural_track, dens_urban_no_motor, dens_bus, dens_luxury, dens_unclassified Slope slope Distances dist2road, dist2roadInter, dist2water Lights at night lights_mean, lights_sd Land Classes lc_urban,lc_crop,lc_tree Distance dist2schools_priv, dist2schools_pub, dist2schools, dist2health, dist2hosp, dist2pubamen, dist2allpubserv, dist2biz, dist2fin, dist2coast_r, dist2water_r, dist2crop_r, dist2tree_r, dist2urban_r References "],
["references.html", "References", " References ’` "]
]
