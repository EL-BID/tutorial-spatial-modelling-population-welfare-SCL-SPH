[
["index.html", "High-resolution spatial modelling of population welfare in El Salvador, a coding tutorial Section 1 Introduction", " High-resolution spatial modelling of population welfare in El Salvador, a coding tutorial Xavier Vollenweider, Claudio Bosco, Roberto Sánchez A, Luis Tejerina 2021-03-25 Section 1 Introduction The UN sustainable development goals (SDGs) number one is to end poverty in all its forms everywhere by 2030 while SDG goal number four and its 4.6 target aims at ensuring that all youth and a substantial proportion of adults, both men and women, achieve literacy and numeracy. The aim for the SDGs to be attained “everywhere” means that no one should be left behind. Therefore, populations at subnational levels and local heterogeneities are to be taken into account in national statistics, ensuring representative monitoring and optimised intervention planning. Mapping with great detail the geographic distribution of populations, of their chararacteristics and of the SDG indicators is hence a central tool for meeting the SDGs. National households surveys are typically representative at the regional level (administrativel level 1). However, the households conditions may vary at much finer scale. For instance, poverty level within a town and in rural areas of the same region can differ markedly. Similarly, poverty between rural towns and the neigbourings hamlet can exhbit large differences while local economic activities (e.g. the presence or absence of a manufacture) can largely determine SDGs outcomes. Information at higher resolution is hence required to support the efficient allocation of ressources across territories and the monitoring of SDG indicators. Conventional approaches to producing high resolution development indicators rely on Small Area Estimation (SAE) methods integrating household survey with census data to estimate the proportion of households in poverty. Household surveys, conducted every 1 to 5 years, have been improved through the introduction of geolocated survey clusters providing more fine-grained spatial data thanks to global positioning systems (GPS). In El Salvador, the national household survey Encuesta de Hogares de Propositos Multiples (EHPM) is conducted on an annual basis. Censuses, on the other hand, are typically undertaken irregularly, sometimes up to every 10 years, or longer in many low-income countries. In El Salvador, the last census was conducted in 2007. The reliance on the latter may weaken the reliability of SAE estimates, preventing an ongoing monitoring of SDG indicators. Geo-referenced national household survey data provide an opportunity to achieve more spatially detailed, accurate and regular estimates of poverty distribution and other SDG indicators. To further improve these estimates, novel sources of spatial data are increasingly being utilised to fill in the outdated census gap. Continually collected information such as rainfall, temperature and vegetation, also called remote sensing and geographic information system data (RS), capture information related to agricultural productivity, while light at nights and distance to roads and cities refects access to markets and information and local economic dynamism. Spatial interpolation approaches consist of overlapping such data with more traditional sources such as survey-based data in order to produce regularly-updatable high-resolution maps of development indicators. Here we use spatial interpolation methods by integrating household survey cluster data with geospatial covariates to produce high-resolution poverty, income and literacy maps for El Salvador. This report is a coding tutorial for creating high resolution maps of SDG indicators. It outlines the relevant data, how they can be processed and analysed. The emphasis is put on providing reproducible codes and exemples and all the RS data are open source. The codes to go from the raw data to the high resolution maps are presented in details. This report is written with the Markdown (Allaire et al. 2019) and the bookdown (Xie 2016) packages. The tutorial assumes that the reader is familiar with the open source statistical computing environment R. For a introductory book on using R, refer to https://cengel.github.io/R-intro/. For an introductory book on using Spatial Data with R, refer to https://cengel.github.io/R-spatial/. The tutorial is structured as follow. Chapter 2 provides an overview of the analysis. Chapter 3 introduces the reader to data exploration and unsupervised predictors selection. In Chapter 4, a simple Bayesian model is fitted. In chapter 5, covariate selection with the Jacknife approach is presented. Chapter 5 shows the results of the selected models and produces the map. Chapter 6 concludes. Appendix 1 provides a short primer on INLA. Appendix 2 provides the codes required to pre-process the RS data, starting from the raw raster and vector layers up to the data in row and columns format used to fit the models. References "],
["overview.html", "Section 2 Overview 2.1 First look at the data", " Section 2 Overview The aim of the analysis is to create a high resolution map of poverty, income and literacy for El Salvador. The data on these three development indicators come from the 2017 household survey Encuesta de Hogares de Propositos Multiples (EHPM). While the survey data are only available for 1,664 segmentos, the lowest administrative units, the goal is to provide a map of the three development indicators for all the 12,435 segmentos of the country. This is made possible by building a statistical model exploiting the relationship between remote sensed (RS) data (e.g. lights a night measured from satellite, precipitation), the EHPM survey data and the spatial correlation between SDGs outcomes across space. Once the relationship between the EHPM survey data and the RS data is modeled in locations where EHPM survey data are available, RS data, which are available for the entire country, are used to predict SDG outcomes across the entire country. Furthermore, as SDG outcomes of two segmentos are more likely to be similar if both segmentos are neighbours than if they are far away from each other, the accuracy of the spatial prediction can be enhanced by taking explicitly into account the spatial distances and spatial relationships between segmentos. The interpolation technique we use in this report is based on Bayesian geospatial methods implemented in the open source statistical computing environment R (R Core Team 2018a). The core of the modelling method is implemented in the R package INLA (Rue, Martino, and Chopin 2009; Lindgren, Rue, and Lindström 2011; Martins et al. 2013; Lindgren and Rue 2015). Several other R packages are used in this tutorial. The main ones are dplyr (Wickham et al. 2019) for data handling, plotly (Sievert 2018) for interactive plots, leaflet (Cheng, Karambelkar, and Xie 2018) for interactive maps, raster (R. J. Hijmans 2019a) for handling raster data, rgdal (Bivand, Keitt, and Rowlingson 2019) for loading vector files, sp (Bivand, Pebesma, and Gomez-Rubio 2013) for adjusting coordinates systems, energy (Rizzo and Szekely 2018) to compute distance correlation and parallel (R Core Team 2018b) to parallise the covariates selection process. These are the five main steps of the analysis: Data pre-processing Covariate pre-selection Model fitting Diagnostic checks Out of sample spatial interpolation to create high resolution maps. A brief introduction to the INLA method is provided in Section 8. Three books are recommended for those willing to get more information on the INLA approach: for readers with only a minimal quantitative background: Beginner’s Guide to Spatial, Temporal, and Spatial-Temporal Ecological Data Analysis with R-Inla (Zuur, Ieno, and Saveliev 2017), for those willing to delve deeper in the methodology behind INLA: Spatial and Spatio-Temporal Bayesian Models with R-Inla (Blangiardo and Cameletti 2015). for those interested by an application to public health: Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny(Moraga 2019) Bakka et al. (Bakka et al. 2018) provide a good overview of spatial modelling with INLA and Steele et al. (Steele et al. 2017) provide a great example of the use of INLA for poverty mapping. For information about the R-INLA package, please refer to the R-INLA project website. 2.1 First look at the data A total of 20,609 households are interviewed in the 2017 EHPM. The EHPM data are collected in 1,664 segmentos out of the 12,435 segmentos of El Salvador. The number of households interviewed per segmento is mapped on Fig ??. Here are the required steps to create the map: load the required packages: rgdal, dplyr and leaflet; Load the: segmentos shapefile with rgdal, list of households per segmento; count the number of households per segmento with dplyr and identify the segmentos where at least 1 survey recipient exists; map the results with leaflet. Let us start by loading packages and the initial data with rgdal. # load packages library(rgdal) library(dplyr) library(leaflet) #source(&quot;../utils.R&quot;) # modify dir_data to where you stored the data root_dir=&quot;~/&quot; project_dir=&quot;data/&quot; dir_data=paste0(root_dir,project_dir) # load segemento shapefile map (simplified for faster rendering): segmentos_to_map=readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Segmentos_simplified.shp&quot;)) # load the households list surveyed in the EHPM per segmento: id_segmento_2017=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;), sheet = 2) We then compute the number of participants per segmento with dplyr. id_segmento_2017_df=id_segmento_2017%&gt;% rename(SEG_ID=seg_id)%&gt;% group_by(SEG_ID)%&gt;% # group data per segmento summarise(ehpm_2017_d=1, # 1 if there is hh in the segmento ehpm_2017_n=n()) # # count number of household per segmento with the function n() Before merging the results with the shapefile, let us turn the identifier of segmentos in the shapefile, “SEG_ID”, into character format. This is to make sure the match between the SEG_ID stored as a character in the household list id_segmento_2017_df and the shapefile map segmentos is correct. segmentos_to_map@data=segmentos_to_map@data%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% # turn SEG_ID into character format left_join(id_segmento_2017_df, # merge the household list with the shapefile data by=&quot;SEG_ID&quot;) We can now map all the segmentos where the EHPM data have been collected. This is done with the package leaflet. Please be patient when running the next command: rendering the 12,435 shapes might take a bit of time. leaflet(segmentos_to_map) %&gt;% # leaflet is used to render the map addProviderTiles(providers$CartoDB.Positron)%&gt;% addPolygons(color = &quot;#444444&quot;, # color of the line of the border of each segmento weight = 1, # thickness of the line of the border of each segmento smoothFactor = 1, # simplify the shape to speed rendering opacity = 0, # opacity of the segmento border lines fillOpacity = 1, # opacity of the segmento areas fillColor = ~colorQuantile(&quot;Greens&quot;, # define the color ramps of polygons ehpm_2017_n,na.color = &quot;transparent&quot;)(ehpm_2017_n)) Number of participants surveyed per segmento The aim of our modelling exercise is to obtain estimates of income, literacy and poverty for the segmentos where no EHPM data is available, i.e. for the blanks on Fig. ??. References "],
["explo.html", "Section 3 Data exploration and preselection of covariates 3.1 Outcome variabes: distribution and outliers 3.2 Unsupervised dimension reduction based on distance correlation 3.3 Correlations between predictors and the outcome variables", " Section 3 Data exploration and preselection of covariates The data used in the model can be separated in two main categories: the outcome variables, i.e. the three development indicators (income, poverty and literacy) the covariates, i.e. the set of predictors derived in Section 9 and which will be used to predict the outcome variables in segmentos not sampled in the EHPM survey. When exploring the outcome data, the aims is to identify possible issues such as a lack of variation or the presence of outliers. It helps also in specifying the likelihood function for the outcome data (e.g. Gaussian, Gamma, Beta, etc). For the covariates, we will start by reducing their number in order to limit the effect of multicollinearity, i.e. the presence of highly correlated covariates in the model. Reducing the number of covariates will also reduce the risk of retaining covariates because of chance correlation in a next stepwise covariates selection process carried in section 5. Lastly, we will investigate the correlation of the preselected covariates with the outcome variables. 3.1 Outcome variabes: distribution and outliers 3.1.1 Income We start by exploring the distribution of the median income per segmento. It is named ingpe in the dataset: Median income per segmento As it is common with incomes’ distribution, the distribution is right skewed: the median income is between 95 and 105 USD in most segmentos and reaches values above 200 USD in a few of them. An option is to log-transform the data to make it Gaussian1. Another option is to model income with a Gamma likelihood function which takes into account the asymmetry of the income distribution. Median income map Fig. ?? shows that all the segmentos with a median income above 265 USD (blue markers) are clustered together in urban centers while the four ones with a income above 700 USD (gold markers) are located in Antiguo Cuscatlan, one of wealthiest neighborhood of the country where several embassies are located. DIGESTYC provides a classification of segmentos into rural and urban one. Fig ?? shows the log transformed distribution of median income for rural and urban segmentos separately. Rural urban median income distributiuon Fig ?? shows that log transformation appears to be successful at making the distribution Gaussian. As already seen on Fig. ??, median income is higher among urban segmentos than rural segmentos. Furthermore, the main difference between the rural and urban income distributions is a location shift: the variance or skewness of the income distribution doesn’t appear to vary significantly between rural and urban segmentos. Fig ?? suggests hence that a single model could be used provided that a binary covariate identifying rural and urban segmentos is used in the model (instead of running two separate models for rural and urban segmentos). As a summary, this first exploration of the median income outcome data suggests that one could use a Gamma likelihood function or a Gaussian one. In the latter case, income should be log-transformed. Furthermore, there are clear spatial patterns, whereby urban areas have higher income levels than rural ones and segmentos close from each other tend to have similar income levels. Modelling explicitly spatial dependency is hence likely to improve the goodness of fit of the model. Lastly, we do not see any major difference between the distributions of income of rural and urban segmentos except for a location shift (all values shifted to the right among urban segmento). A single model ran both on rural and urban segmentos appears hence appropriate, provided a binary covariate identifying the rural/urban categories is used. 3.1.2 Literacy The variable literacy_rate provides the proportion of survey participants 16 years or older who can read and write. Its domain is hence bounded between 0 to 100%. The distribution of literacy in rural and urban segmentos appears quite different. Out of the 950 urban segmentos, 181 have 100% literacy rate among the adult EHPM respondents. By contrast, no such spike is observed in the rural areas. In urban areas, this suggest that literacy could be modeled as a two step process: first, the presence/absence of illiteracy is modeled; second, the proportion of illiteracy is modeled. In rural areas, only the second step would be necessary. Fig ?? shows the map of literacy for the EHPM segmentos. There is a clear spatial pattern: literacy rates are the highest in urban areas and particularly around the capital city. Furthermore, areas in the north west (Department of San Miguel, Morazan and La Union) have low level of literacy. As a summary, modelling literacy will be more complex than income. First, a zero-inflated model might be appropriate, particularly in for urban segmentos. Second, the distribution of literacy appears to vary between rural and urban segmentos and not only in terms of the location. Hence separate or mixed models for rural and urban segmentos might be more appropriate. In both cases, a Beta likelihood function might be appropriate as literacy rate are bounded between 0 and 100%. Lastly, there is a clear spatial pattern, which suggests that taking into account explicitly the spatial dependence structure will increase the goodness of fit of the model. 3.1.3 Moderate Poverty The variable pobreza_mod provides the proportion of survey participants 16 years or older living in moderate poverty. Its domain is hence bounded between 0 to 1. Fig ?? shows that more than 100 urban segmentos have no adults EHPM respondents living in poverty, while no rural segmento is in this situation. Except for this important difference, the distribution of poverty of rural and urban segmentos doesn’t appear to differ much. Fig ?? shows the map of moderate poverty for the EHPM segmentos. Interestingly, the picture is much less clear when mapping moderate poverty than income. However, the general urban-rural divide appears to hold. As poverty is measured as a proportion, we will use a Beta likelihood. Given the high number of segmentos with zero respondents living in poverty, we will test a zero-inflated Beta likelihood. Lastly, as the excess number of zeros is only present among urban segmentos, we will compare results where two separate models for rural and urban segmentos with the results where only one model is ran 3.2 Unsupervised dimension reduction based on distance correlation We are now going to reduce the number of covariates by removing highly correlated covariates. Before starting the process, we standardize the covariates in order to limit the effect of the various scales and measurement units of each covariates on the computation of the correlation. 3.2.1 Obtaining a dataframe with standardized covariates We start by storing the candidate covariates in an data frame. # join wiyth shapefile #### segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) # identify EHPM segmentos #### segmento_sh_data_model=segmento_sh_data ehpm_index=which(is.na(segmento_sh_data_model$n_obs)==F) ehpm17_predictors_survey=segmento_sh_data_model@data[ehpm_index,] # Remove outcome variables and admin information #### # covariates=ehpm17_predictors_survey%&gt;% # select(-c(SEG_ID,municauto,n_obs,pobreza_extrema,pobreza_mod,literacy_rate,ingpe,DEPTO, # COD_DEP,MPIO,COD_MUN,CANTON,COD_CAN,COD_ZON_CE,COD_SEC_CE,COD_SEG_CE,LZCODE, # ndvi_17_perc_median,ndvi_17_perc_sd,ndvi_17_perc_sum, # chirps_ev_perc_norm,chirps_ev_2017, chirps_ev_perc_norm, # dist2allpubserv,lights_sd,gpw_es_TOT,gpw_es_avg_dens,dist2water_r, # soil_prod_state,soil_carb,soil_luc,soil_prod_prf,soil_prod_state,soil_prod_trj,settlements)) covariates=ehpm17_predictors_survey%&gt;% select(-c(SEG_ID,municauto,n_obs,pobreza_extrema,pobreza_mod,literacy_rate,ingpe,DEPTO, COD_DEP,MPIO,COD_MUN,CANTON,COD_CAN,COD_ZON_CE,COD_SEC_CE,COD_SEG_CE,LZCODE,OBJECTID, SHAPE_Leng,AREA_KM2,dens_bus,dens_luxury)) # create binary variables from the livelihood zones categories covariates=covariates%&gt;% mutate(AREA_ID=ifelse(AREA_ID==&quot;U&quot;,1,0), LZ_BasicGrainLaborZone=ifelse(LZNAMEEN==&quot;Basic Grain and Labor Zone&quot;, 1,0), LZ_CentralIndusServ=ifelse(LZNAMEEN==&quot;Central Free-Trade, Services and Industrial Labor Zone&quot;, 1,0), LZ_CoffeeAgroIndus=ifelse(LZNAMEEN==&quot;Coffee, Agro-Industrial and Labor Zone&quot;, 1,0), LZ_LvstckGrainRem=ifelse(LZNAMEEN==&quot;Eastern Basic Grain, Labor, Livestock and Remittance Zone&quot;, 1,0), LZ_FishAgriTourism=ifelse(LZNAMEEN==&quot;Fishing, Aquaculture and Tourism Zone&quot;, 1,0))%&gt;% select(-LZNAMEEN) We write a function myStd to standardise the covariates by subtracting the mean from each observation and dividing by the standard deviation. myStd=function(x){ # myStd is a function to standaridise a given covariate x mu_x=mean(x,na.rm = T) sd_x=sd(x,na.rm = T) x_std=(x-mu_x)/sd_x return(x_std) } We apply myStd to each covariate. covariates_std=apply(covariates,2,myStd) # with apply the function myStd to each covariate The dataframe covariates_std contains the standardized covariates. Before removing highly correlated covariates, we compute the variance inflation factor (VIF) in order to assess to presence of multicollinearity. # calculate vif before cov selection #### covariates_std_df=as.data.frame(covariates_std) data_model=covariates_std_df%&gt;% bind_cols(ehpm17_predictors_survey%&gt;% select(ingpe)) formula_glm= reformulate(paste(names(covariates_std_df),collapse=&quot;+&quot;), &quot;ingpe&quot;) glm_0=glm(formula_glm, data=data_model) sort(car::vif(glm_0)) ## luxury bus hospital ## 1.041469 1.063869 1.105755 ## fuel mkt parking ## 1.130935 1.188140 1.208130 ## soil_prod_prf cult_educ official ## 1.242899 1.252937 1.348125 ## food fin dens_rural_track ## 1.360186 1.386133 1.403419 ## building worship dens_fast_track ## 1.420363 1.435243 1.436840 ## dens_secondary shop secondary ## 1.455963 1.464974 1.473073 ## fast_track tertiary residential ## 1.485937 1.537712 1.538448 ## dens_unclassified health_srv rural_track ## 1.541739 1.551940 1.607205 ## dens_tertiary unclassified dist2health ## 1.705620 1.733906 1.858782 ## all_roads urban_no_motor LZ_FishAgriTourism ## 1.931947 1.932650 1.989607 ## dist2hosp dist2crop_r dens_residential ## 2.181394 2.283840 2.323513 ## slope dist2pubamen dist2road ## 2.372871 2.465642 2.508210 ## soil_prod_trj dist2tree_r gpw_es_TOT ## 2.522349 2.533267 2.596605 ## dist2schools_priv Shape_Area sch_pri ## 2.686345 2.933353 3.113431 ## dist2roadInter dens_all_roads soil_luc ## 3.142500 3.152475 3.218847 ## AREA_ID LZ_LvstckGrainRem pop_dens ## 3.249108 3.327067 3.331335 ## LZ_CoffeeAgroIndus dens_urban_no_motor dist2biz ## 3.415023 3.428458 3.444821 ## chirps_ev_2017 dist2coast_r LZ_BasicGrainLaborZone ## 3.445628 3.510607 3.593043 ## dist2urban_r soil_carb dist2fin ## 3.627804 3.727184 3.768047 ## ndvi_17_perc_sum soil_prod_state LZ_CentralIndusServ ## 4.298087 4.874471 4.950485 ## chirps_ev_med soil_deg_SDG_index settlements ## 5.255571 6.850837 6.951521 ## lights_sd ndvi_17_sum lights_med ## 7.340702 7.865851 12.760513 ## lc_crop dist2schools_pub lc_tree ## 14.256064 17.284021 21.274021 ## dist2allpubserv lc_urban sch_pub ## 21.442907 22.713274 35.288954 ## dist2schools dem sch ## 36.022745 36.222480 37.125670 ## temp_median health_esp_seg health_intermed ## 39.379451 48.457934 352.252893 ## health_basic health dist2water ## 390.098294 763.726685 21007.340493 ## dist2water_r ## 21014.000919 dim(covariates_std_df) ## [1] 1664 82 vif_res=car::vif(glm_0) length(which(c(vif_res)&gt;5)) ## [1] 22 Values below 3 means no multicollinearity issues. Values above 5 are indicative of multicollinearity issues. Out of the 82 candidate covariates, 22 have a VIF higher than 5. Some covariates preselection is hence justified. This is multicollinearity is might be caused by the fact that: many predictors are various expression of the same raw data (e.g. distance to schools and distance to public services) some predictors are covers similar physical features but comes from different data sources (e.g. distance to coastline and distance to water bodies) some predictors are closely associated with each other (e.g. lights at night and percentage of the area classified as urban) 3.2.2 Dimension reduction based on distance correlation We start by computing the distance correlation between all pairs of covariates. We will measure correlation with the Distance correlation. The advantage over the standard Pearson correlation is that the distance correlation allows to take better into account non-linearity in the relationship between two variables. This can be illustrated by computing the Pearson and Distance correlations in the case of a linear or quadratic relationship. x=seq(-10,10,length.out=1000) y=2*x+rnorm(mean=2,sd=10,1000) plotly::plot_ly(x=x, y=y, type=&quot;scatter&quot;, mode=&quot;markers&quot;)%&gt;% plotly::layout(title=paste0(&quot;Pearson corr:&quot;,format(round(cor(x,y),4),scientific=F), &quot;\\nDistance corr:&quot;,round(dcor(x,y),4))) x=seq(-10,10,length.out=1000) y=-x^2+rnorm(mean=2,sd=10,1000) plotly::plot_ly(x=x, y=y, type=&quot;scatter&quot;, mode=&quot;markers&quot;)%&gt;% plotly::layout(title=paste0(&quot;Pearson corr:&quot;,format(round(cor(x,y),4),scientific=F), &quot;\\nDistance corr:&quot;,round(dcor(x,y),4))) While the measure of association is similar for both correlation metrics when the dependence between the variables is linear as shown on Fig. ??, Distance correlation is much better at capturing the association between \\(X\\) and \\(Y\\) when their relation in non-linear as shown by the Distance correlation of 0.46 against -0.0128 for the Pearson correlation reported on Fig. ??. Distance correlation for all pairs of covariates is computed as follow. # 1) compute distance correlation between all pairs of covariates tic() distMatrix=dcor.matrix(covariates_std) toc() ## 2940.184 sec elapsed diag(distMatrix)=NA # replace diag with NA We then remove randomly one of the two most correlated covariates and we repeat the process until only 50 covariates are left, i.e. 5% of sample size of the training. The latter figure is generally accepted rule of thumb to decide on an appropriate number of covariates to start the backward covariate selection process as it limits the risk of chance correlation, i.e finding a significant correlation by chance. # 2) remove randomly one of the two most correlated covariates trainig_set_n=dim(covariates_std)[1]*0.6 # size of the traing set=60%, validation set = 20%, test set=20% target_n_cov=round(0.045*trainig_set_n) # target number of cov: 5% of training set= 50 n_cov=dim(covariates_std)[2] # starting number of covariate= 58 n_to_remove=n_cov-target_n_cov i=1 distMatrix_reduced=distMatrix for(i in 1:n_to_remove){ # 3) repeat step (2) until only number of covariates = 5% of training set index_max &lt;- arrayInd(which.max(distMatrix_reduced), dim(distMatrix_reduced)) # get column&#39;s and row&#39;s index of the max d-corr cell set.seed(i+2) # set seeds for replicability print(paste(colnames(distMatrix_reduced)[index_max])) print(distMatrix_reduced[index_max]) remove_cov_pair=round(runif(1)+1) # either the row (1) or column covariate (2) remove_cov=index_max[remove_cov_pair] # select index of row or column print(paste(&quot;cov removed&quot;,colnames(distMatrix_reduced)[remove_cov])) # print names of cov removed print(&quot;&quot;) distMatrix_reduced=distMatrix_reduced[-remove_cov,-remove_cov] # remove the corresponding cov from distMatrix } ## [1] &quot;dist2water_r&quot; &quot;dist2water&quot; ## [1] 0.9999477 ## [1] &quot;cov removed dist2water_r&quot; ## [1] &quot;&quot; ## [1] &quot;dist2allpubserv&quot; &quot;dist2schools&quot; ## [1] 0.9731764 ## [1] &quot;cov removed dist2schools&quot; ## [1] &quot;&quot; ## [1] &quot;temp_median&quot; &quot;dem&quot; ## [1] 0.9594395 ## [1] &quot;cov removed temp_median&quot; ## [1] &quot;&quot; ## [1] &quot;sch_pub&quot; &quot;sch&quot; ## [1] 0.9500018 ## [1] &quot;cov removed sch&quot; ## [1] &quot;&quot; ## [1] &quot;dist2allpubserv&quot; &quot;dist2schools_pub&quot; ## [1] 0.9212498 ## [1] &quot;cov removed dist2schools_pub&quot; ## [1] &quot;&quot; ## [1] &quot;lights_sd&quot; &quot;lights_med&quot; ## [1] 0.9194312 ## [1] &quot;cov removed lights_sd&quot; ## [1] &quot;&quot; ## [1] &quot;lc_urban&quot; &quot;settlements&quot; ## [1] 0.8422507 ## [1] &quot;cov removed lc_urban&quot; ## [1] &quot;&quot; ## [1] &quot;lights_med&quot; &quot;settlements&quot; ## [1] 0.8173944 ## [1] &quot;cov removed settlements&quot; ## [1] &quot;&quot; ## [1] &quot;soil_luc&quot; &quot;soil_carb&quot; ## [1] 0.8060875 ## [1] &quot;cov removed soil_luc&quot; ## [1] &quot;&quot; ## [1] &quot;lights_med&quot; &quot;ndvi_17_sum&quot; ## [1] 0.7990358 ## [1] &quot;cov removed lights_med&quot; ## [1] &quot;&quot; ## [1] &quot;soil_deg_SDG_index&quot; &quot;soil_prod_state&quot; ## [1] 0.7969524 ## [1] &quot;cov removed soil_prod_state&quot; ## [1] &quot;&quot; ## [1] &quot;dist2allpubserv&quot; &quot;Shape_Area&quot; ## [1] 0.7797326 ## [1] &quot;cov removed dist2allpubserv&quot; ## [1] &quot;&quot; ## [1] &quot;lc_tree&quot; &quot;dist2tree_r&quot; ## [1] 0.7721969 ## [1] &quot;cov removed dist2tree_r&quot; ## [1] &quot;&quot; ## [1] &quot;dens_residential&quot; &quot;dens_all_roads&quot; ## [1] 0.7399636 ## [1] &quot;cov removed dens_all_roads&quot; ## [1] &quot;&quot; ## [1] &quot;Shape_Area&quot; &quot;AREA_ID&quot; ## [1] 0.7322786 ## [1] &quot;cov removed Shape_Area&quot; ## [1] &quot;&quot; ## [1] &quot;pop_dens&quot; &quot;gpw_es_TOT&quot; ## [1] 0.7249122 ## [1] &quot;cov removed gpw_es_TOT&quot; ## [1] &quot;&quot; ## [1] &quot;dens_urban_no_motor&quot; &quot;urban_no_motor&quot; ## [1] 0.7237921 ## [1] &quot;cov removed dens_urban_no_motor&quot; ## [1] &quot;&quot; ## [1] &quot;dist2urban_r&quot; &quot;dist2schools_priv&quot; ## [1] 0.7161026 ## [1] &quot;cov removed dist2schools_priv&quot; ## [1] &quot;&quot; ## [1] &quot;dist2urban_r&quot; &quot;dist2roadInter&quot; ## [1] 0.7024571 ## [1] &quot;cov removed dist2roadInter&quot; ## [1] &quot;&quot; ## [1] &quot;dist2fin&quot; &quot;dist2biz&quot; ## [1] 0.6998672 ## [1] &quot;cov removed dist2fin&quot; ## [1] &quot;&quot; ## [1] &quot;health_basic&quot; &quot;health&quot; ## [1] 0.6977073 ## [1] &quot;cov removed health&quot; ## [1] &quot;&quot; ## [1] &quot;dist2urban_r&quot; &quot;AREA_ID&quot; ## [1] 0.6967835 ## [1] &quot;cov removed dist2urban_r&quot; ## [1] &quot;&quot; ## [1] &quot;soil_deg_SDG_index&quot; &quot;soil_prod_trj&quot; ## [1] 0.6849447 ## [1] &quot;cov removed soil_deg_SDG_index&quot; ## [1] &quot;&quot; ## [1] &quot;LZ_CentralIndusServ&quot; &quot;pop_dens&quot; ## [1] 0.6684595 ## [1] &quot;cov removed LZ_CentralIndusServ&quot; ## [1] &quot;&quot; ## [1] &quot;dens_secondary&quot; &quot;secondary&quot; ## [1] 0.6614154 ## [1] &quot;cov removed secondary&quot; ## [1] &quot;&quot; ## [1] &quot;lc_tree&quot; &quot;slope&quot; ## [1] 0.6532028 ## [1] &quot;cov removed lc_tree&quot; ## [1] &quot;&quot; ## [1] &quot;dist2biz&quot; &quot;dist2pubamen&quot; ## [1] 0.6480211 ## [1] &quot;cov removed dist2biz&quot; ## [1] &quot;&quot; ## [1] &quot;dens_rural_track&quot; &quot;rural_track&quot; ## [1] 0.6472546 ## [1] &quot;cov removed dens_rural_track&quot; ## [1] &quot;&quot; ## [1] &quot;dens_fast_track&quot; &quot;fast_track&quot; ## [1] 0.5912912 ## [1] &quot;cov removed fast_track&quot; ## [1] &quot;&quot; ## [1] &quot;ndvi_17_sum&quot; &quot;AREA_ID&quot; ## [1] 0.5816056 ## [1] &quot;cov removed AREA_ID&quot; ## [1] &quot;&quot; ## [1] &quot;chirps_ev_med&quot; &quot;chirps_ev_2017&quot; ## [1] 0.5710039 ## [1] &quot;cov removed chirps_ev_med&quot; ## [1] &quot;&quot; ## [1] &quot;lc_crop&quot; &quot;dist2crop_r&quot; ## [1] 0.5641672 ## [1] &quot;cov removed lc_crop&quot; ## [1] &quot;&quot; ## [1] &quot;dens_tertiary&quot; &quot;tertiary&quot; ## [1] 0.5551673 ## [1] &quot;cov removed tertiary&quot; ## [1] &quot;&quot; ## [1] &quot;dens_unclassified&quot; &quot;unclassified&quot; ## [1] 0.5353308 ## [1] &quot;cov removed unclassified&quot; ## [1] &quot;&quot; ## [1] &quot;dist2crop_r&quot; &quot;slope&quot; ## [1] 0.522504 ## [1] &quot;cov removed slope&quot; ## [1] &quot;&quot; ## [1] &quot;pop_dens&quot; &quot;ndvi_17_sum&quot; ## [1] 0.4994304 ## [1] &quot;cov removed pop_dens&quot; ## [1] &quot;&quot; ## [1] &quot;dist2coast_r&quot; &quot;chirps_ev_2017&quot; ## [1] 0.488211 ## [1] &quot;cov removed dist2coast_r&quot; ## [1] &quot;&quot; Let us recompute the VIF with the remaining 50 covariates. Median lights at night (named lights_med in the dataset), the rural-urban DYGESTIC binary segmentos classification (named AREA_ID in the dataset), population density (pop_dens) and slope (slope) were removed in the process. We re-introduce them as we expect them to be important predictors of the development outcomes. # RE calculate vif #### sort(colnames(distMatrix_reduced)) ## [1] &quot;all_roads&quot; &quot;building&quot; ## [3] &quot;bus&quot; &quot;chirps_ev_2017&quot; ## [5] &quot;cult_educ&quot; &quot;dem&quot; ## [7] &quot;dens_fast_track&quot; &quot;dens_residential&quot; ## [9] &quot;dens_secondary&quot; &quot;dens_tertiary&quot; ## [11] &quot;dens_unclassified&quot; &quot;dist2crop_r&quot; ## [13] &quot;dist2health&quot; &quot;dist2hosp&quot; ## [15] &quot;dist2pubamen&quot; &quot;dist2road&quot; ## [17] &quot;dist2water&quot; &quot;fin&quot; ## [19] &quot;food&quot; &quot;fuel&quot; ## [21] &quot;health_basic&quot; &quot;health_esp_seg&quot; ## [23] &quot;health_intermed&quot; &quot;health_srv&quot; ## [25] &quot;hospital&quot; &quot;luxury&quot; ## [27] &quot;LZ_BasicGrainLaborZone&quot; &quot;LZ_CoffeeAgroIndus&quot; ## [29] &quot;LZ_FishAgriTourism&quot; &quot;LZ_LvstckGrainRem&quot; ## [31] &quot;mkt&quot; &quot;ndvi_17_perc_sum&quot; ## [33] &quot;ndvi_17_sum&quot; &quot;official&quot; ## [35] &quot;parking&quot; &quot;residential&quot; ## [37] &quot;rural_track&quot; &quot;sch_pri&quot; ## [39] &quot;sch_pub&quot; &quot;shop&quot; ## [41] &quot;soil_carb&quot; &quot;soil_prod_prf&quot; ## [43] &quot;soil_prod_trj&quot; &quot;urban_no_motor&quot; ## [45] &quot;worship&quot; covariates_std_reduced=covariates_std[,c(colnames(distMatrix_reduced),&quot;lights_med&quot;,&quot;AREA_ID&quot;,&quot;pop_dens&quot;,&quot;slope&quot;)] covariates_std_reduced_df=as.data.frame(covariates_std_reduced) data_model=covariates_std_reduced_df%&gt;% bind_cols(ehpm17_predictors_survey%&gt;% select(ingpe)) formula_glm_1= reformulate(paste(names(covariates_std_reduced_df),collapse=&quot;+&quot;), &quot;ingpe&quot;) glm_1=glm(formula_glm_1, data=data_model) sort(car::vif(glm_1)) ## luxury bus dens_secondary ## 1.017735 1.045257 1.069115 ## health_esp_seg hospital health_basic ## 1.077276 1.087613 1.089489 ## health_intermed fuel soil_prod_prf ## 1.093697 1.108080 1.112309 ## dens_fast_track dens_unclassified sch_pri ## 1.119581 1.131510 1.138939 ## dens_tertiary mkt rural_track ## 1.139313 1.147971 1.164022 ## parking cult_educ urban_no_motor ## 1.193745 1.203896 1.215575 ## soil_carb building soil_prod_trj ## 1.241506 1.250470 1.283704 ## official food fin ## 1.313564 1.330757 1.335872 ## worship sch_pub shop ## 1.374467 1.407148 1.419167 ## LZ_FishAgriTourism residential dens_residential ## 1.426517 1.427602 1.448909 ## all_roads health_srv chirps_ev_2017 ## 1.450050 1.483279 1.494114 ## dist2road dist2health dist2hosp ## 1.521071 1.613757 1.659747 ## dist2pubamen pop_dens dist2crop_r ## 1.746986 1.796098 1.805095 ## dist2water LZ_CoffeeAgroIndus LZ_BasicGrainLaborZone ## 1.809049 1.830862 1.990535 ## slope LZ_LvstckGrainRem dem ## 2.057306 2.189399 2.276269 ## AREA_ID ndvi_17_perc_sum lights_med ## 2.364066 3.281148 4.858914 ## ndvi_17_sum ## 5.740155 sort(names(covariates_std_reduced_df)) ## [1] &quot;all_roads&quot; &quot;AREA_ID&quot; ## [3] &quot;building&quot; &quot;bus&quot; ## [5] &quot;chirps_ev_2017&quot; &quot;cult_educ&quot; ## [7] &quot;dem&quot; &quot;dens_fast_track&quot; ## [9] &quot;dens_residential&quot; &quot;dens_secondary&quot; ## [11] &quot;dens_tertiary&quot; &quot;dens_unclassified&quot; ## [13] &quot;dist2crop_r&quot; &quot;dist2health&quot; ## [15] &quot;dist2hosp&quot; &quot;dist2pubamen&quot; ## [17] &quot;dist2road&quot; &quot;dist2water&quot; ## [19] &quot;fin&quot; &quot;food&quot; ## [21] &quot;fuel&quot; &quot;health_basic&quot; ## [23] &quot;health_esp_seg&quot; &quot;health_intermed&quot; ## [25] &quot;health_srv&quot; &quot;hospital&quot; ## [27] &quot;lights_med&quot; &quot;luxury&quot; ## [29] &quot;LZ_BasicGrainLaborZone&quot; &quot;LZ_CoffeeAgroIndus&quot; ## [31] &quot;LZ_FishAgriTourism&quot; &quot;LZ_LvstckGrainRem&quot; ## [33] &quot;mkt&quot; &quot;ndvi_17_perc_sum&quot; ## [35] &quot;ndvi_17_sum&quot; &quot;official&quot; ## [37] &quot;parking&quot; &quot;pop_dens&quot; ## [39] &quot;residential&quot; &quot;rural_track&quot; ## [41] &quot;sch_pri&quot; &quot;sch_pub&quot; ## [43] &quot;shop&quot; &quot;slope&quot; ## [45] &quot;soil_carb&quot; &quot;soil_prod_prf&quot; ## [47] &quot;soil_prod_trj&quot; &quot;urban_no_motor&quot; ## [49] &quot;worship&quot; The VIF figures are much more acceptable now. The reduced number of covariates limit the risk of chance correlation. We write the covariates and their names in .csv files for later use. # write the names of selected features into csv #### write.table(names(covariates_std_reduced_df), paste0(dir_data, &quot;out/distance_corr_var/selected_all.txt&quot;), row.names = F) # write the selected features and outcome into csv #### covariates_std_reduced_df$SEG_ID=ehpm17_predictors_survey$SEG_ID covariates_std_reduced_df$ingpe=ehpm17_predictors_survey$ingpe covariates_std_reduced_df$literacy_rate=ehpm17_predictors_survey$literacy_rate covariates_std_reduced_df$pobreza_extrema=ehpm17_predictors_survey$pobreza_extrema covariates_std_reduced_df$pobreza_mod=ehpm17_predictors_survey$pobreza_mod covariates_std_reduced_df$n_obs=ehpm17_predictors_survey$n_obs write.csv(covariates_std_reduced_df, paste0(dir_data, &quot;out/all_covariates_and_outcomes.csv&quot;), row.names = F) 3.3 Correlations between predictors and the outcome variables We now explore the relationship between predictors and the outcome variables. This will give us a feel of the important covariate in the model. 3.3.1 Correlation with income We compute the distance correlation for all covariates with a loop and store these correlations in a dataframe. covariates_std_reduced_df$SEG_ID=ehpm17_predictors_survey$SEG_ID covariates_std_reduced_df$ingpe=ehpm17_predictors_survey$ingpe covariates_std_reduced_df$literacy_rate=ehpm17_predictors_survey$literacy_rate covariates_std_reduced_df$pobreza_extrema=ehpm17_predictors_survey$pobreza_extrema covariates_std_reduced_df$pobreza_mod=ehpm17_predictors_survey$pobreza_mod covariates_std_reduced_df$n_obs=ehpm17_predictors_survey$n_obs corr_re=list() names_re=list() for(i in 1:50){ # where 50 are the number of cor_test=energy::dcor(covariates_std_reduced_df$ingpe,covariates_std_reduced_df[,i]) corr_re[[i]]=c(cor_test) names_re[[i]]=names(covariates_std_reduced_df)[i] } corr_df=do.call(rbind,corr_re) names_df=do.call(rbind,names_re) corr_df=as_tibble(corr_df) corr_df$covariate = names_df names(corr_df)[1]=&quot;cor&quot; corr_df=corr_df %&gt;% arrange(desc(abs(cor))) Lastly, we select only the variables with an absolute distance correlation with income higher than 20% and we plot the correlation with income. The latter selection is only for illustration purposes:we do not want to clutter the graph. corr_df_top=corr_df%&gt;% filter(!(covariate%in%c(&quot;pobreza_mod&quot;,&quot;literacy_rate&quot;,&quot;ingpe&quot;)))%&gt;% filter(abs(cor)&gt;=.2) corr_df_top$covariate_f=factor(corr_df_top$covariate, levels = corr_df_top$covariate) plot_ly(x=corr_df_top$covariate_f, y=corr_df_top$cor, type=&quot;bar&quot;, name = corr_df_top$covariate)%&gt;% layout(title=&quot;Covariates correlations with Income&quot;, yaxis=list(tickformat = &quot;%&quot;)) knitr::include_graphics(&quot;img/corr_ingpe.PNG&quot;) As expected, among 3 most important covariates are those indicative urban centers: light at night intensity, rural/urban classification the segmento and distance to public amenities. In order to get a sense of the direction of the correlation and the associations between the variables, we plot a correlogram. covariates_std_reduced_df_selected=covariates_std_reduced_df%&gt;% select(c(&quot;ingpe&quot;,corr_df_top$covariate)) qgraph::qgraph(cor(covariates_std_reduced_df_selected), minimum=0.20, layout=&quot;spring&quot;, # groups=gr, labels=names(covariates_std_reduced_df_selected), label.scale=F, title=&quot;Correlations: Income and predictors&quot;) The plot on Fig.?? corrgram-ingpeshows clearly that the top predictors are highly correlated. 3.3.2 Correlation with literacy The process described above is repeated for literacy. Light at night comes again as the best predictor of literacy, by distance the rural-urban segmento identifier and NDVI. 3.3.3 Correlation with poverty NDVI and distance to puiblic amenities and hospitals in particular come first. We see that the level of correlation is lower than in previous plot, suggesting that it might be harder to find good model fit for moderate poverty than for the other indicators. A Gaussian distribution is a Normal distribution. We use the the former terminology as it corresponds to the INLA one↩ "],
["simple-model.html", "Section 4 Fitting a simple model 4.1 Loading the data 4.2 Area data 4.3 Point-referenced data 4.4 K-fold cross-validation", " Section 4 Fitting a simple model The goal of this section is to fit a simple model for income in order to introduce the modified Besag-York-Mollie model (BYM 2) and the SPDE model. In both case, the general process is the same: Define the spatial dependency Split the model into a training and validation set Specify the formula expressing the outcome variables in terms of fixed and random spatial effects Fit the model on the training set Validate the model on the validation set. The fixed effects are the covariates (e.g. population density or light at night). The spatial random effects are spatially correlated random effect used to model the spatial dependency. Note that it is assumed here that the list of fixed effects is known. Next section will present a backward stepwise covariate selection process to select the list of covariates. 4.1 Loading the data We start by loading the covariates data and matching them with the segmento spatial polygon data frame. rm(list=ls()) library(parallel) library(INLA) library(dplyr) #INLA:::inla.dynload.workaround() # modify dir_data to where you stored the data root_dir=&quot;~/&quot; project_dir=&quot;data/&quot; dir_data=paste0(root_dir,project_dir) # load the data #### ehpm17_predictors=read.csv(paste0(dir_data, &quot;out/all_covariates_and_outcomes.csv&quot;)) # correct for xls missbehaviour: the SEG_ID with a leading 0 were shorten ehpm17_predictors=ehpm17_predictors%&gt;% mutate(SEG_ID=as.character(SEG_ID), SEG_ID=ifelse(nchar(SEG_ID)==7, paste0(0,SEG_ID), SEG_ID)) # shape segmento_sh=rgdal::readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Segmentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields # add the survey data to shapefile segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% # dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) 4.2 Area data 4.2.1 Spatial dependency with area data data In order to model the spatial dependency with area data data, we need to inform the model about which segmentos are neighboring which ones. The function spdep::poly2nb creates a neighbours list from the segmento polygon. The list is created in a spatial format suitable for INLA with the function spdep::nb2INLA. It is then read as a graph-object for INLA consumption. # define the neighbouring structure #### segmento_sh_data@data$ID=1:length(segmento_sh_data@data$n_obs) segmento.nb=spdep::poly2nb(segmento_sh_data) spdep::nb2INLA(paste0(dir_data, &quot;out/SEG.graph&quot;), segmento.nb) SEG.adj=paste0(paste0(dir_data, &quot;out/SEG.graph&quot;), sep=&quot;&quot;) Segmento.Inla.nb &lt;- INLA::inla.read.graph(paste0(dir_data, &quot;out/SEG.graph&quot;)) Fig. 4.1 illustrates the results with Departmental level data. ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Departamentos.shp&quot;, layer: &quot;STPLAN_Departamentos&quot; ## with 14 features ## It has 6 fields Figure 4.1: Neigbouring structure at the Departmental level 4.2.2 Store the data for modelling The data are stored in a dataframe data_model. # identify ehpm17 segmentos to fit the model #### segmento_sh_data_model=segmento_sh_data non_na_index=which(is.na(segmento_sh_data_model$n_obs)==F) ehpm17_predictors_nona=segmento_sh_data_model@data[non_na_index,] # covariates ##### cov_candidates_selected_df=read.table(paste0(dir_data, &quot;out/distance_corr_var/selected_all.txt&quot;), header =T) cov_candidates_selected_table=cov_candidates_selected_df names(cov_candidates_selected_table)=&quot;Candidates&quot; # knitr::kable( # cov_candidates_selected_table, # caption = &#39;Candidate covariates for income&#39;, # booktabs = TRUE # ) covariates=ehpm17_predictors_nona[,as.character(cov_candidates_selected_table$Candidates)] # store data for the model #### data_model=data.frame(ingpe=ehpm17_predictors_nona$ingpe, intercept=array(1,dim(covariates)[1]), # intercept for INLA ID=ehpm17_predictors_nona$ID, # ID to link data to the neibhouring structure covariates) 4.2.3 Split the sample into a 80/20 training and validation sets The observations in the dataset are split into a 80/20 training and validation sets. The model will be fitted on the training set. Income will be predicted on the validation sets. Predicted values will be compared with the observed values of the validation set in order to assess the goodness of it of the model. # sample segmentos for training, validation and test #### set.seed(1234) spec = c(train = .8, validate = .2) g = sample(cut( seq(nrow(data_model)), nrow(data_model)*cumsum(c(0,spec)), labels = names(spec) )) index_val=which(g==&quot;validate&quot;) index_train=which(g==&quot;train&quot;) data_model$pred=data_model$ingpe data_model$pred[c(index_val)] &lt;- NA # set the validation pred to NA: they will be predicted by the model and compared with observed values 4.2.4 Formula specifying the relationship between the outcome variable and the fixed and spatial random effects We write below a formula to predict income with a subset of the covariates. We chose to use the following set of covariates: Binary indicator for rural and urban segmentos Median lights at night Population density Average slope in the segmentos Average precipitation Distance to public services Distance to businesses Distance to roads # write the test formula #### formula_test =pred~ AREA_ID+lights_med+pop_dens+slope+chirps_ev_2017+dist2pubamen+dist2road+ f(ID, model = &quot;bym2&quot;, graph = Segmento.Inla.nb, hyper=list( prec = list(prior = &quot;pc.prec&quot;, param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, 0.5))), scale.model = TRUE, constr = T, adjust.for.con.comp = T) The variable pred is the output variable we aim to model, i.e. income. The selected covariates are linearly added. Remember that they have been standardized in the previous section in order to avoid any issue linked to scale. These covariates are called fixed effects. The most complex bit of the formula is the spatial dependence structure specified in f(). The ID is the segmento identifiers linking the graph object Segmento.Inla.nb, where the neigbouring structure is defined, with the dataframe where the covariates and output data are stored. The model parameter for the specification of the spatial correlation model that should be used. The bym2 is used as it flexible and allows for a relatively intuitive specification of spatial random effects. The flexibility comes from the fact that bym2 allows for the error term to be composed of two elements: (1) a pure random noise, (2) a spatially correlated noise. The penalized complexity (PC) priors phi allows to influence the importance of each effect (if phi=0, then we have pure noise, if phi=1, we only have spatially correlated random effect). The prec parameter is the precision parameters, i.e. how much the spatially correlated noise is allowed to vary spatially. If the value is high, then the spatially correlated noise is allowed to vary a lot across space and vice versa. By decreasing the PC prior on the precision parameter, one decreases the risk of over-fitting: less of the spatial variation left unexplained by the covariates is attributed to the spatial random effects and, as results, the model is better able to generalise to areas where it was not trained. Here, we have chosen to set the PC prior on precision as \\(Pr(\\sigma&gt;1)=0.0001\\) instead of the default \\(Pr(\\sigma&gt;1)=0.01\\). Indeed, we were facing over-fitting issues with the default priors: the difference between the goodness of fit in the training and validation set was very large (circa 20% in the \\(R^{2}\\)). 4.2.5 Fit the model on the training set The model is fitted with the inla command. We have to specify the likelihood distribution of the response variable. The family argument defines the type of likelihood function for the response variable. As the distribution of income is strictly positive and right skewed, a Gamma distribution is a good option option. The data argument specifies the data on which the model is fitted, in our case, it is the data_model dataframe. As we use a Gamma likelihood function, the predicted values needs to be back transformed to the original scale by using the exponential function. The control.predictor allows automatise the process once the option list(link=1,compute=T) is specified. We use a simple integration strategy to compute the posterior marginals of the model parameters by specifying control.inla =list(int.strategy = &quot;eb&quot;), where eb stands for “empirical Bayes”. In the empirical Bayes approach, one use only one “integration point equal to the posterior mode of the hyperparameters” (Moraga 2019). This speeds up the estimation of the model. # fit the model on the training set #### start_time=Sys.time() # time the start of the estimation bym.res=inla(formula=formula_test, family = &quot;gamma&quot;, data = data_model, control.predictor=list(link=1,compute=T), control.inla =list(int.strategy = &quot;eb&quot;)) end_time=Sys.time() # time at the end of the estimation duration=end_time-start_time print(duration) ## Time difference of 1.32105 mins The model took 3.7 minutes to be estimated. With the default integration strategy, the model takes 13.5 minutes to be estimated. No significant difference were found between both integration strategies. 4.2.6 Validate the model on the validation set The goodness of fit of the model can now be investigated on the validation set. We start by extracting the fitted values. The fitted values are stored in the bym.res object under summary.fitted.values. The inla model yield a distribution of predicted income for each segmento. Various statistics summarising this prediction are available for each segmento: The mean fitted value The median fitted value The 2.5 and 97.5 percentile of the fitted values The standard deviation of the fitted value distribution We select below the mean fitted value, i.e. the mean prediction value for each segmento. # Extract the fitted values #### M_fit=bym.res$summary.fitted.values[,&quot;mean&quot;] We can now compute the Root Mean Square Error (RMSE) and the pseudo \\(R^{2}\\) as shown below.2 RMSE=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RMSE_val &lt;- sqrt(mean(res^2,na.rm=T)) return(RMSE_val) } pseudo_r2=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } RMSE_val=RMSE(index_val,&quot;ingpe&quot;,data_model,M_fit) RMSE_train=RMSE(index_train,&quot;ingpe&quot;,data_model,M_fit) r2_val=pseudo_r2(index_val,&quot;ingpe&quot;,data_model,M_fit) r2_train=pseudo_r2(index_train,&quot;ingpe&quot;,data_model,M_fit) Figure 4.2: Observed vs predicted income values, BYM 2 model Figure 4.3: Observed vs predicted income values, BYM 2 model Fig. 4.3 shows the predicted against the observed income value for the training and validation set. The model appears to do a relatively good job, although it has difficulty to capture the segmentos with an income above 400. A \\(R^{2}\\) of 44% is relatively satisfactory. It means that 44% of the income variation in the validation set is explained by the model fitted on the training set. However, the fact that the \\(R^{2}\\) is at 69%, close to twice as big as the \\(R^{2}\\) in the validation set, indicates over-fitting. The RMSE is 66 USD in the validation set and 40 usd in the training set. The RMSE is the standard deviation of the residuals, the difference between the fitted and observed values. The results are stored in a list and saved in a .RData file for later consumption. income_bym2_naive=list(&quot;outcome&quot;=&quot;ingpe&quot;, &quot;spat_dep&quot;=&quot;bym2&quot;, &quot;cov_select&quot;=&quot;naive&quot;, &quot;formula&quot;=formula_test, &quot;family&quot;=&quot;gamma&quot;, &quot;data&quot;=data_model, &quot;fit&quot;=M_fit, &quot;index_val&quot;=index_val, &quot;index_train&quot;=index_train, &quot;r2_val&quot;=r2_val, &quot;r2_train&quot;=r2_train, &quot;RMSE_val&quot;=RMSE_val, &quot;RMSE_train&quot;=RMSE_train) save(income_bym2_naive, file=paste0(dir_data, &quot;out/results/income_bym2_naive.RData&quot;)) 4.3 Point-referenced data 4.3.1 Spatial dependency with point-referenced data data The spatial dependence is here model with the SPDE approach. The following steps are required: Create the mesh Create the SPDE Create the matrix of weights 4.3.1.1 Create the mesh In order to create the mesh, we start by identifying the segmentos for which there are EHPM data. Second, we create a spatial polygon of the boundaries of the country by dissolving the shapefile of the departentos with the function unionSpatialPolygons. Third, we transform this spatial polygon into a list that the INLA library can process with the function inla.sp2segment. Lastly, we use the helper function inla.mesh.create.helper to create the mesh, using the boundary and coordinates of the centroids of the segmentos as inputs. # create the mesh #### segmento_sh_subset=subset(segmento_sh_data_model, is.na(segmento_sh_data_model@data$ingpe)==F) # identify where there is EHPM data coords=coordinates(segmento_sh_subset) # collect the coordinates of the segmentos with EHPM datr SVborder &lt;- maptools::unionSpatialPolygons(departamentos_sh, rep(1, nrow(departamentos_sh))) # create one polygone with boundary of the countries SV.bdry &lt;- inla.sp2segment(SVborder) # create an inla boundary object SV.mesh &lt;- inla.mesh.create.helper( boundary=SV.bdry, points=coords, offset=c(2500, 25000), max.edge=c(20000, 50000), min.angle=c(25, 25), cutoff=8000, plot.delay=NULL) plot(SV.mesh,main=&quot;&quot;,asp=1) Figure 4.4: Mesh The offset parameters of inla.mesh.create.helper define the inner and outer distance. For instance, if we change the outer offset parameter to 250 km instead of the original 25k, the result is plotted on Fig 4.5. SV.meshL &lt;- inla.mesh.create.helper( boundary=SV.bdry, points=coords, offset=c(2500, 250000), # we increased from 25000 to 250000 max.edge=c(20000, 50000), min.angle=c(25, 25), cutoff=8000, plot.delay=NULL) plot(SV.meshL,main=&quot;&quot;,asp=1) Figure 4.5: Mesh When choosing the outer offset parameter, the aim is to avoid the presence of boundary effects in the subsequent estimation of the SPDE. It hence best to allow for a sufficient distance as shown on Fig. 4.4. However, Fig. 4.5 is clearly too large and would slow down the estimation process. The inner offset parameter has no effect here as it is superseded by the max.edge, min.angle and cutoff parameters.The max.edge parameters defines the largest allowed triangle edge length. The min.angle parameter defines the smallest allowed triangle angle while the parameter cutoff defines the minimum allowed distance between points. Let us illustrate the impact of modifying the max.edge from 20km to 5km. SV.meshL &lt;- inla.mesh.create.helper( boundary=SV.bdry, points=coords, offset=c(2500, 25000), max.edge=c(20000, 5000), # we decreased from 50000 to 5000 min.angle=c(25, 25), cutoff=8000, plot.delay=NULL) plot(SV.meshL,main=&quot;&quot;,asp=1) Figure 4.6: Mesh Fig 4.6 shows the results. The risk of such a tight mesh is to lead to over-fitting. Furthermore, it is computationally more expensive. 4.3.1.2 Derive the SPDE Next, the SPDE is derived from the mesh thanks to the inla.spde2.matern function and the spatial index with the function inla.spde.make.index. # create the SPDE #### SV.spde &lt;- inla.spde2.matern(mesh=SV.mesh,alpha=2) s.index &lt;- inla.spde.make.index(name=&quot;spatial.field&quot;, n.spde=SV.spde$n.spde) The s.index allows to link the matrix of spatial weights that will be defined below with the mesh #### Create the matrix of weights The matrix of weights is created with the INLA function inla.spde.make.A A.train &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_train,]) A.val &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_val,]) 4.3.2 Store the data for modelling in a stack Here is we use the inla.stack function to store the data conveniently in one object. # Stack #### covariates_list=c(&quot;AREA_ID&quot;,&quot;lights_med&quot;,&quot;pop_dens&quot;,&quot;slope&quot;,&quot;chirps_ev_2017&quot;,&quot;dist2pubamen&quot;,&quot;dist2road&quot;) stack.train &lt;- inla.stack(data = list(pred=segmento_sh_subset@data$ingpe[index_train]), # this output variable (income) A = list(A.train, 1,1), effects = list(s.index, Intercept=1:length(index_train), segmento_sh_subset@data[index_train,c(covariates_list)]), tag=&quot;train&quot;) # the tag allows to extract later selected statistics for desired sample stack.val &lt;- inla.stack(data = list(pred=NA), # we set it to NA A = list(A.val, 1,1), effects = list(s.index, Intercept=1:length(index_val), segmento_sh_subset@data[index_val,c(covariates_list)]), tag=&quot;val&quot;) # combine both stack join.stack &lt;- inla.stack(stack.train, stack.val) 4.3.3 Formula specifying the relationship between the outcome variable and the fixed and spatial random effects We use the same set of fixed effects. # write the test formula #### formula_test =pred~ -1+Intercept+ AREA_ID+lights_med+pop_dens+slope+chirps_ev_2017+dist2pubamen+dist2road+ f(spatial.field, model=SV.spde) The main difference with the BYM 2 specification is the way the spatial random effects are specified by the f() function. It takes here two arguments: spatial.field is the name of the spatial index s.index, themodel is the SPDE defined in SV.spde. 4.3.4 Fit the model on the training set The same inla command is used to fit the SPDE model than when fitting the areal model. The only difference is that the data argument takes the stack data defined above as an input. # fit #### start_time=Sys.time() spde_res=inla(formula_test, # formula data=inla.stack.data(join.stack), family=&quot;gamma&quot;, # likelihood of the data control.predictor=list(A=inla.stack.A(join.stack), link=1,compute=T)) end_time=Sys.time() print(end_time-start_time) ## Time difference of 10.87331 secs The model is much faster to fit: it took only 19 seconds (against 3.5 minutes for the BMY2 model). 4.3.5 Validate the model on the validation set We can now investigate the goodness of fit of the model. In order to extract from the income_spde_test object the fitted values for the training and the validation set, we use the inla.stack.index function. Again,we extracted only the mean predicted value for each segmento. # Extract fitted values index_inla_train = inla.stack.index(join.stack,&quot;train&quot;)$data index_inla_val = inla.stack.index(join.stack,&quot;val&quot;)$data results.train=spde_res$summary.fitted$mean[index_inla_train] results.val=spde_res$summary.fitted$mean[index_inla_val] M_fit_spde=array(NA,length(M_fit)) M_fit_spde[index_train]=results.train M_fit_spde[index_val]=results.val r2_train_spde=pseudo_r2(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) r2_val_spde=pseudo_r2(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_train_spde=RMSE(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_val_spde=RMSE(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) Figure 4.7: Observed vs predicted income values, SPDE model Fig. ?? shows the observed values against the predicted values for the training and validation sets. There are no major difference with the results obtained with the modified BYM model shown on Fig.4.3, except that here there does not appear to be over-fitting as the \\(R^{2}\\) are similar in the training and validation set. Let us save the results for later consumption. income_spde_naive=list(&quot;outcome&quot;=&quot;ingpe&quot;, &quot;spat_dep&quot;=&quot;spde&quot;, &quot;cov_select&quot;=&quot;naive&quot;, &quot;formula&quot;=formula_test, &quot;family&quot;=&quot;gamma&quot;, &quot;data&quot;=join.stack, &quot;fit&quot;=M_fit_spde, &quot;index_val&quot;=index_val, &quot;index_train&quot;=index_train, &quot;r2_val&quot;=r2_val_spde, &quot;r2_train&quot;=r2_train_spde, &quot;RMSE_val&quot;=rmse_val_spde, &quot;RMSE_train&quot;=rmse_train_spde) save(income_spde_naive, file=paste0(dir_data, &quot;out/results/income_spde_naive.RData&quot;)) Lastly, Fig. ?? shows the results when using a Gaussian likelihood and log transforming the income data. There is a minor improvement to the goodness of fit compared to the Gamma model. Figure 4.8: Observed vs predicted income values, SPDE model with Gaussian likelihood and Log-transformed income values 4.4 K-fold cross-validation The split of the observations between the training and validation sets has important impact on the goodness of fit of the model. Indeed, the model could fit very well the observations in the training set but fit poorly the observations in the validation set purely by chance. K-fold crossvalidation is a way to assess the robustness of the results to various splits of the data. In K-fold crossvalidation, the data a split in K folds and each fold is used only once for validation purposes and \\(K-1\\) times for training purpose. At the end of the process, one can assess how the goodness of statistics varies and what is its average value. A typical value for K is 10. The 10 folds are created as follow: set.seed(1) spec = c(val1 = .1, val2 = .1, val3 = .1, val4 = .1, val5 = .1, val6 = .1, val7 = .1, val8 = .1, val9 = .1, val10 = .1) g = sample(cut( seq(nrow(data_model)), nrow(data_model)*cumsum(c(0,spec)), labels = names(spec) )) We then fit the model 10 times, using each fold as a validation set once. r2_train_spde_a=r2_val_spde_a=rmse_train_spde_a=rmse_val_spde_a=c() # prepare empty array to store goodness of fit stats for(k in 1:10){ # define the index index_val=which(g==paste0(&quot;val&quot;,k)) index_train=which(g!=paste0(&quot;val&quot;,k)) # define the weights A.train &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_train,]) A.val &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords[index_val,]) # build the stack covariates_list=c(&quot;AREA_ID&quot;,&quot;lights_med&quot;,&quot;pop_dens&quot;,&quot;slope&quot;,&quot;chirps_ev_2017&quot;,&quot;dist2pubamen&quot;,&quot;dist2road&quot;) stack.train &lt;- inla.stack(data = list(pred=segmento_sh_subset@data$ingpe[index_train]), # this output variable (income) A = list(A.train, 1,1), effects = list(s.index, Intercept=1:length(index_train), segmento_sh_subset@data[index_train,c(covariates_list)]), tag=&quot;train&quot;) # the tag allows to extract later selected statistics for desired sample stack.val &lt;- inla.stack(data = list(pred=NA), # we set it to NA A = list(A.val, 1,1), effects = list(s.index, Intercept=1:length(index_val), segmento_sh_subset@data[index_val,c(covariates_list)]), tag=&quot;val&quot;) join.stack &lt;- inla.stack(stack.train, stack.val) # write the test formula #### formula_test =pred~ -1+Intercept+ AREA_ID+lights_med+pop_dens+slope+chirps_ev_2017+dist2pubamen+dist2road+ f(spatial.field, model=SV.spde) # fit the model spde_res=inla(formula_test, # formula data=inla.stack.data(join.stack), family=&quot;gamma&quot;, # likelihood of the data control.predictor=list(A=inla.stack.A(join.stack), link=1,compute=T)) end_time=Sys.time() # extract the fitted values index_inla_train = inla.stack.index(join.stack,&quot;train&quot;)$data index_inla_val = inla.stack.index(join.stack,&quot;val&quot;)$data results.train=spde_res$summary.fitted$mean[index_inla_train] results.val=spde_res$summary.fitted$mean[index_inla_val] M_fit_spde=array(NA,length(M_fit)) M_fit_spde[index_train]=results.train M_fit_spde[index_val]=results.val # compute goodness of fit stats r2_train_spde=pseudo_r2(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) r2_val_spde=pseudo_r2(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_train_spde=RMSE(index_train,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) rmse_val_spde=RMSE(index_val,&quot;ingpe&quot;,segmento_sh_subset@data,M_fit_spde) # stores goodness of fit stats in an array r2_train_spde_a=c(r2_train_spde_a,r2_train_spde) r2_val_spde_a=c(r2_val_spde_a,r2_val_spde) rmse_train_spde_a=c(rmse_train_spde_a,rmse_train_spde) rmse_val_spde_a=c(rmse_val_spde_a,rmse_val_spde) # print status # print(paste(k, &quot;folds done&quot;)) } print(paste(&quot;Average R2 in the validation set:&quot;, round(mean(r2_val_spde_a)*100),&quot;%&quot;, &quot;\\nAverage R2 in the training set:&quot;, round(mean(r2_train_spde_a)*100),&quot;%&quot;)) ## [1] &quot;Average R2 in the validation set: 43 % \\nAverage R2 in the training set: 48 %&quot; The average \\(R^{2}\\) of the validation and training sets (respectively 41% and 47%) are in line with the results obtained with the original split. References "],
["stepwise.html", "Section 5 Stepwise Covariates Selection 5.1 Step 1: Split the sample into a 60/20/20 training, validation and test sets 5.2 Bundle step 2 to 4 in a function 5.3 Bundle step 5 to 7 in a loop ran in parallel 5.4 Step 8: Inspect the results and select specification 5.5 Step 9 Final test", " Section 5 Stepwise Covariates Selection The goal of this section is to describe a backward stepwise covariates selection process, i.e. a process to reduce the number of covariates from the 50 potential ones identified in section 3 to circa 10 or less. Here is an overview of the process: Split the sample into a 60/20/20 training, validation and test sets; Write the test formula without one of the 50 candidate covariates; Fit the model on the training set; Validate the model on the validation set and compute goodness of fit statistics; Repeat step 2 to 4 once for each covariate (50 times); Drop the covariate in the absence of which the fit us the best; Repeat steps 2 to 6 until no covariate is left; Inspect the result to identify a parsimonious specification (typically below 10 covariates) yielding a good fit on the validation set; Test the final model on the test set. The backward stepwise covariates selection process is computationally expensive. Fitting one modified BYM model takes around 3.5 minutes on a standard PC3. To carry steps 1 to 5, it takes approximately 175 minutes. Step 6 to 7 takes hence approximately 74 hours. In order to speed it up the process, we write a function to parallelise step 5 across CPU cores. As the number of cores might relatively limited on a PC (typically up to 8), an option is to carry the process on the cloud where an instance with more cores can be rented. For instance, the computing time can be reduced to below two hours with 64 cores. The backward stepwise covariates selection is presented with the income model. 5.1 Step 1: Split the sample into a 60/20/20 training, validation and test sets The observations in the dataset are split into a 60/20/20 training, validation and test sets. The model will be fitted on the training set. Income will be predicted on the training and validation sets. Predicted values in the validation set will be compared with the observed values in order to assess the goodness of it of the model. Once the covariate selection process is complete and a given specification has been chosen (steps 1 to 8 above), prediction are made on the final test set to assess the goodness of fit of the final model. # sample segmentos for training, validation and test #### set.seed(1234) spec = c(train = .6, test = .2, validate = .2) g = sample(cut( seq(nrow(data_model)), nrow(data_model)*cumsum(c(0,spec)), labels = names(spec) )) index_val=which(g==&quot;validate&quot;) index_train=which(g==&quot;train&quot;) index_test=which(g==&quot;test&quot;) data_model$pred=data_model$ingpe data_model$pred[c(index_val,index_test)] &lt;- NA # set the validation pred to NA: they will be predicted by the model and compared with observed values mod_data_jack = data_model # store data into a new dataframe for model fitting mod_data_jack=mod_data_jack[c(index_val,index_train),] # select only the training and validation sets, keep test set of final validation index_val_jack=which(is.na(mod_data_jack$pred)) index_train_jack=which(is.na(mod_data_jack$pred)==F) 5.2 Bundle step 2 to 4 in a function In order to ease the reading of the code, we create a function INLA_steps_2_4 implementing steps 2 to 4 of the covariate selection process. The INLA_steps_2_4: Write the test formula without one of the candidate covariates; Fit the model on the training set; Validate the model on the validation set; Compute goodness of fit statistics. The INLA_steps_2_4 accepts the following arguments: - covariates formulation - index of the covariate to be removed - index of the validation set - index of the training set - dataframe for the model - a string defining the outcome variable, e.g. “ingpe” - a string defining the likelihood function, e.g. Gaussian, Gamma or a Beta distribution # INLA_set2_4_function ##### INLA_steps_2_4=function(covariates_formulation, # formulation of each covariate ix, # the index of the covariate to be removed index_val, # index of the validation set index_train, # index of the training set mod_data_jack, # data for the model outcome, # string defining the outcome variable, e.g. ingpe family){ # string defining the likelihood # formula candidate_covariates &lt;- covariates_formulation formula_test &lt;- reformulate(c(&quot;-1&quot;, &quot;intercept&quot;, paste(candidate_covariates[-ix],collapse=&quot;+&quot;), &#39;f(ID,model = &quot;bym2&quot;, hyper=list( prec = list(prior = &quot;pc.prec&quot;, param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, 0.5))), graph = Segmento.Inla.nb, scale.model = TRUE, constr = T, adjust.for.con.comp = T)&#39;), &quot;pred&quot;) # rescale to allow the Newton-Raphson optimizer to converge if(family==&quot;gaussian&quot;){ formula_test &lt;- reformulate(c(&quot;-1&quot;, &quot;intercept&quot;, paste(candidate_covariates[-ix],collapse=&quot;+&quot;), &#39;f(ID,model = &quot;bym2&quot;, hyper=list( prec = list(prior = &quot;pc.prec&quot;, param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, 0.5))), graph = Segmento.Inla.nb, scale.model = TRUE, constr = T, adjust.for.con.comp = T)&#39;), &quot;pred/10&quot;) # rescale to allow the Newton-Raphson optimizer to converge } # fit the model if(family%in%c(&quot;beta&quot;,&quot;zeroinflatedbinomial1&quot;)){ bym.res=INLA::inla(formula=formula_test, family = family, data = mod_data_jack, Ntrials = mod_data_jack$n_obs, control.predictor=list(link=1,compute=T), control.compute=list(dic=T, cpo=F), control.inla =list(int.strategy = &quot;eb&quot;) ) }else{ bym.res=INLA::inla(formula=formula_test, family = family, data = mod_data_jack, control.predictor=list(link=1,compute=T), control.compute=list(dic=T, cpo=F), control.inla =list(int.strategy = &quot;eb&quot;) ) } # extact fitted values M_fit=bym.res$summary.fitted.values[,&quot;mean&quot;] if(family%in%c(&quot;gamma&quot;)){ M_fit=exp(M_fit) # back transform to level } if(family%in%c(&quot;gaussian&quot;)){ M_fit=M_fit*10 # back transform to level } # funcion for RMSE RMSE=function(set,outcome,data){ res = data[set,outcome]-M_fit[set] RMSE_val &lt;- sqrt(mean(res^2,na.rm=T)) return(RMSE_val) } # funcion for pseudo_r2 pseudo_r2=function(set,outcome,data){ res = data[set,outcome]-M_fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(M_fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } # RMSE: RMSE function defined above RMSE_val=RMSE(index_val,outcome,mod_data_jack) RMSE_train=RMSE(index_train,outcome,mod_data_jack) # R2: pseudo_r2 function defined above r2_val=pseudo_r2(index_val,outcome,mod_data_jack) r2_train=pseudo_r2(index_train,outcome,mod_data_jack) # store results results_list=list(&quot;cov_i&quot;=ix, &quot;cov_name&quot;=candidate_covariates[ix], &quot;formula&quot;=formula_test, # &quot;fitted_values&quot;=M_fit, # &quot;index_val&quot;=index_val, # &quot;index_train&quot;=index_train, # &quot;index_train_val&quot;=index_train_val, &quot;RMSE_val&quot;=RMSE_val, &quot;RMSE_train&quot;=RMSE_train, &quot;r2_val&quot;=r2_val, &quot;r2_train&quot;=r2_train, &quot;outcome&quot;=outcome, &quot;family&quot;=family) rm(bym.res) return(results_list) } We can now test the function. # test the function ##### test_fct=INLA_steps_2_4(covariates_formulation, # formulation of each covariate 1, # the index of the covariate to be removed index_val_jack, # index of the validation set index_train_jack, # index of the validation set mod_data_jack, # data for the model &quot;ingpe&quot;, # outcome &quot;gaussian&quot;) # likelihood The results can then be accessed using the $ sign. For instance, we can look at the \\(R^2\\): print(paste(&quot;R-squared in the validation set:&quot;,test_fct$r2_val, &quot;R-squared in the training set:&quot;,test_fct$r2_train)) ## [1] &quot;R-squared in the validation set: 0.443338424436061 R-squared in the training set: 0.497340475698624&quot; 5.3 Bundle step 5 to 7 in a loop ran in parallel We can now launch the entire backward selection process with a loop. In order to speed up the process, we parallelise the estimation of each single model across the cores: each core is in charge of estimating one model. In the first iteration of the loop, we start with a list of \\(50\\) candidate covariates. \\(50\\) models are estimated. Each model contains all covariates minus one \\(j\\) covariate, where \\(j\\) is different in each model and \\(j=1, ..., 50\\). Once the \\(50\\) models have been estimated, results are collected and the best performing model is identified, say model k The corresponding \\(k\\) covariate is removed from the list of candidate covariate and the loop proceed to the next iteration with a list of \\(50-1=49\\) candidate covariates. # loop over all covariates until only 1 is left ##### candidate_covariates=covariates_formulation # start again with the full list of candidate covariate n2drop=length(covariates_formulation)-1 start_t=Sys.time() results_jacknife=list() # create a list to store all the results of the jacknife selectio process for(n_cov_to_drop in 1:n2drop){ # repeat the step n2drop=12 times until having only 8 covariates set.seed(101) INLA_steps_2_4_par=function(cov_n){ # fit the model dropping one covariates after the other model_fct=INLA_steps_2_4(candidate_covariates, # formulation of each covariate cov_n, # the index of the covariate to be removed index_val_jack, # index of the validation set index_train_jack, # index of the validation set mod_data_jack, # data for the model &quot;ingpe&quot;, # outcome &quot;gaussian&quot;) # likelihood cat(&quot;\\014&quot;) # clean the consol # end_t=Sys.time() # duration=end_t-start_t # print(duration) return(model_fct) } cov_n&lt;-1:length(candidate_covariates) n_cores&lt;-detectCores() cl=makeCluster(n_cores) start_T&lt;-Sys.time() clusterExport(cl=cl, varlist=c(&quot;candidate_covariates&quot;, &quot;INLA_steps_2_4&quot;, &quot;index_val_jack&quot;, &quot;index_train_jack&quot;, &quot;mod_data_jack&quot;, &quot;Segmento.Inla.nb&quot;)) results_list&lt;-clusterApply(cl,cov_n,INLA_steps_2_4_par) end_T&lt;-Sys.time() cat(&quot;\\014&quot;) # clean the consol print(end_T-start_T) stopCluster(cl) gc() # store all the results for quality check results_jacknife[[n_cov_to_drop]]=results_list # drop covariate affecting the least the goodness of fit rmse_val=lapply(results_list,function(x) unlist(x$RMSE_val)) rmse_val_min=which.min(rmse_val) covariates_to_be_removed=results_list[[rmse_val_min]]$cov_name candidate_covariates=candidate_covariates[-which(candidate_covariates==covariates_to_be_removed)] rm(results_list) print(paste(length(candidate_covariates),&quot;covariates remaining&quot;)) } end_t=Sys.time() duration=end_t-start_t print(duration) # 1.84 hours The process took slightly less than 2 hours on a cloud instance with 64 cores. 5.4 Step 8: Inspect the results and select specification We can now inspect the results of the selection process. Let us look at the goodness of fit of the best performing models at each covariate selection step, using the R-squared and the RMSE as goodness of fit statistics. # INCOME Bym2 #### load(paste0(dir_data,&quot;workspace/income_bym2_gaussian.RData&quot;)) # visualize R2 #### # extract r2 r2_val_max=r2_train_max=r2_val_min=r2_train_min=c() for(k in 1:length(results_jacknife)){ r2_val=lapply(results_jacknife[[k]],function(x) unlist(x$r2_val)) r2_train=lapply(results_jacknife[[k]],function(x) unlist(x$r2_train)) # get max r2 r2_val_max_index=which.max(r2_val) r2_val_max_i=r2_val[[r2_val_max_index]] r2_train_max_i=r2_train[[r2_val_max_index]] r2_val_max=c(r2_val_max,r2_val_max_i) r2_train_max=c(r2_train_max,r2_train_max_i) # get min r2 r2_val_min_index=which.min(r2_val) r2_val_min_i=r2_val[[r2_val_min_index]] r2_train_min_i=r2_train[[r2_val_min_index]] r2_val_min=c(r2_val_min,r2_val_min_i) r2_train_min=c(r2_train_min,r2_train_min_i) } # viz r2 data2plot=data.frame(set=c(rep(&quot;val&quot;,length(r2_val_max)),rep(&quot;train&quot;,length(r2_val_max))), index=c(rev(1:length(r2_val_max)),rev(1:length(r2_val_max))), index_jack=c(1:length(r2_val_max),1:length(r2_val_max)), r2_max=c(unlist(r2_val_max),unlist(r2_train_max)), r2_min=c(unlist(r2_val_min),unlist(r2_train_min))) plotly::plot_ly(data=data2plot, x=~index_jack, y=~r2_max, name = ~set, type=&quot;scatter&quot;, mode=&quot;line&quot;) # visualize RMSE #### # extract RMSE RMSE_val_max=RMSE_train_max=RMSE_val_min=RMSE_train_min=c() for(k in 1:length(results_jacknife)){ RMSE_val=lapply(results_jacknife[[k]],function(x) unlist(x$RMSE_val)) RMSE_train=lapply(results_jacknife[[k]],function(x) unlist(x$RMSE_train)) # get max RMSE RMSE_val_max_index=which.max(RMSE_val) RMSE_val_max_i=RMSE_val[[RMSE_val_max_index]] RMSE_train_max_i=RMSE_train[[RMSE_val_max_index]] RMSE_val_max=c(RMSE_val_max,RMSE_val_max_i) RMSE_train_max=c(RMSE_train_max,RMSE_train_max_i) # get min RMSE RMSE_val_min_index=which.min(RMSE_val) RMSE_val_min_i=RMSE_val[[RMSE_val_min_index]] RMSE_train_min_i=RMSE_train[[RMSE_val_min_index]] RMSE_val_min=c(RMSE_val_min,RMSE_val_min_i) RMSE_train_min=c(RMSE_train_min,RMSE_train_min_i) } # viz RMSE data2plot=data.frame(set=c(rep(&quot;val&quot;,length(RMSE_val_max)),rep(&quot;train&quot;,length(RMSE_val_max))), index=c(rev(1:length(RMSE_val_max)),rev(1:length(RMSE_val_max))), RMSE_max=c(unlist(RMSE_val_max),unlist(RMSE_train_max)), RMSE_min=c(unlist(RMSE_val_min),unlist(RMSE_train_min))) plotly::plot_ly(data=data2plot, x=~index, y=~RMSE_min, name = ~set, type=&quot;scatter&quot;, mode=&quot;line&quot;) Based on the results above, the 31st step appears to provide a good balance between the sparsity of the formulation and the goodness of fit. # identify the specification #### step_chosen=31 r2_val=lapply(results_jacknife[[step_chosen]],function(x) unlist(x$r2_val)) r2_val_max_index=which.max(r2_val) formula_selected=results_jacknife[[step_chosen]][[r2_val_max_index]]$formula formula_selected ## pred/10 ~ -1 + intercept + chirps_ev_med + lights_med + dist2coast_r + ## pop_dens + lc_tree + dens_all_roads + dens_secondary + dens_bus + ## LZ_CentralIndusServ + f(ID, model = &quot;bym2&quot;, hyper = list(prec = list(prior = &quot;pc.prec&quot;, ## param = c(0.1, 0.0001)), phi = list(prior = &quot;pc&quot;, param = c(0.5, ## 0.5))), graph = Segmento.Inla.nb, scale.model = TRUE, constr = T, ## adjust.for.con.comp = T) ## &lt;environment: 0x55df44e02298&gt; Here is the list of covariates that were selected: chirps_ev_med: median precipitation lights_med: median lights over the year dist2coast_r: distance to coast pop_dens: population density lc_tree: percentage of trees dens_all_roads: road density (all road categories) dens_secondary: secondary road density dens_bus : bus lanes density LZ_CentralIndusServ: Central Livelihood zones on industry and services. 5.5 Step 9 Final test Lastly, we perform the final test of goodness of fit on the test set. As a reminder, the test set comprise of 20% of the data we set aside before starting the selection process, i.e. the model has not yet seen them. We start by building the dataframe for estimating the final model with the training and test set only. # build dataframe for final test #### data_test=data_model data_test$pred=data_test$ingpe # create a pred variables equal to ingpe, the outcome variable # data_test$pred[index_test] &lt;- NA # set the test pred to NA: they will be predicted by the model and compared with observed data_test$pred[c(index_test, index_val)] &lt;- NA # set the test pred to NA: they will be predicted by the model and compared with observed data_test=data_test[c(index_test,index_train),] index_train_bis=which(is.na(data_test$pred)==F) index_test_bis=which(is.na(data_test$pred)) The estimation is done with the usual inla function and results are plotted on Fig. ??. Figure 5.1: Observed vs predicted income values, BYM 2 model with covariates selected with the backward stepwise selection process Processor: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz, 16.0 GB RAM↩ "],
["summary-of-resuls.html", "Section 6 Results summary 6.1 Goodness of fit at the segmento level 6.2 Segmento level map 6.3 Goodness of fit at the Municipio level", " Section 6 Results summary This sections present the results obtained with the models for the three development indicators: median income, literacy and poverty rates. Goodness of fit statistics are presented both at the segmento level and at the municipio level. In the latter case, the results are presented with models fitted either on the entire datasets or for the 50 representative municipios only. Lastly, we present the map of predicted income, literacy and poverty rates as well as uncertainty maps. 6.1 Goodness of fit at the segmento level Various models were tested for the three development indicators. Spatial dependence was modeled either with a Besag-York-Mollie model (BYM 2) or with a SPDE model. Backward selection process was adopted, starting from the same set of covariates. For the income model, we tested both a Gaussian and a gamma likelihood function. Both literacy and poverty are ratio data, they are hence bounded between 0 and 100%. We therefore used a beta model. As beta distribution cannot include 0 and 1, we added and subtracted \\(0.00001\\) to observation with 0 or 1, respectively. We also tested for both models the goodness of fit of Gaussian model. We also investigated the use zero-inflated and zero-altered beta models for literacy and poverty. For poverty, we also tested two separate model for rural and urban segmentos. This did not yield any noticable improvement in the goodness of fit of the model. We therefore do not present the codes and results for these more complex models. We start by loading the model summaries. # load the results #### dir_result=paste0(root_dir,project_dir,&quot;out/results/&quot;) results_list=dir(dir_result) for(i in 1:length(results_list)){ load(paste0(dir_result, results_list[i])) } results_names=gsub(&quot;.RData&quot;,&quot;&quot;,results_list) lines_r=list() i=1 for(i in 1:length(results_names)){ lines_r[[i]]=list(get(results_names[i])[[c(&quot;outcome&quot;)]], get(results_names[i])[[c(&quot;spat_dep&quot;)]], get(results_names[i])[[c(&quot;family&quot;)]], get(results_names[i])[[c(&quot;cov_select&quot;)]], get(results_names[i])[[c(&quot;r2_train&quot;)]], get(results_names[i])[[c(&quot;r2_val&quot;)]], get(results_names[i])[[c(&quot;r2_test&quot;)]], get(results_names[i])[[c(&quot;RMSE_train&quot;)]], get(results_names[i])[[c(&quot;RMSE_val&quot;)]], get(results_names[i])[[c(&quot;RMSE_test&quot;)]]) if(is.null(lines_r[[i]][[7]])){lines_r[[i]][[7]]=NA} if(is.null(lines_r[[i]][[10]])){lines_r[[i]][[10]]=NA} # print(length(lines_r[[i]])) } lines_df=do.call(rbind.data.frame, lines_r) names(lines_df)=c(&quot;Outcome&quot;,&quot;spatial dependency model&quot;,&quot;likelihood&quot;,&quot;covariate selection method&quot;, &quot;r2 training set&quot;,&quot;r2 validation set&quot;,&quot;r2 test set&quot;, &quot;RMSE training set&quot;,&quot;RMSE validation set&quot;,&quot;RMSE test set&quot;) Table 8.1 summarises the RMSE and r-squared for the 10 models. lines_df[,which(unlist(lapply(lines_df,is.numeric)))]= round(lines_df[,which(unlist(lapply(lines_df,is.numeric)))],digit=2) caption_table=&#39;Summary of Goodness of Fit at the Segmento Level&#39; knitr::kable( lines_df, format=&quot;markdown&quot;, caption = caption_table, booktabs = TRUE, row.names = FALSE) Outcome spatial dependency model likelihood covariate selection method r2 training set r2 validation set r2 test set RMSE training set RMSE validation set RMSE test set illiteracy bym2 beta naive 0.65 0.43 NA 0.06 0.08 NA illiteracy_rate spde beta naive 0.53 0.42 NA 0.07 0.08 NA illiteracy_rate spde beta stepwise 0.55 0.46 0.43 0.07 0.07 0.08 ingpe bym2 gamma naive 0.74 0.44 NA 36.70 66.01 NA ingpe bym2 gaussian stepwise 0.45 0.47 0.40 53.58 63.87 57.01 ingpe spde gaussian naive 0.55 0.53 NA 0.31 0.34 NA ingpe spde gamma naive 0.49 0.44 NA 51.28 66.42 NA ingpe spde gamma stepwise 0.50 0.51 0.49 50.30 62.06 52.79 pobreza_mod bym2 beta naive 0.62 0.25 NA 0.13 0.17 NA pobreza_mod spde beta naive 0.31 0.18 NA 0.17 0.18 NA pobreza_mod spde gaussian stepwise 0.31 0.30 0.24 0.17 0.18 0.19 pobreza_mod spde beta stepwise 0.29 0.28 0.20 0.17 0.18 0.19 For income, the best model is the SPDE one with a Gamma likelihood function and where stepwise covariate selection has been applied. The goodness of fit is relatively high (50%) and holds in test set. For literacy, the three tested models yield results which are very close from each other in terms of goodness of fit. We select the SPDE one with stepwise covariate selection, with a goodness of fit of 43%, a final model. However, the model on moderate poverty doesn’t have a good fit: the goodness of fit of the best model is only 23%. We selected the Gaussian with SPDE and stepwise selection as final model. Fig. ??, ?? and ?? show the predicted vs observed values along with the \\(R^{2}\\) and the RMSE. ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields Figure 6.1: Predicted vs Observed Income (USD), SPDE model stepwise selection Figure 6.2: Predicted vs Observed Median Income (USD), SPDE model stepwise selection Figure 6.3: Predicted vs Observed Moderate Poverty (%), SPDE model stepwise selection 6.2 Segmento level map It is now finally time to produce the maps. The detailed process to produce the map is shown for income with the SPDE model. Codes are not shown for literacy and poverty. So far, prediction have been only made on the segmentos for which EHPM data are available, i.e. on 1664 segmentos. To complete the map, prediction have to be made on the whole 12,435 segmentos of El Salvador. We prepare two data stacks: one covers for the 1664 EHPM segmentos and one for the remaining 10’771 where no EHPM data are available. In the latter segmentos, we have however the covariates data. The model will be trained on the EHPM segmentos and out of sample prediction will be made on the non-EHPM segmentos. load(paste0(dir_data,&quot;workspace/income_spde_gamma.RData&quot;)) data_model=segmento_sh_data_model@data index_ehpm=which(is.na(data_model$ingpe)==F) index_NON_ehpm=which(is.na(data_model$ingpe)) coords.ehpm=coordinates(segmento_sh_subset) segmento_non_ehpm=subset(segmento_sh_data, is.na(segmento_sh_data$ingpe)) coords.non.ehpm=coordinates(segmento_non_ehpm) A.ehpm &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords.ehpm) A.non.ehpm &lt;- inla.spde.make.A(mesh=SV.mesh, loc=coords.non.ehpm) stack.ehpm= inla.stack(data = list(pred=data_model$ingpe[index_ehpm]), A = list(A.ehpm, 1,1), effects = list(s.index, Intercept=rep(1,length(index_ehpm)), data_model[index_ehpm, as.character(cov_candidates_selected_table$Candidates)]), tag=&quot;ehpm&quot;) stack.non.ehpm= inla.stack(data = list(pred=data_model$ingpe[index_NON_ehpm]), A = list(A.non.ehpm, 1,1), effects = list(s.index, Intercept=rep(1,length(index_NON_ehpm)), data_model[index_NON_ehpm, as.character(cov_candidates_selected_table$Candidates)]), tag=&quot;non_ehpm&quot;) join.stack &lt;- inla.stack(stack.ehpm, stack.non.ehpm) The model is now fitted with the usual inla command. start_time=Sys.time() M_map =inla(formula_selected, data=inla.stack.data(join.stack, spde=SV.spde), family=&quot;gamma&quot;, control.predictor=list(A=inla.stack.A(join.stack), link=1,compute=T), control.compute=list(cpo=F, dic=F)) end_time=Sys.time() print(end_time-start_time) ## Time difference of 21.2168 secs The fitted values are accessed thanks to the inla.stack.index introduced in the previous sections. We also extract the standard deviation of the prediction in order to map the uncertainty of the prediction. The latter is summarized by the coefficient of variation. For any given segmento \\(s\\), the coefficient of variation is given by: \\[ \\begin{aligned} coef var_{s}=\\hat{\\sigma_{s}}/\\hat{\\mu_{s}} \\end{aligned} \\] where \\(\\hat{\\sigma_{s}}\\) is the standard deviation of the prediction for segmento \\(s\\) and \\(\\hat{\\mu_{s}}\\) is the average predicted income for segmemto \\(s\\). The coefficient of variation is hence the standard deviation of the prediction expressed in percentage of the prediction. index_inla_ehpm = inla.stack.index(join.stack,&quot;ehpm&quot;)$data index_inla_non_ehpm = inla.stack.index(join.stack,&quot;non_ehpm&quot;)$data M_map_fit=M_map$summary.fitted.values[,&quot;mean&quot;] M_map_fit_ehpm=M_map$summary.fitted.values[index_inla_ehpm,&quot;mean&quot;] M_map_fit_non_ehpm=M_map$summary.fitted.values[index_inla_non_ehpm,&quot;mean&quot;] M_map_sd_ehpm=M_map$summary.fitted.values[index_inla_ehpm,&quot;sd&quot;] M_map_sd_non_ehpm=M_map$summary.fitted.values[index_inla_non_ehpm,&quot;sd&quot;] They fitted values, standard deviations and coefficient of variations are now stored in the polygon dataframe segmento_sh_data_model. segmento_sh_data_model@data$fit_income=NA segmento_sh_data_model@data$fit_income[index_ehpm]=M_map_fit_ehpm segmento_sh_data_model@data$fit_income[index_NON_ehpm]=M_map_fit_non_ehpm segmento_sh_data_model@data$sd_income=NA segmento_sh_data_model@data$sd_income[index_ehpm]=M_map_sd_ehpm segmento_sh_data_model@data$sd_income[index_NON_ehpm]=M_map_sd_non_ehpm segmento_sh_data_model@data$coefvar_income=segmento_sh_data_model@data$sd_income/segmento_sh_data_model@data$fit_income We can then save the spatial polygon dataframe into a .geojson file for later inspection. segmento_wgs_income=sp::spTransform(segmento_sh_data_model, &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) segmento_wgs_income@data=segmento_wgs_income@data%&gt;% select(SEG_ID,AREA_ID,fit_income,ingpe,sd_income,coefvar_income,lights_med,pop_dens,slope,n_obs) rgdal::writeOGR(segmento_wgs_income_sp, paste0(dir_data, &quot;out/map/segmentos_spde_income2.geojson&quot;), driver = &quot;GeoJSON&quot;, layer = 1, overwrite_layer=T) Fig. 6.4 shows the map of the predicted income with a continuous color scale as well as the map of prediction uncertainty4. Figure 6.4: Median Income (USD): continuous color scale The prediction uncertainty is relatively low. In most cases, the coefficient of variation indicates that the standard deviation is smaller than 10% of the estimates. Fig. 6.5 shows the map of the predicted income with a quantile color scale, highlighting the rural urban difference. Figure 6.5: Median Income(USD): quantile color scale Fig. 6.6 shows predicted illiteracy with a continuous color scale and uncertainty while Fig. 6.7 shows predicted illiteracy with a quantile color scale. Figure 6.6: Illiteracy (%): continuous color scale Figure 6.7: Illiteracy (%): quantile color scale Lastly, Fig. 6.8 and Fig. 6.9 shows the results for moderate poverty. Figure 6.8: Moderate Poverty (%): continuous color scale Figure 6.9: Moderate Poverty (%): quantile color scale 6.3 Goodness of fit at the Municipio level We can now investigate the goodness of fit at the municipio level. We adopt two methods for doing so. First, we aggregate the results at the municipio level and compare them with municipio level statistics. An issue with this approach is that in each municipio, some segmentos are used in the training and validation sets. Furthermore, EHPM data are representative at the municipio level only for a subset of municipios. The difference between fitted and observed values will hence be affected both by modelling errors and sampling errors while it will be hard to gauge the external validity of the model as the training and validation sets are mixed. To remedy this, we also adopt a second approach. We select the segmentos from representative municipios only. We then split this remaining dataset into a training and validation sets, making sure that all segmentos from any given municipio fall either in the training or the validation set. We can then obtain goodness of fit statistics at the municipio level for observations which have not been used to train the model. 6.3.1 Obtion 1: Aggregating results at the municipio level using all segmentos First we compute the average segmento median income level on the entire sample at the municipio level. We then compute the average prediction of the test set at the municipio level. Finally, we compare observed average income against predicted average income at the municipio level, both for the representative and non-representative municipios. # reload shape source(&quot;../utils.R&quot;) segmento_sh=rgdal::readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Segmentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields ehpm17_predictors=read.csv(paste0(dir_data, &quot;out/all_covariates_and_outcomes.csv&quot;)) ehpm17_predictors=ehpm17_predictors%&gt;% mutate(SEG_ID=as.character(SEG_ID), SEG_ID=ifelse(nchar(SEG_ID)==7, paste0(0,SEG_ID), SEG_ID)) # extract the municipio identifier from the shapefile segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% # dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) # identify where there is EHPM data segmento_sh_subset=subset(segmento_sh_data, is.na(segmento_sh_data@data$ingpe)==F) segmento_sh_subset$fit=NA segmento_sh_subset$fit[income_spde_stepwise$index_train]=income_spde_stepwise$fit[income_spde_stepwise$index_train] segmento_sh_subset$fit[income_spde_stepwise$index_val]=income_spde_stepwise$fit[income_spde_stepwise$index_val] segmento_sh_subset$fit[income_spde_stepwise$index_test]=income_spde_stepwise$fit[income_spde_stepwise$index_test] data_municipio=segmento_sh_subset@data%&gt;% select(SEG_ID,fit,ingpe)%&gt;% left_join(segmento_sh_data@data%&gt;% select(SEG_ID,DEPTO,MPIO)%&gt;% mutate(DEPTO_MPIO=paste0(DEPTO,MPIO))%&gt;% select(-c(DEPTO,MPIO)), by=&quot;SEG_ID&quot;) # extract the representative information from the EHPM ehpm=read.csv(paste0(dir_data, &quot;tables/ehpm-2017.csv&quot;)) segID=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;), sheet=&quot;2017&quot;) ehpm_municauto=ehpm%&gt;% select(r005,autorrepresentado,idboleta)%&gt;% left_join(segID, by=&quot;idboleta&quot;)%&gt;% rename(&quot;SEG_ID&quot;=&quot;seg_id&quot;)%&gt;% group_by(SEG_ID)%&gt;% summarise(autorrepresentado=mean(autorrepresentado)) # add representativity to results dataframe data_municipio_rep=data_municipio%&gt;% left_join(ehpm_municauto, by=&quot;SEG_ID&quot;) # aggregate results at municipios #### # all data_municipio_all=data_municipio_rep%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(obs=mean(ingpe,na.rm=T), fit=mean(fit,na.rm=T))%&gt;% mutate(representative=&quot;all&quot;) # non representative data_municipio_NONREP=data_municipio_rep%&gt;% filter(autorrepresentado==0)%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(obs=mean(ingpe,na.rm=T), fit=mean(fit,na.rm=T))%&gt;% mutate(representative=&quot;no&quot;) # representative data_municipio_REP=data_municipio_rep%&gt;% filter(autorrepresentado==1)%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(obs=mean(ingpe,na.rm=T), fit=mean(fit,na.rm=T))%&gt;% mutate(representative=&quot;yes&quot;) # bind the dataframe data_municipio=data_municipio_REP%&gt;% bind_rows(data_municipio_NONREP)%&gt;% bind_rows(data_municipio_all) index_rep=which(data_municipio$representative==&quot;yes&quot;) index_non_rep=which(data_municipio$representative==&quot;no&quot;) index_all=which(data_municipio$representative==&quot;all&quot;) # fit RMSE=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RMSE_val &lt;- sqrt(mean(res[,&quot;obs&quot;]^2,na.rm=T)) return(RMSE_val) } pseudo_r2=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } r2_all=pseudo_r2(index_all,&quot;obs&quot;,data_municipio,data_municipio$fit) r2_rep=pseudo_r2(index_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) r2_non_rep=pseudo_r2(index_non_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) RMSE_all=RMSE(index_all,&quot;obs&quot;,data_municipio,data_municipio$fit) RMSE_rep=RMSE(index_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) RMSE_non_rep=RMSE(index_non_rep,&quot;obs&quot;,data_municipio,data_municipio$fit) Results are plotted on Fig. ??. Figure 6.10: Predicted vs Observed Median Income: Municipio level, entire sample Results for literacy and poverty are shown on Fig. ?? and Fig. ?? respectively. Figure 6.11: Predicted vs Observed Illiteracy rate: Municipio level, entire sample Figure 6.12: Predicted vs Observed Moderate Poverty rate: Municipio level, entire sample The goodness of fit is much better for the representative municipios (in green) than for the non-representative ones (in red). In the latter ones, the goodness of fit is affected both by the model errors and the sampling errors. Furthermore, it is hard to take the goodness of fit statistics for the representative segmentos as face values as they are based on a mixed of training and validation data. We therefore re-run the model below with only the segmentos in representative municipios. 6.3.2 Option 2: Aggregating results at the municipio level using only segmentos in representative municipios We select only the segmentos in representative municipios and allocate municipios to a training and validation set. This insure that all segmentos from each municipio are either in the training and validation set. There is a total of 50 representative municipios in the EHPM, totaling 1114 segmentos. The only change compared to the code shown in the earlier sections is the creation of the training and validation set. The process is: Allocate representative municipios into a training and validation set; Select the data for the segmentos in representative municipios; Add the municipio training and validation set indexes to the segmento level dataframe. # 1. Allocate representative municipios in training and validation sets #### list_municipio=data_municipio_REP%&gt;% distinct(DEPTO_MPIO) set.seed(12345) spec = c(train = .8, validate = .2) g = sample(cut( seq(nrow(list_municipio)), nrow(list_municipio)*cumsum(c(0,spec)), labels = names(spec) )) list_municipio$index=g # 2. select the data for the segmentos in representative municipios #### segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% # dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) segmento_sh_data@data=segmento_sh_data@data%&gt;% mutate(DEPTO_MPIO=paste0(DEPTO,MPIO)) segmento_sh_subset_muni=subset(segmento_sh_data, is.na(segmento_sh_data@data$ingpe)==F&amp; segmento_sh_data@data$DEPTO_MPIO%in%list_municipio$DEPTO_MPIO) # identify where there is EHPM data and representative municipios # 3. Add the *municipio* training and validation set indexes to the *segmento* level dataframe segmento_sh_subset_muni@data=segmento_sh_subset_muni@data%&gt;% left_join(list_municipio, by=&quot;DEPTO_MPIO&quot;) # identify indexes #### index_train=which(segmento_sh_subset_muni$index==&quot;train&quot;) index_val=which(segmento_sh_subset_muni$index==&quot;validate&quot;) The rest of the process is the same: we create the mesh, the SPDE, the matrix of weights, the stack, the formula (we used to naive one), fit the model and extract the fitted values (code not shown). We then add the fitted values to the shapefile, aggregate the value at the municipio level and compute separate goodness of fit statistics for the municipios in the training and validation sets. # add them into the shapefile #### segmento_sh_subset_muni@data$fit_income=NA segmento_sh_subset_muni@data$fit_income[index_train]=results.train segmento_sh_subset_muni@data$fit_income[index_val]=results.val # aggregate at the municipio level #### data_muni=segmento_sh_subset_muni@data%&gt;% group_by(DEPTO_MPIO)%&gt;% summarise(fit=mean(fit_income), obs=mean(ingpe), set=unique(index)) # goodness of fit #### RMSE=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RMSE_val &lt;- sqrt(mean(res[,&quot;obs&quot;]^2,na.rm=T)) return(RMSE_val) } pseudo_r2=function(set,outcome,data,fit){ res = data[set,outcome]-fit[set] RRes=sum((res)^2,na.rm = T) RRtot=sum((data[set,outcome]-mean(fit[set],na.rm=T))^2,na.rm = T) pseudo_r2_val=1-RRes/RRtot return(pseudo_r2_val) } index_train_muni=which(data_muni$set==&quot;train&quot;) index_val_muni=which(data_muni$set==&quot;validate&quot;) r2_train_income=r2_train=pseudo_r2(index_train_muni,&quot;obs&quot;,data_muni,data_muni$fit) r2_val_income=r2_val=pseudo_r2(index_val_muni,&quot;obs&quot;,data_muni,data_muni$fit) RMSE_train_income=RMSE_train=RMSE(index_train_muni,&quot;obs&quot;,data_muni,data_muni$fit) RMSE_val_income=RMSE_val=RMSE(index_val_muni,&quot;obs&quot;,data_muni,data_muni$fit) Results are plotted on Fig. ??. Figure 6.13: Predicted vs Observed Median Income: Municipio level, representative municipios only Results for literacy and poverty are shown on Fig ?? and @ref(fig: pobreza_mod-fit-municipio-2) respectively. Figure 6.14: Predicted vs Observed Illiteracy: Municipio level, representative municipios only (#fig:pobreza_mod-fit-municipio-2-chunk)Predicted vs Observed Moderate Poverty: Municipio level, representative municipios only The goodness of fit of the three models at the municipio level are very high and summarised on table 8.2. Outcome R-squared training (%) R-squared validation (%) RMSE training (USD) RMSE validation (USD) Median Income (USD) 87.32 83.48 19.21 16.05 Illiteracy (% of adults) 73.18 77.70 0.03 0.03 Moderate Poverty (% of adults) 58.63 60.44 0.05 0.06 An option would have been to create an interactive map with the package leaflet, but we chose to edit the map in QGIS and display it as an image in order to ease the rendering of the maps↩ "],
["discussion.html", "Section 7 Discussion", " Section 7 Discussion This report introduces high resolution mapping based on Bayesian geospatial techniques implemented in the open source statistical computing environment R (R Core Team 2018a). The core of the modelling method is implemented in the R package INLA (Rue, Martino, and Chopin 2009; Lindgren, Rue, and Lindström 2011; Martins et al. 2013; Lindgren and Rue 2015). The emphasis is on providing and explaining the R codes required to go from the raw data to the final maps. Three development indicators are mapped for El Salvador: median income, adult literacy and poverty. The data on these development indicators come from the Encuesta de Hogares de Propositos Multiples (EHPM), an annual survey conducted in 1664 segmentos among 20,645 household in 2017. A suite of more than 20 remote sensing and geographic information system open source data are used to derive a list of 86 potential predictors, hereafter covariates. After having standardized the covariates, we apply an unsupervised dimension reduction process to reduce this number to 50. This limits the effect of multicollinearity in subsequent modelling and limits the risk of retaining covariates because of chance correlation in the subsequent covariates selection. Next, we apply a backward stepwise covariates selection process in order to select a parsimonious model specification. We tested several likelihood function (Gaussian, Gamma and Beta) and spatial dependence structure (SPDE or the BYM 2 models). The best results are obtained with the SPDE approach and, at the segmento level, yields a \\(R^2\\) on the final test set of 50% for income, 43% for illiteracy and only 23% for poverty. Once the results are aggregated at the municipio level, the \\(R^2\\) increases to 84% for income, 80% for literacy and 61% for poverty. We spent limited time exploring interaction terms and non-linear relationships between covariates and outcome variables. Another avenue for amelioration would be to develop a spatio-temporal model in order to take into account the time dimension. This is made possible as the EHPM is collected annually. A spatio-temporal model would permit to monitor trends in the socio-economic and SDG indicators of interest. Lastly, other modeling architectures such as random forests or convolutional neural networks could take advantage of nonlinear effects, potentially increasing the predictive accuracy of the models. Another direction for potentially improving the granularity of socio-economic maps lies in the use of mobile phone call-detail records. Covariate data derived from mobile phone call-detail records (CDRs) have been shown to correlate well with poverty indicators both at the household and aggregate level (e.g. Blumenstock, Cadamuro, and On 2015; Steele et al. 2017). CDRs have the advantage also of being highly granular in both space and time, presenting the possibility to potentially monitor variation in population wellbeing at higher frequency. While the technical demands and requirements to negotiate access to CDR data and conduct meaningful analyses with them is high, there are many potential applications for these data (e.g. mobility analysis, disaster response and preparedness) and the marginal cost of each additional project is low once the agreements and the IT system are in place. Lastly, a lot of the data pre-processing work and some of the analytical steps could be packaged and automatised in a software. This could further streamline the process of creating these high resolution maps of development indicators. These high resolution maps of development indicators may allow more accurate targeting of government interventions aimed at reducing poverty or increasing literacy rates among adults. Secondly, the methods presented here are portable to other socioeconomic and SDG indicators of interest. They could play a key role in monitoring and reporting on SDG achievements. Thirdly, other than the survey segmento level survey data, the methods described here are conducted using freely available data sources on freely-available software. Fourth, the method does not rely on the use of census data as do traditional small area estimation methods. It can hence be applied independently of any census round. Fifthly, the methods can be readily deployed by analysts with masters-level statistical training. These five points make the method very suitable for incorporation in routine and standard practices of National Statistical Offices (NSOs). A next possible step is to present these methods to NSOs to generate discourse on the value of these methods, potential improvements to increase their operational relevance and implications for their integration into decision-making frameworks. Our hope is that the relative ease of these methods, the open-source nature of the software and covariate data as well as the large number of code examples in this report (and on the web in general) will encourage their adoption by NSOs. Experts from the Flowminder Foundation are available to provide technical training and support to strengthen in-house capacity at NSOs to use these methods. This report could constitute the backbone of a short course of up to five days to equip a first cohort of statisticians with the know-how for using in confidence these methods, data and the R software for high resolution mapping of development indicators. References "],
["theory.html", "Section 8 Appendix 1: Primer on INLA 8.1 Area data vs point-referenced data 8.2 Model formulation", " Section 8 Appendix 1: Primer on INLA This section provides an introduction to high resolution mapping based on Bayesian geostatistical techniques. The aim is to provides an intuitive understanding of the models rather than an in-depth derivation and explanation of the theory underpinning them. This section is largely based on the books by Zuur et al. (Zuur, Ieno, and Saveliev 2017) and Blangiardo and Cameletti (Blangiardo and Cameletti 2015). 8.1 Area data vs point-referenced data There are three types of spatial data (Blangiardo and Cameletti 2015): Area or lattice data: the outcome variable to be modelled is a “random aggregate value over an aeral unit”(Blangiardo and Cameletti 2015). The difference between area and lattice data is that the latter is aggregated over a regular grid while the former is aggregated over a set of irregular polygons such as administrative boudaries. For instance, income aggregated at the segmento level are area data. Point-referenced (or geostatistical) data: the outcome variable to be modelled is “a random outcome at a specific location” (Blangiardo and Cameletti 2015), such as households’ income measured at the household location georeferenced with latitude and longitude coordinates. Spatial point patterns: the outcome variable to be modelled represents the occurence or not of an event at a given location. While point referenced data are random outcomes at a specific location, the location of the occurence or not are themselve random in spatial point patterns. For instance, road traffic accidents could be modelled as a spatial point patterns. The EHPM data were collected at the household level. They were then aggregated at the segmento level, they are hence area data. Nevertheless, we will also explore a model assuming they are point-referenced data, using the centroids of the segmentos as the location. 8.1.1 Bayesian approach In the frequentist approach, the interest lies in estimating in the probabilty of the data, say income, given the parameters, \\(P(data | \\beta)\\), i.e. one assumes the parameters exist. In a Bayesian approach, the interest lies in estimating the probability of the parameters given the data, \\(P(\\beta | data)\\). The Bayes theorem states that: \\[ \\begin{aligned} P(A&amp;B)=P(A|B)P(B) \\end{aligned} \\] \\[ \\begin{aligned} P(A|B)=\\frac{P(B|A)P(A)}{P(B)} \\end{aligned} \\] Replacing \\(A\\) and \\(B\\) with the quantity of interest, this gives: \\[ \\begin{aligned} P(\\beta|data)=\\frac{P(data|\\beta)P(\\beta)}{P(data)} \\end{aligned} \\] where \\(P(\\beta|data)\\) is the posterior distribution of the parameter \\(\\beta\\), \\(P(data|\\beta)\\) is the likelihood function of the data (e.g. a Gamma distribution for income), \\(P(\\beta)\\) is the the prior distribution of the \\(\\beta\\) parameter. This can be expressed as: \\[ \\begin{aligned} P(\\beta|data)\\propto P(data|\\beta)P(\\beta) \\end{aligned} \\] which reads as the posterior distribution of \\(\\beta\\) is proportional to \\(P(data|\\beta)P(\\beta)\\). There are various strategies for getting \\(P(data|\\beta)P(\\beta)\\): simulation with a Monte Carlo or Markov Chain Monte Carlo algorythm, or deterministic approach via Integrated Nested Laplace Approximation (INLA) implemented in the INLA package. 8.2 Model formulation The aim is to model the relationship between the outcome variable \\(y\\), for instance segmento median income, and a set of covariates \\(\\mathbf{X}\\), the bold notation denoting that \\(\\mathbf{X}\\) is matrix with each column corresponding to a covariate. 8.2.1 A simple model not accounting for spatial dependence Ignoring for now the spatial dimension, this could be modeled as: \\[ \\begin{aligned} \\mathbf{y}=\\beta_{0}+\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\epsilon} \\end{aligned} \\] \\[ \\begin{aligned} \\mathbf{\\epsilon}\\sim N(0,\\mathbf\\Omega) \\end{aligned} \\] where \\(\\mathbf{y}\\) is a income vector of size \\(n\\) for the \\(n\\) segmentos, with each element \\(y_{i}\\) of \\(i=1, ... ,n\\) is the median income level in segmemto \\(i\\), \\(\\mathbf{X}\\) is the matrix of covariates with \\(k\\) columns for the \\(k\\) covariates and \\(n\\) rows for the \\(n\\) segmentos, \\(\\mathbf{\\beta}\\) is the vector of \\(k\\) parameters indicating the effect of each covariate on \\(y\\), \\(\\mathbf{\\epsilon}\\) is the vector of error term of size \\(n\\), distributed normally with mean \\(0\\) and variance-covariance defined by the matrix \\(\\mathbf{\\Omega}\\): \\[ \\begin{aligned} \\mathbf\\Omega = \\left[\\begin{array} {rrrr} \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\\\ 0 &amp; \\sigma^2 &amp; \\dots &amp; \\vdots \\\\\\ \\vdots &amp; &amp;\\ddots &amp; 0 \\\\\\ 0 &amp; \\dots &amp; 0 &amp; \\sigma^2 \\end{array}\\right]=\\sigma^2\\mathbf{I} \\end{aligned} \\] with each off-diagonal element of \\(\\mathbf\\Omega\\) representing the covariance of \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\), here set to \\(0\\), and \\(\\sigma^2\\) is the variance, which can hence be more consisely written as \\(\\sigma^2\\mathbf{I}\\), where \\(\\mathbf{I}\\) is an identity matrix where the diagonal elements are \\(1\\) and the off-diagonal elements are \\(0\\). The strong assumption of this model is that the term \\(\\epsilon_{i}\\) is idependently and identically distributed, often abreviated as iid distributed. Let us unpack this expression. Independence. The independence assumption implies that \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\) for any two pairs of segmento \\(i\\) and \\(j\\) will be independent, even if \\(i\\) and \\(j\\) are neighbouring segmentos. This is represented by the covariance between \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\) for \\(i\\neq j\\) being set to zero in \\(\\Omega\\). We can expect that it does not hold in the case of average income per segmento. For instance, let us imagine that a large company is recruiting its worksforce in segmentos \\(i\\) and \\(j\\) and that no data is avaible in \\(\\mathbf{X}\\) to take this into account. Hence, \\(\\epsilon_{i}\\) and \\(\\epsilon_{j}\\) will be highly correlated, violating the independence assumption. Identical distribution. The are no symbol \\(i\\) in the normal distribution \\(N(\\mu,\\sigma^2)\\), i.e. \\(\\epsilon_{i}\\) of all segmento are assumed to have a mean \\(\\mu\\) and a variance \\(\\sigma^2\\). However, both the mean and the variance might vary across locations. For instance, one can expected that in an areas where most of the workforce is employed in agriculture, weather shocks might implies larger deviation from model prediction than in areas where most of the workforce is employed in the service sector, leading to different variance. Similarly, the model might systematically over- or under-predict in some areas, leading to \\(\\epsilon_{i}\\) with \\(\\mu\\) higher or under zero. The violation of theses assumptions lead to incorrect estimate of the precison of the \\(\\mathbf{\\beta}\\) parameters. Furthermore, modelling explicilty the spatial dependence increase the predictive performance of the model; not taking into account the location implies throwing away a imporant piece of information. 8.2.2 A spatial model In order to model explicitly the spatial dependence, an additional term is added to equation: \\[ \\begin{aligned} \\mathbf{y}=\\beta_{0}+\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\epsilon}+\\mathbf{\\upsilon} \\end{aligned} \\] where \\(\\mathbf{\\epsilon}\\) is distributed according to equation presented in the previous sub-section and \\(\\mathbf{\\upsilon}\\) is a spatially correlated random effect distributed according: \\[ \\begin{aligned} \\mathbf{\\upsilon}\\sim N(0,\\mathbf\\Sigma) \\end{aligned} \\] where \\(\\Sigma\\) is non-diagonal matrix given by: \\[ \\begin{aligned} \\mathbf\\Sigma = \\left[\\begin{array} {rrrr} \\sigma_{u}^2 &amp;\\phi_{1,2}^2 &amp; \\dots &amp; \\phi_{1,n}^2 \\\\\\ \\vdots &amp; \\sigma_{u}^2 &amp; \\dots &amp; \\vdots \\\\\\ \\vdots &amp; &amp;\\ddots &amp; \\phi_{n-1,n-1}^2 \\\\\\ \\dots &amp; \\dots &amp; \\dots &amp; \\sigma_{u}^2 \\end{array}\\right] \\end{aligned} \\] where \\(\\sigma_{u}^2\\) is the variance of the \\(\\upsilon\\) and the \\(\\phi_{i,j}=corr(\\upsilon_{i},\\upsilon_{j})\\neq0\\) is the correlation between the spatial random effect at location \\(i\\) and \\(j\\). The challenge now is to estimate the \\(\\phi_{i,j}\\) parameters. In the present case, the matrix \\(\\mathbf\\Sigma\\) has a dimension of \\(1664*1664\\), i.e 2’768’896 parameters to estimates. 8.2.3 Estimating the model with aeral data When working with aearal data, quantiyfying proximity with an euclidean distance is not adequate. First, one would need to figure out which point of the area should be considered when measuring the distance. One option could be to take some measure of the centre of each segmentos, however larger segmentos would then appear more remote than smaller one. The prefered approach with aeral data is hence slightly different: proximity is defined in terms of which segmentos do share a border. For sack of simplicity, let us assume for the moment that the link function is a simple identity function such that \\(g(\\mu(s_{i}))=\\mu(s_{i})\\). \\(\\mu(s_{i})\\) can hence be expressed as: \\[ \\begin{aligned} \\mu(s_{i})=\\eta_{i}=\\mathbf{X(s_{i})}\\mathbf{\\beta}+\\upsilon_{i} \\end{aligned} \\] A conditional autoregressive model (CAR) correlation function is used to model the spatial random effect \\(\\upsilon_{i}\\) (when dealing with point reference data, a Matern correlation and its SDPE expression is used instead, see next sub-section). The spatial dependency is assumed to be Markovian in nature, i.e. the spatial correlation can be summarized by the spatial correlation between direct neighbours. The distribution of each \\(\\upsilon_{i}\\) conditional on all other \\(\\upsilon\\) is expressed as: \\[ \\begin{aligned} \\upsilon_{i}|\\upsilon_{-i} \\sim N(\\sum_{j\\neq i}^{N}c_{i,j}\\upsilon_{j}, \\sigma_{i}^{2}) \\end{aligned} \\] where \\({-i}\\) means all except \\(i\\). In the simplest model, called the intrinsic CAR, \\(c_{i,j}\\) is \\(1\\) if \\(j\\) is neigbour of \\(i\\) and \\(0\\) otherwise. The conditional mean of each \\(\\upsilon_{i}\\) is hence an average of its direct neighbour. The joint distribution of all \\(\\upsilon_{i}\\) is also Gaussian and written as: \\[ \\begin{aligned} \\mathbf{\\upsilon} \\sim N(0, (\\mathbf{I-C})^{-1}\\mathbf{M}) \\end{aligned} \\] where \\(\\mathbf{I}\\) is a matrix of \\(1\\) and \\(0\\), \\(\\mathbf{C}\\) contains the \\(c_{i,j}\\) and \\(\\mathbf{M}\\) is the covariance matrix. Various CAR model specify various function for \\(\\mathbf{C}\\) and \\(\\mathbf{M}\\). The homogeneous CAR model set: \\[ \\begin{aligned} \\mathbf{C}=\\phi\\mathbf{A} \\space \\space \\space \\space \\space \\mathbf{M}=\\mathbf{I}\\sigma^{2}_{CAR} \\end{aligned} \\] where \\(\\mathbf{A}\\) is \\(1\\) if area are neighbour and \\(0\\) otherwise and \\(\\phi\\) needs to be estimated: larger it is, more corellated are the random effects. The instric car model set \\(\\phi\\) to 1. One of the issues of CAR models is to lead to overfitting, i.e. model perform well on the training set of the data but poorly when doing out of sample prediction on the test set for validation purposes. A solution is to smooth the pattern of the spatial random effects. With a lower \\(\\sigma^{2}_{CAR}\\), the \\(\\upsilon\\) will vary less from neighbour to neigbour, leading to less overfit. In order to push the \\(\\sigma^{2}_{CAR}\\) to be lower, one can work on the priors. The standard way of doing this is by using penalized complexity priors whereby one specify the probability that \\(\\sigma^{2}_{CAR}\\) will be larger than a given number. To further limit the risk of overfitting. The standard way is to decompose the spatial random effects into one part which is spatially correlated and one part which is pure noise. This is called the modified Besag-York-Mollie (Simpson et al. 2017), which we will be using in the next section. 8.2.4 Estimating the model with point-referenced data Let us now assume that the segmentos median income are point referenced data. We now reconsider \\(\\mathbf{y}\\) as a random variable distributed at \\(n\\) locations \\(s_{1}, ...,s_{n}\\). The income data can be considered as a sample of \\(y(s_{1}), ..., y(s_{n})\\) from the random process \\(\\mathbf{y(s)}\\). Assuming that the \\(y(s_{i})\\) are normally distributed, we have: \\[ \\begin{aligned} y(s_{i})\\sim N(\\mu(s_{i}),\\sigma^2) \\end{aligned} \\] where \\(\\mu(s_{i})\\) is expressed as a function of a structured additive predictor \\(\\eta_{i}\\), such that \\(g(\\mu(s_{i}))=\\eta_{i}\\). For sack of simplicity, let us assume for the moment that the link function is a simple identity function such that \\(g(\\mu(s_{i}))=\\mu(s_{i})\\). \\(\\mu(s_{i})\\) can hence be expressed as: \\[ \\begin{aligned} \\mu(s_{i})=\\eta_{i}=\\mathbf{X(s_{i})}\\mathbf{\\beta}+\\upsilon_{i}+\\epsilon_{i} \\end{aligned} \\] Assuming the that \\(\\upsilon_{i}\\) is normally distributed, it makes it a Gaussian Field: \\[ \\begin{aligned} \\upsilon_{i} \\sim GF(\\mathbf{0},\\mathbf{\\Sigma}) \\end{aligned} \\] where \\(GF\\) stands for Gaussian fields. Assuming that the covariance is Markovian in nature, i.e. only the spatial correlation can be summarized by the spatial correlation between direct neighbours, then the \\(\\upsilon_{i}\\) is distributed acording to a Gaussian Markov Random Field (GMRF): \\[ \\begin{aligned} \\upsilon_{i} \\sim GRMF(\\mathbf{0},\\mathbf{\\Sigma}) \\end{aligned} \\] As \\(\\mathbf{\\Sigma}\\) can be large (a matrix with more than 2 millions element in case of the segmentos), a determininistic structure is imposed on it in order to speed up the estimation, When working with point referenced data, the variance-covariance \\(\\mathbf{\\Sigma}\\) is expressed in terms of the Matern correlation function : \\[ \\begin{aligned} \\mathbf{\\Sigma}=\\sigma_{\\upsilon}^2 cor_{Matern}(\\upsilon(s_{i}),\\upsilon(s_{j})) \\end{aligned} \\] The advantage of the Matern correlation function is that only a few parameters need to be estimated. In order to further simplifiy the computation, the Matern correaltion can be re-expressed with a Stochastic Partial Differencial Equation (SPDE). Once the SPDE equation solved, one can use the SPDE parameters to solve the Matern correlation parameters. In order to solve the SPDE, a spatial mesh is created on the point reference data. For each node of the mesh, we get a value \\(w_{k}\\) via the finite element approach. The \\(w_{k}\\) also form a GMRF. Once we have the \\(w_{k}\\), we can calculate the \\(\\upsilon_{i}\\) as a weighted sum of \\(w_{k}\\) time \\(a_{i,k}\\)s, where \\(a_{i,k}\\) are the distance of \\(s{i}\\) to node \\(k\\). This allows us to obtain the posterior distribution of the \\(\\upsilon_{i}\\)s. References "],
["data.html", "Section 9 Appendix 2: Data Pre-processing 9.1 Data sources 9.2 Loading the survey 9.3 Loading the vectors 9.4 Loading the rasters 9.5 Rasters calculation 9.6 Distance calculation 9.7 Spatial statistics at segmento level 9.8 Merging the covariates with the EHPM data 9.9 Summary", " Section 9 Appendix 2: Data Pre-processing Data pre-processing can be defined as the work required to go from raw data into data usable for modelling. The raw data used in this analysis are remote sensing and other geographic information system data, herafter RS data, such as digitized maps of elevation, population density or public services locations. These data need to be aggregated and linked with the survey containing the data on the development indicators we aim to map. Data pre-processing is arguably one of the most time-consuming tasks of any data science project. This section documents the path required to convert the raw data into data usable for high resolution mapping. Here are the main steps: Load the RS layers; Create summaries for RS layers which come with a time dimension (e.g. average precipitation over the last thirthy years); Perform additional calculation as required (e.g. compute the Soil degradation index based on soil degradation sub-indicator); Compute distance metrics to public services, businesses and some natural features; Align the Coordinates reference systems across layers (geographic or projected); Aggregate the RS layers to the map of interest (e.g. the segmento map) with a chosen set of spatial statistics (average precipitation per segmento); Match the RS aggregates with the outcome data (poverty, income, literay); Write the data frame to a .csv file for later use. Readers only interested by the modelling part can skip this section entirely. 9.1 Data sources The 2017 household survey Encuesta de Hogares de Propositos Multiples (EHPM), collected by the national statistical office of El Salvador (DIGESTYC), is used to compute the three main development indicators below: Median income, Poverty, Literacy. The EHPM survey data are stored in the file ehpm-2017.csv. It is publicly available on the DIGESTYC website here. DIGESTYC provided the research team with the segmento identifiers, allowing for linking the RS data with the EPHM data at the segmento level rather than the canton level as it would have been the case if only using the publicly available data. The segmento identifier is stored in the file Identificador de segmento.xlsx. A suite of RS data are used as potential predictors of the three indicators above. Data sources are listed in Table ?? (see web-book version of this tutorial for the links). Names Type Source Population count raster CIESIN Precipitation raster Climate Hazards Center Lights at night raster NOAA Intergrated Food Security Phase Classsification vector FEWS Net Livelihood Zones vector FEWS Net Altitude raster NASA Shuttle Radar Topography Mission (SRTMv003) Soil degradation raster Trend.Earth Buildings vector OpenStreetMap Points of interest points vector OpenStreetMap Roads network vector OpenStreetMap Vegetation Greenness (NDVI) raster U.S. Geological Survey (USGS) Schools vector La Direccion General de Estadistica y Censos (DIGESTYC) Health centres csv La Direccion General de Estadistica y Censos (DIGESTYC) Hospitals csv La Direccion General de Estadistica y Censos (DIGESTYC) Temperature raster WorldClim Version2 (Fick and Hijmans 2017) Slope raster WorldPop Archive global gridded spatial datasets Distance to OSM major roads raster WorldPop and CIESIN Distance to OSM major roads intersections raster WorldPop and CIESIN Distance to OSM major waterway raster WorldPop and CIESIN Built settlement raster WorldPop and CIESIN Land cover raster European Space Agency 9.2 Loading the survey We start by loading the segmento shapefile and the EHPM survey data: ehpm: this is the EHPM survey data segmento_hh_id: this is the segmento id matched to the household id, it allows us to map households at the segmento level segmento shapefile: this is the map of all segmentos In order to get a more easily readable directory path, let us create an object with the path to the folder where the data are stored: # modify dir_data to where you stored the data root_dir=&quot;~/&quot; project_dir=&quot;data/&quot; dir_data=paste0(root_dir,project_dir) ehpm=read.csv(paste0(dir_data, &quot;tables/ehpm-2017.csv&quot;)) segmento_hh_id=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;)) 9.3 Loading the vectors Vector data are polygons (e.g. adminstrative areas), lines (e.g. roads network) or points (e.g. coordinates of schools). They can be stored in various formats, the most common being ESRI shapefiles and geojson files. 9.3.1 Loading the administrative boundaries Let us now load the administrative maps of the segmentos and departementos withg the command readOGR from the rgdal package. Shapefiles are loaded as a SpatialPolygonDataframe, i.e. a geo-referenced set of polygons to which a data frame is attached. dir_shape=&quot;spatial/shape/&quot; segmento_sh=readOGR(paste0(dir_data, dir_shape, &quot;admin/STPLAN_Segmentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Segmentos.shp&quot;, layer: &quot;STPLAN_Segmentos&quot; ## with 12435 features ## It has 17 fields departamentos_sh=readOGR(paste0(dir_data, dir_shape, &quot;admin/STPLAN_Departamentos.shp&quot;)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/rstudio/data/spatial/shape/admin/STPLAN_Departamentos.shp&quot;, layer: &quot;STPLAN_Departamentos&quot; ## with 14 features ## It has 6 fields The segmento_sh shape file loaded above consists for instance of a data frame with 17 fields (i.e. variables). The names of each field can be accesed by running the command below. names(segmento_sh) ## [1] &quot;OBJECTID&quot; &quot;DEPTO&quot; &quot;COD_DEP&quot; &quot;MPIO&quot; &quot;COD_MUN&quot; ## [6] &quot;CANTON&quot; &quot;COD_CAN&quot; &quot;COD_ZON_CE&quot; &quot;COD_SEC_CE&quot; &quot;COD_SEG_CE&quot; ## [11] &quot;AREA_ID&quot; &quot;SEG_ID&quot; &quot;AREA_KM2&quot; &quot;SHAPE_Leng&quot; &quot;Shape_Le_1&quot; ## [16] &quot;Shape_Area&quot; &quot;demmean&quot; The data frame attached to the polygon map can be inspected using the following functions. head(segmento_sh@data) # provide the 5 first rows of the dataframe segmento_sh@data ## OBJECTID DEPTO COD_DEP MPIO COD_MUN CANTON COD_CAN ## 0 1 AHUACHAPAN 01 ATIQUIZAYA 03 AREA URBANA 00 ## 1 2 AHUACHAPAN 01 GUAYMANGO 06 EL ESCALON 04 ## 2 3 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 ## 3 4 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 ## 4 5 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 ## 5 6 AHUACHAPAN 01 AHUACHAPAN 01 AREA URBANA 00 ## COD_ZON_CE COD_SEC_CE COD_SEG_CE AREA_ID SEG_ID AREA_KM2 SHAPE_Leng ## 0 00 002 1015 U 01031015 0.04352 1261.0632 ## 1 00 005 0016 R 01060016 1.65778 8536.1704 ## 2 01 003 1015 U 01011015 0.07500 1165.9750 ## 3 01 003 1014 U 01011014 0.05010 988.8543 ## 4 02 009 1019 U 01011019 0.06910 1135.7582 ## 5 02 009 1042 U 01011042 0.04690 857.8403 ## Shape_Le_1 Shape_Area demmean ## 0 1261.0217 43526.87 NA ## 1 8535.8890 1657675.21 NA ## 2 1165.9366 74995.98 NA ## 3 988.8216 50104.03 NA ## 4 1135.7209 69104.02 NA ## 5 857.8122 46899.53 NA summary(segmento_sh@data) # summarise each field of the dataframe segmento_sh@data ## OBJECTID DEPTO COD_DEP MPIO ## Min. : 1 SAN SALVADOR:3156 06 :3156 SAN SALVADOR: 662 ## 1st Qu.: 3110 LA LIBERTAD :1419 05 :1419 SAN MIGUEL : 494 ## Median : 6219 SANTA ANA :1124 02 :1124 SANTA ANA : 492 ## Mean : 6219 SAN MIGUEL : 962 12 : 962 SOYAPANGO : 476 ## 3rd Qu.: 9328 SONSONATE : 901 03 : 901 MEJICANOS : 274 ## Max. :12437 USULUTAN : 778 11 : 778 APOPA : 260 ## (Other) :4095 (Other):4095 (Other) :9777 ## COD_MUN CANTON COD_CAN COD_ZON_CE ## 17 :1085 AREA URBANA :4945 00 :4945 00 :4325 ## 10 : 922 SAN BARTOLO : 157 02 : 914 01 :2763 ## 14 : 864 VERACRUZ : 145 01 : 908 02 :1747 ## 02 : 836 LAS FLORES : 100 04 : 871 03 :1021 ## 08 : 809 SAN LUIS MARIONA: 90 03 : 813 04 : 763 ## 03 : 751 LAS DELICIAS : 84 06 : 673 05 : 504 ## (Other):7168 (Other) :6914 (Other):3311 (Other):1312 ## COD_SEC_CE COD_SEG_CE AREA_ID SEG_ID ## 002 :1066 1001 : 262 R:5088 01010001: 1 ## 003 : 996 0001 : 259 U:7347 01010002: 1 ## 001 : 877 0002 : 259 01010003: 1 ## 004 : 788 0003 : 255 01010004: 1 ## 005 : 696 0004 : 252 01010005: 1 ## 006 : 568 0005 : 247 01010006: 1 ## (Other):7444 (Other):10901 (Other) :12429 ## AREA_KM2 SHAPE_Leng Shape_Le_1 Shape_Area ## Min. : 0.00000 Min. : 281.4 Min. : 281.4 Min. : 3903 ## 1st Qu.: 0.06062 1st Qu.: 1256.3 1st Qu.: 1256.3 1st Qu.: 60567 ## Median : 0.27345 Median : 2859.6 Median : 2859.5 Median : 273177 ## Mean : 1.67100 Mean : 5494.0 Mean : 5493.8 Mean : 1672961 ## 3rd Qu.: 2.08214 3rd Qu.: 8492.5 3rd Qu.: 8492.3 3rd Qu.: 2082009 ## Max. :48.36518 Max. :54684.4 Max. :54682.6 Max. :48194678 ## ## demmean ## Min. : NA ## 1st Qu.: NA ## Median : NA ## Mean :NaN ## 3rd Qu.: NA ## Max. : NA ## NA&#39;s :12435 We see that Shape_Le_1 and SHAPE_Leng are duplicates, while demmean only comprises of null values. Let us now remove these fields using the dplyr function select. segmento_sh@data=segmento_sh@data%&gt;% select(-c(Shape_Le_1,demmean)) In order to get a country mask, i.e. the boundary of the country, we dissolve the multiples polygons of the departamentos shapefile in one overall map of the country with the function maptools::unionSpatialPolygons. SLV_adm0_proj=maptools::unionSpatialPolygons(departamentos_sh, rep(1, nrow(departamentos_sh))) The country mask is shown on figure ??. plot(SLV_adm0_proj) El Salvador country mask 9.3.1.1 Adjusting the coordinates system There are two families of coordinates systems: Geographic Coordinates Systems (GCS) : “A system that uses a three-dimensional spherical surface to define locations on the earth”5. Projected coordinates systems (PCS) : “A system defined on a flat, two-dimensional surface. Unlike a geographic coordinate system, a projected coordinate system has constant lengths, angles, and areas across the two dimensions”6 The shapefile for the segmentos, departmentos and the country mask have a projected coordinate system. SLV_adm0_proj@proj4string ## CRS arguments: ## +proj=lcc +lat_1=13.316666 +lat_2=14.25 +lat_0=13.783333 ## +lon_0=-89 +x_0=500000 +y_0=295809.184 +datum=NAD27 +units=m ## +no_defs +ellps=clrk66 ## +nadgrids=@conus,@alaska,@ntv2_0.gsb,@ntv1_can.dat The coordinates are expressed in meters: sp::bbox(SLV_adm0_proj) ## min max ## x 377579.6 642687.1 ## y 226506.1 369595.5 Since most spatial RS data are based on a geographic coordinates system, we create a second version of the country mask with a geographic coordinates system. This is done with the function spTransform from the sp library. The use of :: allows to call the function without laoding the the sp library in then environment. proj_WGS_84=&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; segmento_sh_wgs=sp::spTransform(segmento_sh, proj_WGS_84) SLV_adm0=sp::spTransform(SLV_adm0_proj, proj_WGS_84) sp::bbox(SLV_adm0) ## min max ## x -90.13196 -87.68389 ## y 13.15441 14.44997 The coordinates are now expressed in decimal degrees. 9.3.2 Loading the shapefiles containing the fields used as covariates We now load the shapefiles containing the fields used as covariates, i.e. containing variables that will be used as predictors in the model we will fit in the next sections. We start with the Livelihood zone maps accessed from the Famine Early Warning Systems Network (FEWS NET). LZ_sh=readOGR(paste0(dir_data, dir_shape, &quot;LZ/SV_LHZ_2010.shp&quot;)) It is mapped on ?? for inspection. leaflet(LZ_sh)%&gt;% addTiles()%&gt;% addPolygons(weight=1, color = &quot;#444444&quot;, smoothFactor = 1, fillOpacity = 1, fillColor = ~colorFactor(&quot;Accent&quot;, LZNAMEEN)(LZNAMEEN), popup = ~LZNAMEEN)%&gt;% addLegend(&quot;bottomright&quot;, colors = ~colorFactor(&quot;Accent&quot;, LZNAMEEN)(LZNAMEEN), labels = ~LZNAMEEN, opacity = 1) Livelihood zones Let us now load all the other vectors data. school_sh=readOGR(paste0(dir_data, dir_shape, &quot;schools/MINED.shp&quot;),use_iconv=TRUE, encoding = &quot;UTF-8&quot;) buildings_poly_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_buildings_polygons.shp&quot;), use_iconv=TRUE) poi_points_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_points_of_interest_points.shp&quot;)) poi_poly_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_points_of_interest_polygons.shp&quot;)) roads_lines_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_roads_lines.shp&quot;)) roads_poly_sh=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_roads_polygons.shp&quot;)) coastline=readOGR(paste0(dir_data, dir_shape, &quot;coastline/coastline.shp&quot;)) waterbodies=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_waterways_polygons.shp&quot;)) proj_lcc=&quot; +proj=lcc +lat_1=13.316666 +lat_2=14.25 +lat_0=13.783333 +lon_0=-89 +x_0=500000 +y_0=295809.184 +datum=NAD27 +units=m +no_defs +ellps=clrk66 +nadgrids=@conus,@alaska,@ntv2_0.gsb,@ntv1_can.dat&quot; waterbodies_proj=sp::spTransform(waterbodies, proj_lcc) rivers=readOGR(paste0(dir_data, dir_shape, &quot;hotosm/hotosm_slv_waterways_lines.shp&quot;)) rivers_proj=sp::spTransform(rivers, proj_lcc) 9.3.3 Loading point-referenced data from tables Two datasets are provided as R data frames in a .Rda file. They are loaded with the function load: load(paste0(dir_data, dir_shape, &quot;hospitales/hospitales.Rda&quot;)) load(paste0(dir_data, dir_shape, &quot;UCSF/UCSF.Rda&quot;)) The package sp is then used to to turn these data frames into SpatialPointsDataframe objects: # in the .Rda,there is an error in the naming of the coordinates: latitudes are actually longitudes and vice versa hospitales=hospitales%&gt;% rename(lon=latitude, lat=longitude) hospitals_sp=as.data.frame(hospitales) # Features &quot;lon&quot; and &quot;lat&quot; are declared the the spatial coordinates of the data frame &quot;hospitals_sp&quot; # The data frame &quot;hospitals_sp&quot; becomes am object of the type SpatialPointsDataframe sp::coordinates(hospitals_sp)=~lon+lat # A spatial coordinate system is assigned to the SpatialPointsDataframe &quot;hospitals_sp&quot; sp::proj4string(hospitals_sp)=sp::CRS(proj_WGS_84) The same is done with the health center (not shown). The resulting map is plotted on ?? for reference7. leaflet(SLV_adm0)%&gt;% addPolygons()%&gt;% addTiles()%&gt;% addMarkers(lng=coordinates(hospitals_sp)[,1], lat=coordinates(hospitals_sp)[,2], popup=hospitals_sp$name)%&gt;% addCircleMarkers(lng=coordinates(health_centres_sp)[,1], lat=coordinates(health_centres_sp)[,2], radius = 4, popup=paste0(health_centres_sp$name,&quot;\\n(&quot;,health_centres_sp$tipo,&quot;)&quot;), stroke = F, fillOpacity = 1, fillColor = colorFactor(&quot;Accent&quot;, health_centres_sp$tipo)(health_centres_sp$tipo)) 9.4 Loading the rasters We now turn to the raster files. Raster files can be described as geo-referenced pixel data. For instance, a raster of population density consists of a map of pixels. The value of each pixel is the people density in this pixel. raster data are provided in two main formats: zipped .tif and .ncdffiles. 9.4.1 Lights at night Lights at night data products are available both per calendar month and per year. The annual data are used for 1983 to 2016. For 2017, the yearly aggregate were not yet released at the time of data pre-processing. Monthly aggregated were hence used. The first step is to unzip the files with the untar command. For the sake of convenience, data are already provided as extracted .tif in the data folder data/spatial/raster/l@n/. Hence, the code snippet below is just shown for learning purposes, no need to run it. #get the list of all files in the l@n directory with the command -dir-, which takes as argument # the path to the directory. list_of_files=dir(paste0(dir_data, &quot;spatial/raster/l@n/&quot;)) #identify the index of the 2017 monthly files index_of_2017_files=grep(&quot;npp_2017&quot;, dir(paste0(dir_data, &quot;spatial/raster/l@n/&quot;))) # select in the list of all file the 2017 monthly files thanks to the index files_2017=list_of_files[index_of_2017_files] for(monthly_file in files_2017){ untar(paste0(dir_data, &quot;spatial/raster/l@n/&quot;, monthly_file), # the path to the file of interest compressed=&quot;gzip&quot;, # specify it was zipped with gzip exdir=paste0(dir_data, &quot;spatial/raster/l@n/&quot;)) # specify the exdir } The 12 monthly lights at night .tif files are loaded with the raster::raster function, cropped to the El Salvador’s boundaries with raster::crop and stacked in one object withraster::stack. Note the use of the for loop function to loop over the 12 monthly files. # we get the list of the files in the directory list_of_files=dir(paste0(dir_data, &quot;spatial/raster/l@n/&quot;)) # we get the list of lights at night files labelled &quot;avg_rade9h&quot; monthly_file_light=list_of_files[grepl(&quot;avg_rade9h&quot;, list_of_files, fixed=T)] # create a vector of character to name the raster as file_1, file_2, etc. names_file=paste0(&quot;file_&quot;,1:12) # we loop over the 12 monthly files in order to for(i in 1:length(monthly_file_light)){ # load each i monthly layer r=raster::raster(paste0(dir_data, &quot;spatial/raster/l@n/&quot;, monthly_file_light[i])) # crop each i monthly layer r=raster::crop(r, SLV_adm0) # assign each i cropped monthly layer to a name assign(names_file[i], # assign it to an object name r) } # stack all the layers lights_all=raster::stack(lapply(names_file,FUN=get)) Let us have a look at the object lights_all: class(lights_all)[1] ## [1] &quot;RasterStack&quot; dim(lights_all) ## [1] 310 587 12 raster::res(lights_all) ## [1] 0.004166667 0.004166667 raster::extent(lights_all) ## class : Extent ## xmin : -90.13125 ## xmax : -87.68542 ## ymin : 13.15625 ## ymax : 14.44792 raster::crs(lights_all) ## CRS arguments: ## +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 This a RasterStack object, i.e. a stack of raster where each layer is aligned one above the other. It has 310 rows, 587 columns, 12 layers (1 per month) for a total of 181,970 cells. The resolution is provided in decimal degrees: 0.004166667 degrees by 0.004166667 degrees pixels. Let us plot the 12 monthly layers for inspection. raster::plot(lights_all, main=c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;,&quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) Figure 9.1: 2017 monthly average lights at night (average DNB radiance) The maps in 9.1 show there are variations over the year. 9.4.2 Precipitation The CHIRPS precipitation data are provided as a .nc file. A .nc file is a collection of raster files, similar to a raster stack. Data are loaded as a brick thanks to the raster::brick function, similar to the raster stack function8. chirps=raster::brick(paste0(dir_data, &quot;spatial/raster/chirps/chirps-v2.0.annual.nc&quot;), band=1:38) print(chirps) ## File /home/rstudio/data/spatial/raster/chirps/chirps-v2.0.annual.nc (NC_FORMAT_NETCDF4): ## ## 1 variables (excluding dimension variables): ## float precip[longitude,latitude,time] (Chunking: [800,223,4]) (Compression: level 5) ## units: mm/year ## standard_name: convective precipitation rate ## long_name: Climate Hazards group InfraRed Precipitation with Stations ## time_step: year ## missing_value: -9999 ## _FillValue: -9999 ## geostatial_lat_min: -50 ## geostatial_lat_max: 50 ## geostatial_lon_min: -180 ## geostatial_lon_max: 180 ## ## 3 dimensions: ## longitude Size:7200 ## units: degrees_east ## standard_name: longitude ## long_name: longitude ## axis: X ## latitude Size:2000 ## units: degrees_north ## standard_name: latitude ## long_name: latitude ## axis: Y ## time Size:38 ## units: days since 1980-1-1 0:0:0 ## standard_name: time ## calendar: gregorian ## axis: T ## ## 15 global attributes: ## Conventions: CF-1.6 ## title: CHIRPS Version 2.0 ## history: created by Climate Hazards Group ## version: Version 2.0 ## date_created: 2019-01-30 ## creator_name: Pete Peterson ## creator_email: pete@geog.ucsb.edu ## institution: Climate Hazards Group. University of California at Santa Barbara ## documentation: http://pubs.usgs.gov/ds/832/ ## reference: Funk, C.C., Peterson, P.J., Landsfeld, M.F., Pedreros, D.H., Verdin, J.P., Rowland, J.D., Romero, B.E., Husak, G.J., Michaelsen, J.C., and Verdin, A.P., 2014, A quasi-global precipitation time series for drought monitoring: U.S. Geological Survey Data Series 832, 4 p., http://dx.doi.org/110.3133/ds832. ## comments: time variable denotes the first day of the given year. ## acknowledgements: The Climate Hazards Group InfraRed Precipitation with Stations development process was carried out through U.S. Geological Survey (USGS) cooperative agreement #G09AC000001 &quot;Monitoring and Forecasting Climate, Water and Land Use for Food Production in the Developing World&quot; with funding from: U.S. Agency for International Development Office of Food for Peace, award #AID-FFP-P-10-00002 for &quot;Famine Early Warning Systems Network Support,&quot; the National Aeronautics and Space Administration Applied Sciences Program, Decisions award #NN10AN26I for &quot;A Land Data Assimilation System for Famine Early Warning,&quot; SERVIR award #NNH12AU22I for &quot;A Long Time-Series Indicator of Agricultural Drought for the Greater Horn of Africa,&quot; The National Oceanic and Atmospheric Administration award NA11OAR4310151 for &quot;A Global Standardized Precipitation Index supporting the US Drought Portal and the Famine Early Warning System Network,&quot; and the USGS Land Change Science Program. ## ftp_url: ftp://chg-ftpout.geog.ucsb.edu/pub/org/chg/products/CHIRPS-latest/ ## website: http://chg.geog.ucsb.edu/data/chirps/index.html ## faq: http://chg-wiki.geog.ucsb.edu/wiki/CHIRPS_FAQ The raster chirps provides annual cumulative precipitation for the globe over 38 years (38 bands), from 1981 to 2018 at a resolution of 0.05 lon/lat decimal degree. Next, the data is cropped to the El Salvador’s boundaries and all years until 2017 are selected (data are provided up to Jan 2018 in the file): # crop the data chirps_ev=raster::crop(chirps, SLV_adm0) time = raster::getZ(chirps_ev) # get the time index index_of_1981_2017 = which(dplyr::between(time, as.Date(&quot;1981-01-01&quot;), as.Date(&quot;2017-01-01&quot;))) index_of_2017 = which(time==as.Date(&quot;2017-01-01&quot;)) # subset the CHIRPS on 1981-2017 (take out 2018) chirps_ev_1981_2017=raster::subset(chirps_ev, index_of_1981_2017) 9.4.3 Soil degradation The soil degradation data were obtained via the QGIS plugin tool “TRENDS.EARTH”9. Three main soil-degradation indexes are provided in the TRENDS.EARTH: Soil carbon stock degradation, Soil Land use change degradation, Soil Productivity degradation: trajectory performance state Data for each of the above are provided in a series of multi-layers .tif files. The .json file provides the information about each layer. soil_deg_carb_path=&quot;spatial/raster/soil_deg_carb/soil_deg_carb.json&quot; soil_deg_carb_json=RJSONIO::fromJSON(paste0(dir_data, soil_deg_carb_path)) grep(&quot;degradation&quot;, lapply(lapply(soil_deg_carb_json$bands, unlist), cbind)) ## [1] 1 soil_deg_luc_json_path=&quot;spatial/raster/soil_deg_luc/soil_deg_luc.json&quot; soil_deg_luc_json=RJSONIO::fromJSON(paste0(dir_data, soil_deg_luc_json_path)) lapply(lapply(soil_deg_luc_json$bands, unlist), cbind) ## [[1]] ## [,1] ## add_to_map &quot;TRUE&quot; ## metadata.year_baseline &quot;1992&quot; ## metadata.year_target &quot;2015&quot; ## name &quot;Land cover (degradation)&quot; ## no_data_value &quot;-32768&quot; ## ## [[2]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1992&quot; ## name &quot;Land cover (ESA classes)&quot; ## no_data_value &quot;-32768&quot; ## ## [[3]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2015&quot; ## name &quot;Land cover (ESA classes)&quot; ## no_data_value &quot;-32768&quot; ## ## [[4]] ## [,1] ## add_to_map &quot;TRUE&quot; ## metadata.year_baseline &quot;1992&quot; ## metadata.year_target &quot;2015&quot; ## name &quot;Land cover transitions&quot; ## no_data_value &quot;-32768&quot; ## ## [[5]] ## [,1] ## add_to_map &quot;TRUE&quot; ## metadata.year &quot;1992&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[6]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1993&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[7]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1994&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[8]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1995&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[9]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1996&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[10]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1997&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[11]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1998&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[12]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;1999&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[13]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2000&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[14]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2001&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[15]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2002&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[16]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2003&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[17]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2004&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[18]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2005&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[19]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2006&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[20]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2007&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[21]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2008&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[22]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2009&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[23]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2010&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[24]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2011&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[25]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2012&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[26]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2013&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[27]] ## [,1] ## add_to_map &quot;FALSE&quot; ## metadata.year &quot;2014&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; ## ## [[28]] ## [,1] ## add_to_map &quot;TRUE&quot; ## metadata.year &quot;2015&quot; ## name &quot;Land cover (7 class)&quot; ## no_data_value &quot;-32768&quot; grep(&quot;degradation&quot;, lapply(lapply(soil_deg_luc_json$bands, unlist), cbind)) ## [1] 1 soil_deg_prod_json_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.json&quot; soil_deg_prod_json=RJSONIO::fromJSON(paste0(dir_data, soil_deg_prod_json_path)) grep(&quot;degradation&quot;, lapply(lapply(soil_deg_prod_json$bands, unlist), cbind)) ## [1] 4 7 For the carbon and land use change files, Band 1 provides the soil degradation index. For the productivity file, the index values on the trajectory, performance and state are stored in band 2, 4 and 7. soil_deg_carb_path=&quot;spatial/raster/soil_deg_carb/soil_deg_carb.tif&quot; soil_deg_carb=raster::raster(paste0(dir_data, soil_deg_carb_path), band=1) # soil degradation, change in the carbon method soil_deg_luc_path=&quot;spatial/raster/soil_deg_luc/soil_deg_luc.tif&quot; soil_deg_luc=raster::raster(paste0(dir_data, soil_deg_luc_path), band=1) # soil degradation, land use change soil_deg_prod_trj_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.tif&quot; soil_deg_prod_trj=raster::raster(paste0(dir_data, soil_deg_prod_trj_path), band=2) # soil degradation, productivity trajectory deg soil_deg_prod_prf_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.tif&quot; soil_deg_prod_prf=raster::raster(paste0(dir_data, soil_deg_prod_prf_path), band=4) # soil degradation, productivity performance deg soil_deg_prod_stt_path=&quot;spatial/raster/soil_deg_prod/soil_deg_prod.tif&quot; soil_deg_prod_stt=raster::raster(paste0(dir_data, soil_deg_prod_stt_path), band=7) # soil degradation, productivity state deg 9.4.4 Vegetation greenness Vegetation greenness data, as measured by normalized difference vegetation index, are provided on a dekadal basis i.e. every 10 days. Two data products are available: The smoothed dekadal NDVI value for 2017 The median dekadal NDVI value over the period 2003-2017. The first step is to unzip the data with the following code snippet. For sake of convenience, the data are provided as unzipped .tif in the data pack, but the code to unzip the files is provided below. #get the list of all files in the ndvi directory with the command -dir- list_of_files=dir(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;)) # identify the index of the 2017 monthly files and norma index_of_2017_std_files=grep(&quot;17|stm&quot;, dir(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;))) # select in the list of all file the 2017 monthly files thanks to the index files_of_interest=list_of_files[index_of_2017_std_files] for(file in files_of_interest){ untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), # the path to the file of interest compressed=&quot;gzip&quot;, exdir=paste0(dir_data, &quot;spatial/raster/ndvi/&quot;)) } ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca01stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca01stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca02stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca02stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca03stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca03stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca04stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca04stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca05stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca05stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca06stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca06stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca07stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca07stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca08stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca08stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca09stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca09stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca10stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca10stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca11stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca11stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca12stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca12stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca13stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca13stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca14stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca14stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca15stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca15stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca16stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca16stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1701.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1701.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1702.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1702.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1703.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1703.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1704.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1704.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1705.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1705.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1706.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1706.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1707.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1707.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1708.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1708.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1709.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1709.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1710.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1710.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1711.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1711.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1712.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1712.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1713.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1713.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1714.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1714.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1715.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1715.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1716.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1716.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1717.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1717.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1718.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1718.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1719.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1719.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1720.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1720.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1721.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1721.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1722.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1722.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1723.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1723.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1724.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1724.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1725.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1725.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1726.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1726.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1727.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1727.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1728.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1728.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1729.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1729.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1730.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1730.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1731.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1731.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1732.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1732.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1733.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1733.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1734.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1734.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1735.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1735.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1736.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca1736.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca17stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca17stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca18stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca18stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca19stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca19stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca20stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca20stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca21stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca21stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca22stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca22stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca23stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca23stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca24stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca24stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca25stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca25stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca26stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca26stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca27stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca27stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca28stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca28stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca29stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca29stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca30stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca30stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca31stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca31stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca32stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca32stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca33stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca33stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca34stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca34stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca35stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca35stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca36stm.tfw&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## ca36stm.tif&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1701.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1702.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1703.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1704.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1705.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1706.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1707.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1708.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1709.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1710.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1711.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## cam1712.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm01.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm02.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm03.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm04.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm05.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm06.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm07.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm08.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm09.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm10.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm11.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 ## Warning in untar(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, file), compressed ## = &quot;gzip&quot;, : &#39;/bin/tar -xf &#39;/home/rstudio/data/spatial/raster/ndvi/ ## camstm12.zip&#39; -C &#39;~/data/spatial/raster/ndvi/&#39;&#39; returned error code 2 The data are then loaded. Note the use of the function grep and the regular expressions to flexibly select the appropriate set of files: &quot;ca17[[:digit:]]{2}.tif&quot; means: “ca17”, followed by a suite of 2 digits, followed by “.tif” -&gt; these are the 2017 files &quot;ca[[:digit:]]{2}stm.tif&quot; means: “ca”, followed by a suite of 2 digits, followed by “stm.tif” -&gt; these are the median files # create a vector of character to name the raster as ndvi_1, ndvi_2, etc. names_ndvi_17=paste0(&quot;ndvi_&quot;,1:36) names_ndvi_norm=paste0(&quot;ndvi_norm_&quot;,1:36) # List of the 36 dekadal file for 2017 thanks to the command -grep- and the regular expression list_of_files=dir(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;)) files_17=list_of_files[grep(&quot;ca17[[:digit:]]{2}.tif&quot;, list_of_files)] # and the list of 36 ones for the 2003-17 median files_norm=list_of_files[grep(&quot;ca[[:digit:]]{2}stm.tif&quot;, list_of_files)] # loop over the 36 dekadale 2017 and normal files in order to laod them and to crop them for(i in 1:length(files_17)){ # 1) the 2017 data: # load each i dekadale layer r=raster::raster(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, files_17[i])) # crop each i monthly layer r=raster::crop(r, SLV_adm0) # assign each i cropped monthly layer to a name assign(names_ndvi_17[i], # assign it to an object name r) # 2) the median data: # load each i dekadale layer r=raster::raster(paste0(dir_data, &quot;spatial/raster/ndvi/&quot;, files_norm[i])) # crop each i monthly layer r=raster::crop(r, SLV_adm0) # assign each i cropped monthly layer to a name assign(names_ndvi_norm[i], # assign it to an object name r) } # stack all the layers ndvi_17=raster::stack(lapply(names_ndvi_17, FUN=get)) ndvi_norm=raster::stack(lapply(names_ndvi_norm, FUN=get)) Rasters are then re-sampled in order to have both sets of rasters perfectly aligned: ndvi_17_re=raster::resample(ndvi_17, ndvi_norm) 9.4.5 Temperature The 12 monthly average temperature data are cropped and stacked. files_temp=dir(paste0(dir_data, &quot;spatial/raster/temperature/&quot;))[grep(&quot;wc2.0_30s_tavg_&quot;, dir(paste0(dir_data, &quot;spatial/raster/temperature/&quot;)))] month=1 for(temp_m in files_temp){ temp=raster::raster(paste0(dir_data, &quot;spatial/raster/temperature/&quot;,temp_m)) temp=raster::crop(temp, SLV_adm0) assign(paste0(&quot;temp_&quot;,month), temp) month=month+1 } temp_months=raster::stack(lapply(paste0(&quot;temp_&quot;, 1:12), FUN=get)) 9.4.6 Slope The slope data are obtained from the WorldPop dataverse as a suite of raster files, each covering a portion of the El Salvador territory. They are combined in one single raster to ease pre-processing work. slope_files=paste0(dir_data, &quot;spatial/raster/slope/&quot;, dir(paste0(dir_data, &quot;spatial/raster/slope/&quot;))) i=1 raster_list=list() for(file in slope_files){ raster_f=raster::raster(file) raster_list[[i]]=raster_f i=i+1 } slope &lt;- do.call(raster::merge, raster_list) slope=raster::crop(slope, SLV_adm0) raster::plot(slope, main=&quot;Slope (degrees)&quot;) sp::plot(SLV_adm0, add=T) 9.4.7 Other rasters The other rasters are loaded with the function raster: dem=raster::raster(paste0(dir_data, &quot;spatial/raster/dem/srtm_dem.tif&quot;)) dist2road_path=&quot;spatial/raster/distance2roads/slv_osm_dst_road_100m_2016.tif&quot; dist2road=raster::raster(paste0(dir_data, dist2road_path)) dist2roadInter_path=&quot;spatial/raster/distance2intersections/slv_osm_dst_roadintersec_100m_2016.tif&quot; dist2roadInter=raster::raster(paste0(dir_data, dist2roadInter_path)) settlements_path=&quot;spatial/raster/settlements/slv_bsgme_v0a_100m_2017.tif&quot; settlements=raster::raster(paste0(dir_data, settlements_path)) dist2water_path=&quot;spatial/raster/distance2waterway/slv_osm_dst_waterway_100m_2016.tif&quot; dist2water=raster::raster(paste0(dir_data, dist2water_path)) gpw_path=&quot;spatial/raster/GPW/gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2015_30_sec.tif&quot; gpw=raster::raster(paste0(dir_data,gpw_path)) # 2015 population count gpw_es=raster::crop(gpw, SLV_adm0) lc_path=&quot;spatial/raster/CCILC/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7_sv.tif&quot; lc=raster::raster(paste0(dir_data,lc_path)) # land class We can plot them for quality controls. Fig ?? shows the the digital elevation model, i.e. the altitude across the country. raster::plot(dem) sp::plot(SLV_adm0, add=T) Digital Elevation Model Fig ?? shows the population density data. When examining the GPW data, one can identify some geometric forms corresponding to the enumeration areas of the census. raster::plot(gpw_es) sp::plot(SLV_adm0, add=T) Population density data (GPW) 9.5 Rasters calculation Lights at night is expected to be a good predictor of economic activity (Henderson, Storeygard, and Weil 2012). However, it is a priori not clear which summary statistics will best predict the three development indicators. For example, we cannot establish with certainty whether annual average lights at night or annual minimum average monthly lights at night is a better predictor of poverty. This holds true for the other raster data as well. 9.5.1 Lights at night The median and standard deviation of the lights at night data is computed for each map pixel: lights_med=raster::calc(lights_all, fun = median, na.rm = T) lights_sd=raster::calc(lights_all, fun = sd, na.rm = T) lights_summaries=raster::stack(lights_med, lights_sd) names(lights_summaries)=c(&quot;lights_med&quot;, &quot;lights_sd&quot;) Results are plotted on Fig ??. raster::plot(lights_summaries, main=c(&quot;Median&quot;, &quot;Standard Deviation&quot;)) sp::plot(SLV_adm0, add=T) 9.5.2 Precipitation Again, as it is not clear a priori which summary statistics of precipitation is best at predicting the development indicators, a series of summary statistics is computed for each pixel of the map: Two precipitation summaries related to short-term weather: the 2017 annual cumulative precipitation the 2017 annual cumulative precipitation as percentage of normal Two precipitation summaries related to climate: the median annual cumulative precipitation over the 38 years period the standard deviation in precipitation between years # compute summaries chirps_ev_2017=raster::subset(chirps_ev, index_of_2017) # 2017 chirps_ev_med=raster::calc(chirps_ev_1981_2017, fun = median, na.rm = T) #medi chirps_summaries=raster::stack(chirps_ev_2017, chirps_ev_med) names(chirps_summaries)=c(&quot;chirps_ev_2017&quot;, &quot;chirps_ev_med&quot;) For quality control, let us plot the various summaries. raster::plot(chirps_summaries, main=c(&quot;2017 precipitation&quot;, &quot;2017 precipitation as percentage of normal&quot;, &quot;1981-2017 median&quot;, &quot;1981-2017 standard deviation&quot;)) 9.5.3 Vegetation greenness Various statistics are computed on the dekadal NDVI and its deviation from the 2003-17 normal (median pixel value over the period). ndvi_17_perc=(ndvi_17_re/ndvi_norm)*100 # annual NDVI values ndvi_17_sum=raster::calc(ndvi_17_re, fun = sum, na.rm = T) ndvi_17_summaries=raster::stack(ndvi_17_sum) names(ndvi_17_summaries)=c(&quot;ndvi_17_sum&quot;) # NDVI as a percentage of the 2003-17 norm ndvi_17_perc_sum=raster::calc(ndvi_17_perc, fun = sum, na.rm = T) ndvi_17_perc_summaries=raster::stack(ndvi_17_perc_sum) names(ndvi_17_perc_summaries)=c(&quot;ndvi_17_perc_sum&quot;) 9.5.4 Soil degradation The missing data are stored in the raw rasters as -32768. They are turned into NA. Next, the soil degradation data are simplified by creating 1 binary raster for the three soil degradation proxies: the soil is degraded or not. # Soil carbon stock degradation # replace the NA soil_deg_carb_val=raster::getValues(soil_deg_carb) index_na=which(soil_deg_carb_val==-32768) soil_deg_carb_val[index_na]=NA #set to NA the cell identified with original value of -32768 # and binarise for degradation index_deg=which(soil_deg_carb_val&lt;0) soil_deg_carb_deg=soil_deg_carb_val soil_deg_carb_deg[index_deg]=1 soil_deg_carb_deg[-index_deg]=0 soil_deg_carb_deg[index_na]=NA soil_deg_carb_deg=raster::setValues(soil_deg_carb, soil_deg_carb_deg) As the code is quite lengthy, it is only shown for the carbon stock soil degradation. # Soil Land use change degradation soil_deg_luc_val=raster::getValues(soil_deg_luc) index_deg=which(soil_deg_luc_val&lt;0) soil_deg_luc_deg=soil_deg_luc_val soil_deg_luc_deg[index_deg]=1 soil_deg_luc_deg[-index_deg]=0 soil_deg_luc_deg=raster::setValues(soil_deg_luc, soil_deg_luc_deg) # Soil Productivity degradation # trajectory soil_deg_prod_trj_val=raster::getValues(soil_deg_prod_trj) index_na=which(soil_deg_prod_trj_val==-32768) index_deg=which(soil_deg_prod_trj_val&lt;0) soil_deg_prod_trj_val[index_na]=NA soil_deg_prod_trj_deg=soil_deg_prod_trj_val soil_deg_prod_trj_deg[index_deg]=1 soil_deg_prod_trj_deg[-index_deg]=0 soil_deg_prod_trj_deg[index_na]=NA soil_deg_prod_trj_deg=raster::setValues(soil_deg_prod_trj, soil_deg_prod_trj_deg) # performance soil_deg_prod_prf_val=raster::getValues(soil_deg_prod_prf) index_na=which(soil_deg_prod_prf_val==-32768) index_deg=which(soil_deg_prod_prf_val&lt;0) soil_deg_prod_prf_val[index_na]=NA soil_deg_prod_prf_deg=soil_deg_prod_prf_val soil_deg_prod_prf_deg[index_deg]=1 soil_deg_prod_prf_deg[-index_deg]=0 soil_deg_prod_prf_deg[index_na]=NA soil_deg_prod_prf_deg=raster::setValues(soil_deg_prod_prf, soil_deg_prod_prf_deg) # state soil_deg_prod_stt_val=raster::getValues(soil_deg_prod_stt) index_na=which(soil_deg_prod_stt_val==-32768) index_deg=which(soil_deg_prod_stt_val&lt;0) soil_deg_prod_stt_val[index_na]=NA soil_deg_prod_stt_deg=soil_deg_prod_stt_val soil_deg_prod_stt_deg[index_deg]=1 soil_deg_prod_stt_deg[-index_deg]=0 soil_deg_prod_stt_deg[index_na]=NA soil_deg_prod_stt_deg=raster::setValues(soil_deg_prod_stt, soil_deg_prod_stt_deg) Finally, the SDG indicator (15.3) for soil degradation indicator is computed. It is based on the one-out-all-out principle i.e. if a pixel is considered as soil degraded in any of the three soil degradation layers, then it is considered as degraded in the SDG indicator. The first step consists of aligning the raster, i.e. making sure that the pixels of each layer are perfectly overlaid. # check the rasters&#39; extents are the same all(raster::extent(soil_deg_luc_deg)==raster::extent(soil_deg_prod_stt_deg), raster::extent(soil_deg_carb_deg)==raster::extent(soil_deg_prod_stt_deg)) ## [1] FALSE As some of the layers have a slightly different extent, the values of each layer is re-sampled in the same raster grid. # resample the raster &quot;soil_deg_luc_deg&quot; to the same resolution and extent than the raster &quot;soil_deg_prod_stt_deg&quot; soil_deg_luc_deg_re=raster::resample(soil_deg_luc_deg, soil_deg_prod_stt_deg) # we need to round the value as the resampling takes averages whil we want 0/1 soil_deg_luc_deg_re_val=raster::getValues(soil_deg_luc_deg_re) soil_deg_luc_deg_re_val_r=round(soil_deg_luc_deg_re_val) soil_deg_luc_deg_re_r=raster::setValues(soil_deg_luc_deg_re, soil_deg_luc_deg_re_val_r) raster::plot(soil_deg_luc_deg_re_r) sp::plot(SLV_adm0, add=T) The same is done with the raster soil_deg_carb_deg (not shown). The resulting rasters are stacked and the SDG index computed. # stack the raster #### soil_deg=raster::stack(soil_deg_carb_deg_re_r, soil_deg_luc_deg_re_r, soil_deg_prod_stt_deg, soil_deg_prod_prf_deg, soil_deg_prod_trj_deg) names(soil_deg)=c(&quot;soil_carb&quot;, &quot;soil_luc&quot;, &quot;soil_prod_state&quot;, &quot;soil_prod_prf&quot;, &quot;soil_prod_trj&quot;) # SDG 15.3.1 soil degradation index (one out all out principle) #### soil_deg_SDG_index=raster::calc(soil_deg, sum, na.rm=T) soil_deg_SDG_index=raster::calc(soil_deg_SDG_index, fun=function(x) {x[x&gt;0] = 1; return(x)} ) And here is the map of soil degradation according to the SDG indicator 15.3: raster::plot(soil_deg_SDG_index) sp::plot(SLV_adm0, add=T) Figure 9.2: Soil Degradation, SDG indicator 15.3 9.5.5 Temperature Let us calculate the median and standard deviation of temperature variations for the year 2017. temp_median=raster::calc(temp_months, fun = median, na.rm = T) temp_summaries=raster::stack(temp_median) names(temp_summaries)=c(&quot;temp_median&quot;) raster::plot(temp_median) sp::plot(SLV_adm0, add=T) 9.6 Distance calculation Distance to public services and businesses may be a good indicator of poverty and other development indicators: remote areas tend to be poorer and face lower level of literacy. Furthermore, the distance to some natural features, such as distance to the coast or distance to forested areas may be correlated with local economic development. We compute below the distance to: Public services: hospitals (source: IDB) health centres (source: IDB) private, public and schools in general (source: IDB), government offices (education and health excluded) (source: OSM) any of the above Businessess: Banks or ATMs (source: OSM) any businesses, Bank or ATM included (source: OSM) Natural features: water bodies (land class from ESA) cropland (land class from ESA) forested areas (land class from ESA) urban areas (land class from ESA) 9.6.1 Distance to public services The first step is to create a raster which is used as a canvas to compute distances. Distances will be computed from each raster cell centre to the closest point of interest (e.g. the closest school). In order to have a relatively good precision of the distance estimates, we will create a raster with a resolution of 100 metres. Note the use of the projected coordinates system from the segmento_sh shapefile. # create a raster for the computation of distances raster_dist=raster::raster(raster::extent(segmento_sh), crs=segmento_sh@proj4string, res=c(100,100)) print(segmento_sh@proj4string) ## CRS arguments: ## +proj=lcc +lat_1=13.316666 +lat_2=14.25 +lat_0=13.783333 ## +lon_0=-89 +x_0=500000 +y_0=295809.184 +datum=NAD27 +units=m ## +no_defs +ellps=clrk66 ## +nadgrids=@conus,@alaska,@ntv2_0.gsb,@ntv1_can.dat We transform the CRS of the hospitals_sp shapefile to the corresponding projected CRS: # Project the hospital shapefile #### hospitals_proj=sp::spTransform(hospitals_sp, segmento_sh@proj4string) We can now compute the distance from each cell centre of the raster, raster_dist, to the projected shapefile of hospitals hospitals_proj with the function raster::distanceFromPoints10. dist2hosp=raster::distanceFromPoints(raster_dist, hospitals_proj) The distance map is plotted for inspection: raster::plot(dist2hosp) sp::plot(SLV_adm0_proj, add=T) The colorscale indicates the distance to hospitals. The same can be repeated for the other public services (not shown). For the OSM data, 2 additional steps are required: The data is filtered to select only the public amenities defined as: Town hall Post office Court house Police station Prison Fire station Community centre Public building (a loose OSM definition) The polygon data is rasterized to allow for distance calculation with the function raster::distance a buffer of 25 metres needs to be included as the function raster::rasterize considers a polygon to cross a cell only if it covers the cell centre the function raster::distance computes the distance from all pixels with a NA value to the nearest pixel which is not NA. # distance to public amenities (health and educ excluded) #### amenities=c(&quot;townhall&quot;,&quot;post_office&quot;,&quot;courthouse&quot;,&quot;police&quot;, &quot;prison&quot;,&quot;public_building&quot;,&quot;fire_station&quot;,&quot;community_centre&quot;) # poly poi_poly_pubserv=as(subset(poi_poly_sh, amenity%in%amenities), &quot;SpatialPolygons&quot;) poi_poly_pubserv_proj=sp::spTransform(poi_poly_pubserv, segmento_sh@proj4string) poi_poly_pubserv_proj_buffer=rgeos::gBuffer(poi_poly_pubserv_proj,width=50) poly_pubserv_proj_r=raster::rasterize(poi_poly_pubserv_proj_buffer, y=raster_dist) dist2poly_pubserv=raster::distance(poly_pubserv_proj_r) # points poi_points_pubserv=as(subset(poi_points_sh, amenity%in%amenities), &quot;SpatialPoints&quot;) poi_points_pubserv_proj=sp::spTransform(poi_points_pubserv, segmento_sh@proj4string) dist2points_pubserv=raster::distanceFromPoints(raster_dist, poi_points_pubserv_proj) dist2pubamen=min(dist2points_pubserv, dist2poly_pubserv) Lastly, we compute the distance to any public services for each pixel taking the minimum distance of the 4 distance maps: # distance to all public services ##### dist2allpubserv=min(dist2pubamen, dist2schools, dist2health, dist2hosp) 9.6.2 Distance to businesses The same approach is adopted for the distance to businesses. We start with the distance to financial services access points defined as banks and ATM. # distance to FAPS #### amenities=c(&quot;bank&quot;,&quot;atm&quot;) poi_poly_fin=as(subset(poi_poly_sh, amenity%in%amenities), &quot;SpatialPolygons&quot;) poi_poly_fin_proj=sp::spTransform(poi_poly_fin, segmento_sh@proj4string) poi_poly_fin_proj_buffer=rgeos::gBuffer(poi_poly_fin_proj,width=50) poly_fin_proj_r=raster::rasterize(poi_poly_fin_proj_buffer, y=raster_dist) dist2poly_fin=raster::distance(poly_fin_proj_r) poi_points_fin=as(subset(poi_points_sh, amenity%in%amenities), &quot;SpatialPoints&quot;) poi_points_fin_proj=sp::spTransform(poi_points_fin, segmento_sh@proj4string) dist2points_fin=raster::distanceFromPoints(raster_dist, poi_points_fin_proj) dist2fin=min(dist2points_fin, dist2poly_fin) We proceed with the other businesses. These are the OpenStreetMap businesses categories considered on top of banks and ATM: * Market place * Food court * Restaurant * Fast food * Cafe * Bar * Ice cream shop * Pharmacy * Internet cafe * Cinema * Fuel # businesses in amenities amenities=c(&quot;marketplace&quot;,&quot;pharmacy&quot;,&quot;restaurant&quot;,&quot;fast_food&quot;,&quot;cafe&quot;,&quot;bar&quot;,&quot;ice_cream&quot;,&quot;food_court&quot;, &quot;internet_cafe&quot;, &quot;cinema&quot;,&quot;fuel&quot;,&quot;atm&quot;,&quot;bank&quot;) # poly poi_poly_eco=as(subset(poi_poly_sh, amenity%in%amenities), &quot;SpatialPolygons&quot;) poi_poly_eco_proj=sp::spTransform(poi_poly_eco, segmento_sh@proj4string) poi_poly_eco_proj_buffer=rgeos::gBuffer(poi_poly_eco_proj,width=50) poly_eco_proj_r=raster::rasterize(poi_poly_eco_proj_buffer, y=raster_dist) dist2poly_eco=raster::distance(poly_eco_proj_r) # points poi_points_eco=as(subset(poi_points_sh, amenity%in%amenities), &quot;SpatialPoints&quot;) poi_points_eco_proj=sp::spTransform(poi_points_eco, segmento_sh@proj4string) dist2points_eco=raster::distanceFromPoints(raster_dist, poi_points_eco_proj) In addition to being listed in the amenity field of the OSM data, shop locations are also tagged separately. Given that shops are also businesses, they are taken into account in the distance calculations. # businesses as shop # poly poi_poly_shop=as(subset(poi_poly_sh, is.na(shop)==F), &quot;SpatialPolygons&quot;) poi_poly_shop_proj=sp::spTransform(poi_poly_shop, segmento_sh@proj4string) poi_poly_shop_proj_buffer=rgeos::gBuffer(poi_poly_shop_proj,width=50) poly_shop_proj_r=raster::rasterize(poi_poly_shop_proj_buffer, y=raster_dist) dist2poly_shop=raster::distance(poly_shop_proj_r) # points poi_points_shop=as(subset(poi_points_sh, is.na(shop)==F), &quot;SpatialPoints&quot;) poi_points_shop_proj=sp::spTransform(poi_points_shop, segmento_sh@proj4string) dist2points_shop=raster::distanceFromPoints(raster_dist, poi_points_shop_proj) Finally, we gather all business-specific distance maps into one general Distance to a businesses map. # collect all businesses dist2biz=min(dist2points_shop, dist2poly_shop, dist2points_eco, dist2poly_eco) 9.6.3 Distance to natural features Next we compute the distance to natural features such as the coastline, urban areas, forested areas and croplands. We start by cropping the world coastline to El-Salvadorian boundaries. Once this operation is completed, we project the map and create the necessary buffer. # distance to coastline coastline_c=raster::crop(coastline, sp::bbox(SLV_adm0)) coastline_proj=sp::spTransform(coastline_c, segmento_sh@proj4string) coastline_proj_buffer=rgeos::gBuffer(coastline_proj,width=50) coastline_proj_r=raster::rasterize(coastline_proj_buffer, y=raster_dist) dist2coast=raster::distance(coastline_proj_r) For the distance to the urban areas, forested areas, the croplands, and water bodies we use to NA pixels which are not urban, tree cover, cropland or water bodies, respectively. We then we use the function raster::distance which computes the distance from each NA pixel to the closest non NA pixel. # distance to urban area #### lc_urban=lc lc_urban=raster::calc(lc_urban, fun=function(x) {ifelse(x==190,1,NA)}) lc_urban_proj=raster::projectRaster(lc_urban, crs=segmento_sh@proj4string) dist2urban=raster::distance(lc_urban_proj) # distance to forest #### lc_tree=lc lc_tree=raster::calc(lc_tree, fun=function(x) {ifelse(x%in%c(50,60),1,NA)}) lc_tree_proj=raster::projectRaster(lc_tree, crs=segmento_sh@proj4string) dist2tree=raster::distance(lc_tree_proj) # distance to crop #### lc_crop=lc lc_crop=raster::calc(lc_crop, fun=function(x) {ifelse(x%in%c(10,20,30,40),1,NA)}) lc_crop_proj=raster::projectRaster(lc_crop, crs=segmento_sh@proj4string) dist2crop=raster::distance(lc_crop_proj) # distance to water #### lc_water=lc lc_water=raster::calc(lc_water, fun=function(x) {ifelse(x%in%c(210),1,NA)}) lc_water_proj=raster::projectRaster(lc_water, crs=segmento_sh@proj4string) dist2water=raster::distance(lc_water_proj) 9.6.4 Stack distance maps and inspect results Once the extent and resolution have been aligned with the function raster::resample, the distance maps are stacked with the function raster::stack. dist2coast_r=raster::resample(dist2coast, dist2fin) dist2water_r=raster::resample(dist2water, dist2fin) dist2crop_r=raster::resample(dist2crop, dist2fin) dist2tree_r=raster::resample(dist2tree, dist2fin) dist2urban_r=raster::resample(dist2urban, dist2fin) dist_stack=raster::stack(dist2schools_priv, # dist2schools_pub, dist2schools, dist2health, dist2hosp, dist2pubamen, dist2allpubserv, dist2biz, dist2fin, dist2coast_r, dist2water_r, dist2crop_r, dist2tree_r, dist2urban_r) names(dist_stack)=c(&quot;dist2schools_priv&quot;, # &quot;dist2schools_pub&quot;, &quot;dist2schools&quot;, &quot;dist2health&quot;, &quot;dist2hosp&quot;, &quot;dist2pubamen&quot;, &quot;dist2allpubserv&quot;, &quot;dist2biz&quot;, &quot;dist2fin&quot;, &quot;dist2coast_r&quot;, &quot;dist2water_r&quot;, &quot;dist2crop_r&quot;, &quot;dist2tree_r&quot;, &quot;dist2urban_r&quot;) We can now examine the results. The next maps show the distance financial access points, businesses, private schools, public schools, (only the code for distance to financial services access points is shown). SLV_adm0_proj=sp::spTransform(SLV_adm0, segmento_sh@proj4string) raster::plot(dist_stack$dist2fin, main=&quot;Distance to financial access points, source: OSM&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Distance to financial access points raster::plot(dist_stack$dist2biz, main=&quot;Distance to businesses, source: OSM&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Distance to financial access points raster::plot(dist_stack$dist2schools_priv, main=&quot;Distance to private school, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) # raster::plot(dist_stack$dist2schools_pub, main=&quot;Distance to public schools, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Distance to financial access points raster::plot(dist_stack$dist2urban_r, main=&quot;Distance to urban areas, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Distance to financial access points raster::plot(dist_stack$dist2hosp, main=&quot;Distance to hospital, source: IDB&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Distance to financial access points raster::plot(dist_stack$dist2pubamen, main=&quot;Distance to public amenities, source: OSM&quot;) sp::plot(SLV_adm0_proj,add=T) sp::plot(waterbodies_proj,col=&quot;blue&quot;,add=T) Distance to financial access points 9.7 Spatial statistics at segmento level The raster and vector layers built above need to be matched with the EHPM survey data for them to be used in the analysis. This can be done either by: identifying the centroid (i.e. the middle point) of each segmento and extracting the value of each covariate layer at the centroid of each segmento; computing the spatial average, minimum or maximum (or other) of each covariate for each segmento. Although the first approach is simpler to implement, the second one is preferred: data on poverty, income and literacy are available at the segmento level, i.e. these are area data rather than point-referenced data. Therefore, it is best to provide spatial summary statistics at the same level of spatial aggregation level rather than extracting data at the centroid of the segmento, an arbitrary location which might not be representative of the condition prevailing in the segmento. A prerequisite for merging to the data to is to have all data defined on the same coordinates reference system. 9.7.1 Adjusting the Coordinates Reference System for raster extaction to segmento In order to make sure all the pre-processed RS layers are set to this same CRS, the following process is adopted: a list of layer is created layer with a different CRS are identified layer with a different CRS are re-projected # create the list of rasters rasters_list=list(chirps_summaries, soil_deg,soil_deg_SDG_index, ndvi_17_summaries,ndvi_17_perc_summaries, dem, gpw_es, temp_summaries, dist2road, dist2roadInter, slope, settlements, dist2water, lights_summaries, lc, dist_stack) sum(unlist(lapply(rasters_list, raster::nlayers))) # number of layers ## [1] 34 names(rasters_list)=c(&quot;chirps_summaries&quot;, &quot;soil_deg&quot;,&quot;soil_deg_SDG_index&quot;, &quot;ndvi_17_summaries&quot;,&quot;ndvi_17_perc_summaries&quot;, &quot;dem&quot;, &quot;gpw_es&quot;, &quot;temp_summaries&quot;, &quot;dist2road&quot;, &quot;dist2roadInter&quot;, &quot;slope&quot;, &quot;settlements&quot;, &quot;dist2water&quot;, &quot;lights_summaries&quot;, &quot;lc&quot;, &quot;dist_stack&quot;) # identify the rasters with a different projection different_proj=which(lapply(rasters_list, FUN=function(x) sp::proj4string(x)==sp::proj4string(segmento_sh_wgs))==F) # reproject these raster to the same geographic coordinates system than Segmento rasters_list[different_proj]=lapply(rasters_list[different_proj], FUN=function(x) raster::projectRaster(x, crs=sp::proj4string(segmento_sh_wgs))) # turn the list items into object, i.e. replace the original raster objects with # the new raster objects having the correct coordinate system for(i in 1:length(rasters_list)) { assign(names(rasters_list)[i], rasters_list[[i]]) } The same is done for the list of vectors files (not shown), the only difference being that the function raster::projectRaster is replaced by the function sp::spTransform as a vector is used. 9.7.2 Rasters For each raster layer, we now need to get a summary statistics for each segmento area. We will start by getting segmento area average value for each raster layer11, except for the population where we will also get the \\(sum\\), i.e. total population count per segmento, and land class where we will get the percentage of segmento area for a subset of classes. The package velox rather than standard extract function from the raster package is used as it is much faster12. rasters_extraction_list=c(&quot;chirps_summaries&quot;,&quot;soil_deg&quot;,&quot;soil_deg_SDG_index&quot;,&quot;ndvi_17_summaries&quot;,&quot;ndvi_17_perc_summaries&quot;, &quot;dem&quot;,&quot;temp_summaries&quot;,&quot;dist2road&quot;,&quot;dist2roadInter&quot;,&quot;slope&quot;,&quot;settlements&quot;,&quot;dist2water&quot;,&quot;lights_summaries&quot;,&quot;dist_stack&quot;) seg_r_s=c() # create a empty vector to store the extracted data START_e=Sys.time() # record the start time of the loop for(r in rasters_extraction_list){ # loop over the list of rasters # create a velox object from the rastr r_velox=velox::velox(get(r)) # The function `get(x)` interpret the character *x* as an # object. In the code snippet below, it allows us to loop through the list of raster names. ex.mat &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) mean(x,na.rm=T), small=T) # get the mean seg_r_s=cbind(seg_r_s,ex.mat) # add the extracted data as a new column } end_e=Sys.time() print(end_e-START_e) ## Time difference of 3.008357 mins colnames(seg_r_s)=c(names(chirps_summaries), names(soil_deg), &quot;soil_deg_SDG_index&quot;, names(ndvi_17_summaries), names(ndvi_17_perc_summaries), &quot;dem&quot;,names(temp_summaries), &quot;dist2road&quot;,&quot;dist2roadInter&quot;,&quot;slope&quot;,&quot;settlements&quot;,&quot;dist2water&quot;, names(lights_summaries), names(dist_stack)) For the population raster, we get the \\(sum\\) per segmento: # pop r_velox=velox::velox(gpw_es) ex.mat_pop &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x,na.rm=T), small=T) # get the sum colnames(ex.mat_pop)=&quot;gpw_es_TOT&quot; For the land cover, we extract the share of each segmento area classified as: Urban areas: land class: 190 Tree covers area13: land class: 50, Tree cover broadleaved evergreen closed to open (&gt;15% of pixel area) land class: 60, Tree cover broadleaved deciduous closed to open (&gt;15% of pixel area) Cropland: land class: 10, Cropland rainfed land class: 20, Cropland irrigated or post-flooding land class: 30, Mosaic cropland (&gt;50% of pixel area) / natural vegetation (tree shrub herbaceous cover) (&lt;50% of pixel area) land class: 40, Mosaic natural vegetation (tree shrub herbaceous cover) (&gt;50% of pixel area) / cropland (&lt;50% of pixel area) # LC #### r_velox=velox::velox(lc) # urban LC ex.mat_urb &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x==190,na.rm=T)/(sum(x==190,na.rm=T)+sum(x!=190,na.rm=T)), small=T) # get the sum # Tree cover # 50;Tree cover broadleaved evergreen closed to open (&gt;15%);0;100;0 # 60;Tree cover broadleaved deciduous closed to open (&gt;15%);0;160;0 ex.mat_tree &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x%in%c(50,60, 100,110),na.rm=T)/(sum(x%in%c(50,60,100,110),na.rm=T)+sum(!(x%in%c(50,60,100,110)),na.rm=T)), small=T) # get the sum # crop land # 10;Cropland rainfed;255;255;100 # 20;Cropland irrigated or post-flooding;170;240;240 # 30;Mosaic cropland (&gt;50%) / natural vegetation (tree shrub herbaceous cover) (&lt;50%);220;240;100 # 40;Mosaic natural vegetation (tree shrub herbaceous cover) (&gt;50%) / cropland (&lt;50%) ;200;200;100 ex.mat_crop &lt;- r_velox$extract(segmento_sh_wgs, fun=function(x) sum(x%in%c(10,20,30,40),na.rm=T)/(sum(x%in%c(10,20,30,40),na.rm=T)+sum(!(x%in%c(10,20,30,40)),na.rm=T)), small=T) # get the sum colnames(ex.mat_urb)=&quot;lc_urban&quot; colnames(ex.mat_tree)=&quot;lc_tree&quot; colnames(ex.mat_crop)=&quot;lc_crop&quot; Lastly, the extracted data are added to a data frame before merging it with the segmento SpatialPolygonDataframe by binding its columns with the function cbind. seg_r=cbind(seg_r_s, ex.mat_pop, ex.mat_urb, ex.mat_tree, ex.mat_crop) # compile the extracted data seg_r_df=as.data.frame(seg_r) segmento_sh_wgs@data=cbind(segmento_sh_wgs@data, seg_r_df) # ogrDrivers() # writeOGR(segmento_sh_wgs, # paste0(dir_data, # &quot;spatial/shape/out/STPLAN_Segmentos_WGS84_wdata2.shp&quot;), # driver = &quot;ESRI Shapefile&quot;, # layer=1) # 9.7.2.1 Quality check We now check that the extraction worked well and did not produced any missing values. This could be caused notably by the raster layers not providing any value for small segmentos on islands. missing_r=apply(segmento_sh_wgs@data,2,FUN=function(x) which(is.na(x))) missing_r=unique(c(unlist(missing_r))) missing_SEG=segmento_sh_wgs%&gt;% subset(SEG_ID%in%segmento_sh_wgs$SEG_ID[missing_r]) missing_SEG_coords=sp::coordinates(missing_SEG) leaflet(SLV_adm0)%&gt;% addPolygons()%&gt;% addMarkers(missing_SEG_coords[,1],missing_SEG_coords[,2]) Figure 9.3: Observations with no data We observe on Fig. 9.3 that most missing values are in border areas. Two main reasons explain these NA: A lot of raster maps produced by WorldPop and HDX used boudaries produced by the open source Global Administrative Boundaries project. Unfortunately, the El Salvador boundaries in the the GADM are incorrect. Most pre-processed rasters are clipped in order to not overpass the boudaries, coastline and waterbodies. Some small segmentos at the boarder, on the coast or on the shore of Lago Ilopango are hence not covered. The raster with missing values are listed below. missing_r=apply(segmento_sh_wgs@data,2,FUN=function(x) which(is.na(x))) data.frame(n_miss=unlist(lapply(missing_r,length)), var_miss=names(missing_r))%&gt;% arrange(desc(n_miss))%&gt;% filter(n_miss&gt;0) ## n_miss var_miss ## 1 22 dist2road ## 2 22 dist2roadInter ## 3 22 settlements ## 4 17 temp_median ## 5 15 soil_prod_prf ## 6 6 soil_carb ## 7 1 chirps_ev_2017 ## 8 1 chirps_ev_med ## 9 1 soil_luc ## 10 1 soil_prod_state ## 11 1 soil_prod_trj ## 12 1 soil_deg_SDG_index ## 13 1 slope As their number is relatively small (a max of 22 missing values for 13 covariates), we decided to simply replace the missing value by the Canton average. For the one segmento where this did not work, we used the municipio average (code not shown). 9.7.3 Vectors For the vectors, a choice has to be made on the type of statistics which are desirable from each vector layer. Here are the spatial summaries statistics we will compute: Livelihood Zone Map: the Livelihood type of the segmento Schools: private and public schools, the sum of each category and their total is computed Health facilities: hospitals and three categories of health centers, the sum of each category and the sum of categories is computed OpenStreetMap point of interests, number of: buildings points of interest shops amenities classified in 12 categories and their sum OpenStreetMap roads, streets or path:s length of roads, streets or paths classified in 9 categories and their sum density of roads, streets or paths classified in 9 categories (length of roads/segmento area) share of rural and urban roads in total road, street or path of the segmento. 9.7.3.1 Livelihood zones As there is only on categorical value per area, the process is simple: the function over from the package sp is used to extract for each segmento the corresponding value of the segmento_sh_wgs’ SpatialPolygonDataFrame. The result of the function over is a data frame with one row per segmento of the segmento_sh_wgs. The Livelihood zone code (“LZCODE”) and Livelihood zone name (“LZNAMEEN”) is then added into the original data frame of segmento_sh_wgs. # extract for each segmento the corresponding value of the segmento_sh_wgs&#39;s SpatialPolygonDataFrame LZ_seg=sp::over(segmento_sh_wgs, LZ_sh) LZ_seg=LZ_seg[,c(&quot;LZCODE&quot;,&quot;LZNAMEEN&quot;)] # add the LZ data to segmento shape segmento_sh_wgs@data$LZCODE=LZ_seg$LZCODE segmento_sh_wgs@data$LZNAMEEN=LZ_seg$LZNAMEEN Checking whether there are any missing values, # check there are no NA summary(is.na(segmento_sh_wgs@data$LZNAMEEN)) ## Mode FALSE TRUE ## logical 12425 10 We see that there 10 segmentos with no assigned livelihood zones. They are mapped on Fig 9.4. missing_SEG=segmento_sh_wgs%&gt;% subset(SEG_ID%in%segmento_sh_wgs$SEG_ID[which(is.na(segmento_sh_wgs@data$LZNAMEEN))]) missing_SEG_coords=sp::coordinates(missing_SEG) leaflet(SLV_adm0)%&gt;% addPolygons()%&gt;% addMarkers(missing_SEG_coords[,1],missing_SEG_coords[,2]) Figure 9.4: Observations with no data All the missing values are in border areas: as the livelihood zone shapefile is not perfectly aligned to the segmento one, some segmentos fall outside the livelihood zone shapefile. A simple expedient is to replace it manually after a map inspection. missing_canton=which(is.na(segmento_sh_wgs@data$LZNAMEEN)&amp; segmento_sh_wgs@data$CANTON%in%c(&quot;AREA URBANA&quot;,&quot;GUISQUIL&quot;)) segmento_sh_wgs@data$LZNAMEEN[missing_canton]=&quot;Fishing, Aquaculture and Tourism Zone&quot; segmento_sh_wgs@data$LZCODE[missing_canton]=&quot;SV06&quot; missing_canton=which(is.na(segmento_sh_wgs@data$LZNAMEEN)&amp; segmento_sh_wgs@data$CANTON%in%c(&quot;ISLA ZACATILLO&quot;,&quot;ISLA MEANGUERITA&quot;)) segmento_sh_wgs@data$LZNAMEEN[missing_canton]=&quot;Basic Grain and Labor Zone&quot; segmento_sh_wgs@data$LZCODE[missing_canton]=&quot;SV01&quot; missing_canton=which(is.na(segmento_sh_wgs@data$LZNAMEEN)&amp; segmento_sh_wgs@data$CANTON==&quot;EL JOCOTILLO&quot;) segmento_sh_wgs@data$LZNAMEEN[missing_canton]=&quot;Sugarcane, Agro-Industry and Labor Zone&quot; segmento_sh_wgs@data$LZCODE[missing_canton]=&quot;SV03&quot; summary(is.na(segmento_sh_wgs@data$LZNAMEEN)) ## Mode FALSE ## logical 12435 summary(is.na(segmento_sh_wgs@data$LZCODE)) ## Mode FALSE ## logical 12435 9.7.3.2 Schools Extracting the schools’ and health facilities data follows the same principle. The difference is that the data are point data instead of polygon data. The schools and health facilities datasets are hence SpatialPointsDataFrame. The SpatialPointsDataFrame are first transformed into a SpatialPoints object thanks to the function as. The function over is then run, with an additional parameter specified: fn=sum. This tells the function over to compute the sum of SpatialPoints per segmento, i.e. the number of schools per segmento. For segmentos where there are no schools, the function over yields a NA value. NA values a replaced by zeros. # Schools # total sum of schools sch_seg=sp::over(segmento_sh_wgs, as(school_sh, &quot;SpatialPoints&quot;), fn=sum,na.rm=T) # replace the NA value sch_seg[which(is.na(sch_seg))]=0 The same is done for public and private schools separately. The function subset is used to subset only one category. # N public schools sch_pub_seg=sp::over(segmento_sh_wgs, as(subset(school_sh, Sector!=&quot;PRIVADO&quot;), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) sch_pub_seg[which(is.na(sch_pub_seg))]=0 # N private schools sch_pri_seg=sp::over(segmento_sh_wgs, as(subset(school_sh, Sector==&quot;PRIVADO&quot;), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) sch_pri_seg[which(is.na(sch_pri_seg))]=0 Lastly, the newly created covariates are added the to SpatialPolyongsDataFrame segmento_sh_wgs. # add the school data to segmento shape segmento_sh_wgs@data$sch=sch_seg segmento_sh_wgs@data$sch_pub=sch_pub_seg segmento_sh_wgs@data$sch_pri=sch_pri_seg The same process is adopted for the health facilities before adding the covariate to segmento_sh_wgs (not shown). 9.7.3.3 OpenStreetMap data The OpenStreetMap data are provided in 3 sets of files: buildings polygons (hotosm_slv_buildings_polygons.shp) points of interest points (hotosm_slv_points_of_interest_points.shp) points of interest polygons (hotosm_slv_points_of_interest_polygons.shp) For the buildings data, the sum of building per segmento is computed. This provides an indication of how urban a segmento is. build_seg=sp::over(segmento_sh_wgs, as(buildings_poly_sh, &quot;SpatialPolygons&quot;), fn=sum,na.rm=T) build_seg[which(is.na(build_seg))]=0 # add the building data to segmento shape segmento_sh_wgs@data$building=build_seg For the points of interests vectors (points and poly), the number of locations per segmento identified as a Shop in OSM, i.e. a place selling retail products or services, is calculated. This provides an indication of how urban a segmento is and it can also shed some light on the local economic dynamism. # shop shop_poly_seg=sp::over(segmento_sh_wgs, as(subset(poi_poly_sh, is.na(shop)==F), &quot;SpatialPolygons&quot;), fn=sum,na.rm=T) shop_poly_seg[which(is.na(shop_poly_seg))]=0 shop_points_seg=sp::over(segmento_sh_wgs, as(subset(poi_points_sh, is.na(shop)==F), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) shop_points_seg[which(is.na(shop_points_seg))]=0 shop_n=shop_points_seg+shop_poly_seg # add the shop data to segmento shape segmento_sh_wgs@data$shop=shop_n Second, the number of amenities according to 11 categories is computed. ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify ## format in kable. kableExtra can customize either HTML or LaTeX outputs. See ## https://haozhu233.github.io/kableExtra/ for details. Category Feature’s name in the data OSM amenity categories Financial access points fin bank, atm Marketplace mkt marketplace Bus Station bus bus_station Parking parking parking Place of Worship worship place_of_worship Health service health_srv pharmacy, clinic, dentist, doctor, hospital Food and drink food restaurant, fast_food, cafe, bar, ice_cream, food_court Culture and Education cult_educ school, college, university, community_centre, theatre, arts_centre, internet_cafe, cinema Fuel: petrol station fuel fuel Official buildings official townhall, post_office, courthouse, police, prison, public_building, fire_station The for loop below execute the extraction. for(i in 1: length(list_amenities)){ extracted_poly_seg=sp::over(segmento_sh_wgs, as(subset(poi_poly_sh, amenity%in%list_amenities[[i]]), &quot;SpatialPolygons&quot;), fn=sum,na.rm=T) extracted_poly_seg[which(is.na(extracted_poly_seg))]=0 extracted_point_seg=sp::over(segmento_sh_wgs, as(subset(poi_points_sh, amenity%in%list_amenities[[i]]), &quot;SpatialPoints&quot;), fn=sum,na.rm=T) extracted_point_seg[which(is.na(extracted_point_seg))]=0 extracted_seg=extracted_point_seg+extracted_poly_seg assign(list_amenities_name[[i]], extracted_seg) } # add the shop data to segmento shape unlist(list_amenities_name) ## [1] &quot;fin&quot; &quot;mkt&quot; &quot;bus&quot; &quot;parking&quot; &quot;worship&quot; ## [6] &quot;health_srv&quot; &quot;food&quot; &quot;cult_educ&quot; &quot;fuel&quot; &quot;official&quot; segmento_sh_wgs@data$fin=fin segmento_sh_wgs@data$mkt=mkt segmento_sh_wgs@data$bus=bus segmento_sh_wgs@data$parking=parking segmento_sh_wgs@data$worship=worship segmento_sh_wgs@data$health_srv=health_srv segmento_sh_wgs@data$food=food segmento_sh_wgs@data$cult_educ=cult_educ segmento_sh_wgs@data$fuel=fuel segmento_sh_wgs@data$official=official Lastly, a series of covariates is computed on the roads data. As the goal is to measure length, the first step is to change the coordinates reference system (CRS) of the roads vector into a projected coordinates system with units expressed in meters. The CRS of the original segmento data is used. roads_lines_pr=sp::spTransform(roads_lines_sh, sp::proj4string(segmento_sh)) The OSM’ roads’ are then regrouped into nine categories according to their highway class. ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify ## format in kable. kableExtra can customize either HTML or LaTeX outputs. See ## https://haozhu233.github.io/kableExtra/ for details. Category Feature’s name OSM highway class Fast track fast_track motorway_link, motorway, trunk_link, trunk, primary_link, primary Secondary secondary secondary, secondary_link Tertiary tertiary tertiary_link, tertiary Residential residential residential, corridor Rural track rural_track track Pedestrian and bike line urban_no_motor living_street, cycleway, steps, path, footway, pedestrian, service Bus lane bus bus_stop, bus_guideway Luxury luxury raceway, bridleway Unclassified unclassified no construction, unclassified, road The length of roads’ segments per segmento and according to the roads’ categories is computed thanks to the function lineLength applied via over. The results is expressed in meters, as the unit of the CRS is meters. # length length_all_seg=sp::over(segmento_sh, as(roads_lines_pr, &quot;SpatialLines&quot;), fn=lineLength, na.rm=T) length_all_seg[is.na(length_all_seg)]=0 for(i in 1:length(list_road_types)){ roads_type_sub=subset(roads_lines_pr, highway %in%list_road_types[[i]]) length_seg=sp::over(segmento_sh, as(roads_type_sub, &quot;SpatialLines&quot;), fn=lineLength, na.rm=T) length_seg[is.na(length_seg)]=0 assign(list_road_names[[i]], length_seg) } # add road length data to the shapefile segmento_sh_wgs@data$all_roads=length_all_seg segmento_sh_wgs@data$fast_track=fast_track segmento_sh_wgs@data$secondary=secondary segmento_sh_wgs@data$tertiary=tertiary segmento_sh_wgs@data$residential=residential segmento_sh_wgs@data$rural_track=rural_track segmento_sh_wgs@data$urban_no_motor=urban_no_motor segmento_sh_wgs@data$bus=bus segmento_sh_wgs@data$luxury=luxury segmento_sh_wgs@data$unclassified=unclassified 9.7.3.4 Normalizing the road data The road network is normalized by the area of segmento: we obtain the kilometers of roads per square kilometers. The density of roads is computed as the length of roads divided by the area of segmento. The latter is already computed in the shapefile and stored in the feature Shape_Area. # density: length km/area road_density_all=length_all_seg/segmento_sh@data$Shape_Area for(i in 1:length(list_road_types)){ density_type_i=get(list_road_names[[i]])/segmento_sh_wgs@data$Shape_Area assign(paste0(&quot;dens_&quot;, list_road_names[[i]]), density_type_i) } # add road density data to the shapefile segmento_sh_wgs@data$dens_all_roads=road_density_all segmento_sh_wgs@data$dens_fast_track=dens_fast_track segmento_sh_wgs@data$dens_secondary=dens_secondary segmento_sh_wgs@data$dens_tertiary=dens_tertiary segmento_sh_wgs@data$dens_residential=dens_residential segmento_sh_wgs@data$dens_rural_track=dens_rural_track segmento_sh_wgs@data$dens_urban_no_motor=dens_urban_no_motor segmento_sh_wgs@data$dens_bus=dens_bus segmento_sh_wgs@data$dens_luxury=dens_luxury segmento_sh_wgs@data$dens_unclassified=dens_unclassified 9.7.4 Population density We compute the average population density by dividing the segmento total population by its areas. segmento_sh_wgs@data$pop_dens=segmento_sh_wgs@data$gpw_es_TOT/segmento_sh_wgs@data$Shape_Area/10000 9.8 Merging the covariates with the EHPM data The data from SpatialPolygonDataframe are saved to a standard data frame. The EHPM survey data tables/ehpm-2017.csv are loaded, one selects only the development indicators and the the household id idboleta. The idboleta is used to match the EHPM with the segmento ID in tables/Identificador de segmento.xlsx. Lastly, the SEG_ID is used to match the survey data with the predictor. # save the data stored in segmento_sh_wgs@data in a standard data frame #select a subset of the covariates predictors=segmento_sh_wgs@data # survey data ehpm17=read.csv(paste0(dir_data, &quot;tables/ehpm-2017.csv&quot;)) # read the files with the segmento ID and the idboleta segID=readxl::read_xlsx(paste0(dir_data, &quot;tables/Identificador de segmento.xlsx&quot;), sheet=&quot;2017&quot;) # select only the dev indicators ehpm17_dev_indic=ehpm17%&gt;% dplyr::select(municauto, idboleta, pobreza, # poverty ingpe, # household income r202a, #Sabe leer y escribir r106 # age ) %&gt;% dplyr::rename(&quot;lit&quot;=&quot;r202a&quot;, # rename these 2 variables &quot;age&quot;=&quot;r106&quot;)%&gt;% dplyr::mutate(over_15=ifelse(age&gt;=15,1,0), literate=ifelse(lit==1,1,0), literate_over_15=over_15*literate) # Literate over 15 years old # Match the survey data with the segID ehpm17_seg=ehpm17_dev_indic%&gt;% left_join(segID, by=&quot;idboleta&quot;)%&gt;% rename(&quot;SEG_ID&quot;=&quot;seg_id&quot;) We filter out below all survey participants less than 16 years old. We compute hence development indicators for the population aged 16 years old and above. # Aggregated the survey data at the segmento level ehpm17_seg_agg=ehpm17_seg%&gt;% filter(over_15==1)%&gt;% group_by(SEG_ID)%&gt;% summarise(municauto=unique(municauto), n_obs=n(), pobreza_extrema=sum(pobreza==1,na.rm = T)/n(), pobreza_mod=sum(pobreza%in%c(1,2),na.rm = T)/n(), literacy_rate=sum(literate_over_15,na.rm = T)/n(), ingpe=median(ingpe,na.rm = T)) # Merge the aggregated survey data with the predictor data ehpm17_predictors=ehpm17_seg_agg%&gt;% right_join(predictors%&gt;% mutate(SEG_ID=as.character(SEG_ID)), by=&quot;SEG_ID&quot;) # write out the file write.csv(ehpm17_predictors, paste0(dir_data, &quot;out/ehpm17_predictors2.csv&quot;), row.names = F) print(paste(ncol(ehpm17_predictors)-21, &quot;predictors&quot;)) ## [1] &quot;78 predictors&quot; The csv file out/ehpm17_predictors.csv contains the 79 predictors derived above plus the following development indicators: ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify ## format in kable. kableExtra can customize either HTML or LaTeX outputs. See ## https://haozhu233.github.io/kableExtra/ for details. Variable name Definition pobreza_extrema Proportion of individuals 15 years in extreme poverty: income below pobreza_mod Proportion of individuals 15 years in moderate poverty: income below $107.26 in urban areas or $66.90 in rural areas literacy_rate Proportion of individuals 15 years or older who can read and write ingpe Median income of individuals 15 years or older (USD) 9.9 Summary The process of going from raw RS layers to a SpatialPolygonDataframe ready for analysis can be summarized as follows: load the RS layers create summaries for RS layers which come with a time dimension (e.g. average precipitation over the last thirthy years) perform additional raster calculation as required (e.g. compute the Soil degradation index based on soil degradation sub-indicator) compute distance metrics to public services, businesses and some natural features align the Coordinates reference systems across layers (geographic or projected) aggregate the RS layers to the map of interest (e.g. the segmento map) with a chosen set of spatial statistics match the RS aggregates with the outcome data (poverty, income, literay) write the data frame to a .csv file for later use The 22 main data sources listed on the next table are summarized in the SpatialPolygonDataframe segmento_sh_wgs into 79 segmento level spatial statistics listed on table ??. These are the candidate covariates to predict average income, poverty and literacy measured in the in the 2017 EHPM survey. ## Warning in kableExtra::column_spec(., 3, width = &quot;7cm&quot;): Please specify ## format in kable. kableExtra can customize either HTML or LaTeX outputs. See ## https://haozhu233.github.io/kableExtra/ for details. Category Features Precipitation chirps_ev_2017, chirps_ev_med Soil degradation soil_carb, soil_luc, soil_prod_state, soil_prod_prf, soil_prod_trj, soil_deg_SDG_index Vegetation Greenness (NDVI) ndvi_17_median, ndvi_17_sd, ndvi_17_sum, ndvi_17_perc_median, ndvi_17_perc_sd, ndvi_17_perc_sum Altitude (DEM) dem Population density gpw_es, pop_dens, gpw_es_TOT Temperature temp_median Livelihood Zones LZCODE,LZNAMEEN Education dist2schools_priv, dist2schools_pub,dist2schools Health services dist2health, dist2hosp Buildings people_building Shops See distances Other points of interest See distances Roads dens_all_roads, dens_fast_track, dens_secondary, dens_tertiary, dens_residential, dens_rural_track, dens_urban_no_motor, dens_bus, dens_luxury, dens_unclassified Slope slope Distances dist2road, dist2roadInter, dist2water Lights at night lights_mean, lights_sd Land Classes lc_urban,lc_crop,lc_tree Distance dist2schools_priv, dist2schools_pub, dist2schools, dist2health, dist2hosp, dist2pubamen, dist2allpubserv, dist2biz, dist2fin, dist2coast_r, dist2water_r, dist2crop_r, dist2tree_r, dist2urban_r In the next next section, we explore these data and their link with average income, poverty and literacy. References "],
["references.html", "References", " References ’` "]
]
