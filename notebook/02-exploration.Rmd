# Data exploration and preselection of covariates {#explo}


The data used in the model can be separated into two main categories: 

1.   The *outcome* variables, which are the three development indicators (income, poverty, and literacy)
2.    The *covariates*, which are the set of predictors derived in Section \@ref(data) that will be used to predict the outcome variables in segmentos not sampled in the EHPM survey.

When exploring the outcome data, the aim is to identify possible issues such as a lack of variation or the presence of outliers. The exploration also helps  in specifying the likelihood function for the outcome data (e.g., Gaussian, gamma, beta, etc.). 

For the covariates, we start by reducing their number to limit the effect of multicollinearity, which is the presence of highly correlated covariates in the model. Reducing the number of covariates will also reduce the risk of retaining covariates because of chance correlation in a next stepwise covariates selection process as shown in section \@ref(stepwise). 

Lastly, we will investigate the correlation of the preselected covariates with the outcome variables.

## Outcome Variables: Distribution and Outliers 
```{r echo=FALSE, message=FALSE, warning=FALSE,  results="hide"}
rm(list=ls())
source("../utils.R")
# Modify the directory path “dir_data” to where you stored the data
root_dir="~/"
project_dir="data/"

dir_data=paste0(root_dir,project_dir)

ehpm17_predictors=read.csv(paste0(dir_data,
                                  "out/ehpm17_predictors2.csv"))
segmento_sh=rgdal::readOGR(paste0(dir_data,
                                  "spatial/shape/admin/STPLAN_Segmentos_simplified.shp"))

# Correct for .xls misbehavior
ehpm17_predictors=ehpm17_predictors%>%
  mutate(SEG_ID=as.character(SEG_ID),
         SEG_ID=ifelse(nchar(SEG_ID)==7,
                       paste0(0,SEG_ID),
                       SEG_ID))
```

### Income 
We start by exploring the distribution of the median income per segmento. It is named `ingpe` in the dataset:


```{r income-hist-chunck,eval=T,  echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Median income per segmento', out.width='80%', fig.align='center', cache=TRUE}
ehpm17_predictors%>%
  plotly::plot_ly(x=~ingpe,
                  y=~AREA_ID,
                  type = "histogram")%>%
  layout(yaxis=list(title="Number of segmentos"),
         xaxis=list(title="Median income (USD)"),
         title="Distribution of Median Income (USD)")
```                

```{r income-hist,eval=F, echo=FALSE,out.width = '100%',fig.cap='Median Income per Segmento'}
knitr::include_graphics("img/graph_dis_ingpe.PNG")

```

As is common with income distributions, this distribution is right skewed: the median income is between USD 95 and 105 in most segmentos and reaches values above USD 200 in a few of them. An option is to log-transform the data to make it Gaussian.^[A Gaussian distribution is a normal distribution. We use the former terminology as it corresponds to the `INLA` one] Another option is to model income with a gamma likelihood function, which accounts for the asymmetry of the income distribution. 

```{r echo=FALSE,include=FALSE,message=FALSE,message=FALSE}
segmentos_wgs=sp::spTransform(segmento_sh, 
                              "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") 

segmentos_wgs@data=segmentos_wgs@data%>%
  mutate(SEG_ID=as.character(SEG_ID))%>%
  select(SEG_ID)%>%
  left_join(ehpm17_predictors%>%
              select(SEG_ID,ingpe,n_obs)%>%
              mutate(SEG_ID=as.character(SEG_ID)),
            by="SEG_ID")

segmentos_wgs_ehpm=subset(segmentos_wgs,
                          is.na(segmentos_wgs$n_obs)==F)
outliers_index=which(segmentos_wgs_ehpm$ingpe>265)
mega_outliers_index=which(segmentos_wgs_ehpm$ingpe>700)


```

```{r income-map-chunk, eval=T, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Median Income Map', out.width='80%', fig.width=3, fig.height=3,fig.show='hold',fig.align='center', cache=TRUE}
segmentos_wgs=sp::spTransform(segmento_sh, 
                              "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") 

segmentos_wgs@data=segmentos_wgs@data%>%
  mutate(SEG_ID=as.character(SEG_ID))%>%
  select(SEG_ID)%>%
  left_join(ehpm17_predictors%>%
              select(SEG_ID,ingpe,n_obs)%>%
              mutate(SEG_ID=as.character(SEG_ID)),
            by="SEG_ID")

segmentos_wgs_ehpm=subset(segmentos_wgs,
                          is.na(segmentos_wgs$n_obs)==F)
outliers_index=which(segmentos_wgs_ehpm$ingpe>265)
mega_outliers_index=which(segmentos_wgs_ehpm$ingpe>700)
leaflet(segmentos_wgs_ehpm)%>%
  addTiles()%>%
  addCircleMarkers(sp::coordinates(segmentos_wgs_ehpm)[,1],
                   sp::coordinates(segmentos_wgs_ehpm)[,2],
                   weight = 0,
                   radius = 5,
                   fillColor =~colorQuantile("RdYlGn", ingpe,na.color = "transparent",)(ingpe),
                   fillOpacity = 0.7,
                   popup = ~paste("Median income", segmentos_wgs_ehpm$ingpe,
                                  "\nNumber of obs:", segmentos_wgs_ehpm$n_obs))%>%
  addMarkers(sp::coordinates(segmentos_wgs_ehpm)[outliers_index,1],
             sp::coordinates(segmentos_wgs_ehpm)[outliers_index,2],
             popup = ~paste("Median income", segmentos_wgs_ehpm$ingpe[outliers_index],
                            "\nNumber of obs:", segmentos_wgs_ehpm$n_obs[outliers_index]))%>%
  leaflet.extras::addPulseMarkers(sp::coordinates(segmentos_wgs_ehpm)[mega_outliers_index,1],
                                  sp::coordinates(segmentos_wgs_ehpm)[mega_outliers_index,2],
                                  popup = ~paste("Median income", segmentos_wgs_ehpm$ingpe[mega_outliers_index],
                                                 "\nNumber of obs:", segmentos_wgs_ehpm$n_obs[mega_outliers_index]),
                                  icon = leaflet.extras::makePulseIcon(heartbeat = 0.5,color ="gold"))
```                

```{r income-map, eval=F,echo=FALSE,out.width = '100%',fig.cap='Median Income Map'}
knitr::include_graphics("img/map_ingpe_1.PNG")

```

Figure \@ref(fig:income-map) shows that all the segmentos with a median income above USD 265 (blue markers) are clustered together in urban centers, while the four with an income above USD 700 (gold markers) are located in Antiguo Cuscatlan, one of the country’s wealthiest neighborhoods where several embassies are located.

The national statistical office of El Salvador (DIGESTYC) classifies segmentos into rural and urban. Figure \@ref(fig:income-rur-urb) shows the log-transformed distribution of median income for rural and urban segmentos separately.

```{r income-rur-urb-chunk, eval=T,echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Rural-Urban Median Income Distribution', out.width='49%', fig.show='hold',fig.align='center', cache=TRUE}
ehpm17_predictors%>%
  group_by(AREA_ID)%>%
  plotly::plot_ly(alpha = 0.6,
                  x=~log(ingpe), # Log transform income
                  color = ~AREA_ID,
                  type = "histogram")%>%
  plotly::layout(title="Median Income in Log",
                 xaxis=list(title="Log median income per capita (USD)"),
                 yaxis=list(title="Number of segmentos"),
                 barmode = "overlay")
```                


```{r income-rur-urb,eval=F, echo=FALSE,out.width = '100%',fig.cap='Rural-Urban Median Income Distribution'}
knitr::include_graphics("img/graph_dis_ingpe_rur_urb.PNG")

```

Figure \@ref(fig:income-rur-urb) shows that log transformation appears to be successful at making the distribution Gaussian. As seen in Figure \@ref(fig:income-map), the median income is higher among urban than rural segmentos.

Furthermore, the main difference between the rural and urban income distributions is a location shift: the variance or skewness of the income distribution does not appear to vary significantly between rural and urban segmentos. Figure \@ref(fig:income-rur-urb) suggests that a single model could be used provided that a binary covariate identifying rural and urban segmentos is used in the model (instead of running separate models for rural and urban segmentos). 

In summary, this first exploration of the median income outcome data suggests that one could use either a gamma or a Gaussian likelihood function. In the latter case, income should be log-transformed. Furthermore, there are clear spatial patterns whereby urban areas have higher income levels than rural ones and segmentos that are close to each other tend to have similar income levels. Explicitly modeling spatial dependency is hence likely to improve the model’s goodness of fit. Lastly, we do not see any major difference between the distributions of income of rural and urban segmentos except for a location shift (all values shifted to the right among urban segmentos). A single model run both on rural and urban segmentos appears appropriate provided that a binary covariate identifying the rural or urban category is used. 

### Literacy
The variable `literacy_rate` provides the proportion of survey participants 16 years or older who can read and write. Its domain is bounded between 0 percent to 100 percent. 

```{r lit-dis-chunck, eval=T, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
ehpm17_predictors%>%
  group_by(AREA_ID)%>%
  plotly::plot_ly(alpha = 0.6,
                  x=~literacy_rate, # Log transform income
                  color = ~AREA_ID,
                  type = "histogram")%>%
  plotly::layout(title="Literacy Rate",
                 xaxis=list(title="Literate respondents",
                            tickformat = "%"),
                 yaxis=list(title="Number of segmentos"),
                 barmode = "overlay")
```

```{r lit-dis, eval=F,echo=FALSE,out.width = '100%',fig.cap='Rural-Urban Literacy Rates'}
knitr::include_graphics("img/graph_dis_lit_rur_urb.PNG")

```

The distribution of literacy in rural and urban segmentos appears quite different. Of the 950 urban segmentos, 181 have a 100 percent literacy rate among adult EHPM respondents. In contrast, no such spike is observed in the rural areas. This finding suggests that literacy could be modeled as a two-step process for urban areas: (1) the presence/absence of illiteracy is modeled, and (2) the proportion of illiteracy is modeled. In rural areas, only the second step would be necessary.

Figure \@ref(fig:lit-map) shows the map of literacy for the EHPM segmentos.

```{r echo=FALSE,include=FALSE,warning=FALSE,message=FALSE}
segmentos_wgs=sp::spTransform(segmento_sh, 
                              "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") 

segmentos_wgs@data=segmentos_wgs@data%>%
  mutate(SEG_ID=as.character(SEG_ID))%>%
  select(SEG_ID)%>%
  left_join(ehpm17_predictors%>%
              select(SEG_ID,literacy_rate,n_obs)%>%
              mutate(SEG_ID=as.character(SEG_ID)),
            by="SEG_ID")

segmentos_wgs_ehpm=subset(segmentos_wgs,
                          is.na(segmentos_wgs$n_obs)==F)

```

```{r lit-map-chunck, eval=T, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
segmentos_wgs=sp::spTransform(segmento_sh, 
                              "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") 

segmentos_wgs@data=segmentos_wgs@data%>%
  mutate(SEG_ID=as.character(SEG_ID))%>%
  select(SEG_ID)%>%
  left_join(ehpm17_predictors%>%
              select(SEG_ID,literacy_rate,n_obs)%>%
              mutate(SEG_ID=as.character(SEG_ID)),
            by="SEG_ID")

segmentos_wgs_ehpm=subset(segmentos_wgs,
                          is.na(segmentos_wgs$n_obs)==F)

# Create quantile color bins to increase contrast between areas with different literacy rates
qpal = colorQuantile("RdYlGn", segmentos_wgs_ehpm$literacy_rate,na.color = "transparent",n = 5)

qpal_colors = unique(qpal(sort(segmentos_wgs_ehpm$literacy_rate))) # Hex codes
qpal_labs = quantile(segmentos_wgs_ehpm$literacy_rate, seq(0, 1, .2),na.rm=T) # Depends on n from pal
qpal_labs=round(qpal_labs*100)
qpal_labs = paste(lag(qpal_labs), qpal_labs, sep = " - ")[-1] # First lag is NA
qpal_labs=paste(qpal_labs, "%")

# Map
leaflet(segmentos_wgs_ehpm)%>%
  addTiles()%>%
  addCircleMarkers(sp::coordinates(segmentos_wgs_ehpm)[,1],
                   sp::coordinates(segmentos_wgs_ehpm)[,2],
                   weight = 0,
                   radius = 5,
                   fillColor =~qpal(literacy_rate),
                   fillOpacity = 1,
                   popup = ~paste("Literacy", round(segmentos_wgs_ehpm$literacy_rate*100),"%",
                                  "\nNumber of obs:", segmentos_wgs_ehpm$n_obs))%>%
  addLegend(colors = qpal_colors, 
            labels = qpal_labs,
            opacity = 1)
```

```{r lit-map,eval=F, echo=FALSE,out.width = '100%',fig.cap='Literacy Rate Map'}
knitr::include_graphics("img/map_lit_1.PNG")

```

There is a clear spatial pattern: literacy rates are highest in urban areas, particularly around the capital city. Areas in the northwest (the departments of San Miguel, Morazan, and La Union) have low literacy levels.

In summary, modeling literacy will be more complex than modeling income. First, a zero-inflated model might be appropriate, particularly for urban segmentos. Second, the distribution of literacy appears to vary between rural and urban segmentos, and not only in terms of the location. Therefore, separate or mixed models for rural and urban segmentos might be more appropriate. In both cases, a beta likelihood function might be appropriate, as literacy rates are bounded between 0 and 100 percent. Lastly, there is a clear spatial pattern suggesting that explicitly taking the spatial dependence structure into account will increase the model’s goodness of fit.


### Moderate Poverty
The variable `pobreza_mod` provides the proportion of survey participants 16 years or older living in moderate poverty. Its domain is bounded between 0 to 1. 

```{r modpov-dis-chunk, eval=T, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
ehpm17_predictors%>%
  group_by(AREA_ID)%>%
  plotly::plot_ly(alpha = 0.6,
                  x=~pobreza_mod, # Log transform income
                  color = ~AREA_ID,
                  type = "histogram")%>%
  plotly::layout(title="Moderate Poverty",
                 xaxis=list(title="Adult respondents living in moderate poverty",
                            tickformat = "%"),
                 yaxis=list(title="Number of segmentos"),
                 barmode = "overlay")
```


```{r modpov-dis, eval=F,echo=FALSE,out.width = '100%',fig.cap='Rural-Urban Moderate Poverty Rate'}
knitr::include_graphics("img/graph_dis_modpov_rur_urb.PNG")

```


Figure \@ref(fig:modpov-dis) shows that more than 100 urban segmentos have no adult EHPM respondents living in poverty but no rural segmento does. Except for this important difference, the distribution of poverty of rural and urban segmentos does not appear to differ much.

Figure \@ref(fig:pov-map) shows the map of moderate poverty for the EHPM segmentos.
```{r pov-map-chunk, eval=T, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
segmentos_wgs=sp::spTransform(segmento_sh, 
                              "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") 

segmentos_wgs@data=segmentos_wgs@data%>%
  mutate(SEG_ID=as.character(SEG_ID))%>%
  select(SEG_ID)%>%
  left_join(ehpm17_predictors%>%
              select(SEG_ID,pobreza_mod,n_obs)%>%
              mutate(SEG_ID=as.character(SEG_ID)),
            by="SEG_ID")

segmentos_wgs_ehpm=subset(segmentos_wgs,
                          is.na(segmentos_wgs$n_obs)==F)

# Create quantile color bins to increase contrast between areas with different literacy rates
qpal = colorQuantile("RdYlGn", segmentos_wgs_ehpm$pobreza_mod,na.color = "transparent",n = 5,
                     reverse = T)

qpal_colors = unique(qpal(sort(segmentos_wgs_ehpm$pobreza_mod))) # Hex codes
qpal_labs = quantile(segmentos_wgs_ehpm$pobreza_mod, seq(0, 1, .2),na.rm=T) # Depends on n from pal
qpal_labs=round(qpal_labs*100)
qpal_labs = paste(lag(qpal_labs), qpal_labs, sep = " - ")[-1] # First lag is NA
qpal_labs=paste(qpal_labs, "%")

# Map
leaflet(segmentos_wgs_ehpm)%>%
  addTiles()%>%
  addCircleMarkers(sp::coordinates(segmentos_wgs_ehpm)[,1],
                   sp::coordinates(segmentos_wgs_ehpm)[,2],
                   weight = 0,
                   radius = 5,
                   fillColor =~qpal(pobreza_mod),
                   fillOpacity = 1,
                   popup = ~paste("Moderate poverty", round(segmentos_wgs_ehpm$pobreza_mod*100),"%",
                                  "\nNumber of obs:", segmentos_wgs_ehpm$n_obs))%>%
  addLegend(colors = qpal_colors, 
            labels = qpal_labs,
            opacity = 1)
```

```{r pov-map, eval=F,echo=FALSE,out.width = '100%',fig.cap='Moderate Poverty Map'}
knitr::include_graphics("img/map_modpov_1.PNG")

```

Interestingly, the picture is much less clear when mapping moderate poverty than income. However, the general urban-rural divide appears to hold.

As poverty is measured as a proportion, we will use a beta likelihood. Given the high number of segmentos with zero respondents living in poverty, we will test a zero-inflated beta likelihood. Lastly, as the excess number of zeros is present only among urban segmentos, we will compare results of the two separate models for rural and urban segmentos with the results of when only one model is run.

## Unsupervised Dimension Reduction Based on Distance Correlation
We now reduce the number of covariates by removing highly correlated covariates. Before starting the process, we standardize the covariates to limit the effect of the various scales and measurement units of each covariate on the computation of the correlation. 

### Obtaining a Data Frame with Standardized Covariates 
We start by storing the candidate covariates in a data frame.

```{r}
# Join with shapefile ####
segmento_sh_data=segmento_sh
segmento_sh_data@data=segmento_sh_data@data%>%
  dplyr::select(SEG_ID)%>%
  mutate(SEG_ID=as.character(SEG_ID))%>%
  left_join(ehpm17_predictors,
            by="SEG_ID")

# Identify EHPM segmentos ####
segmento_sh_data_model=segmento_sh_data 

ehpm_index=which(is.na(segmento_sh_data_model$n_obs)==F)
ehpm17_predictors_survey=segmento_sh_data_model@data[ehpm_index,]

# Remove outcome variables and administrative information ####

# covariates=ehpm17_predictors_survey%>%
#   select(-c(SEG_ID,municauto,n_obs,pobreza_extrema,pobreza_mod,literacy_rate,ingpe,DEPTO,
#             COD_DEP,MPIO,COD_MUN,CANTON,COD_CAN,COD_ZON_CE,COD_SEC_CE,COD_SEG_CE,LZCODE,
#             ndvi_17_perc_median,ndvi_17_perc_sd,ndvi_17_perc_sum,
#             chirps_ev_perc_norm,chirps_ev_2017, chirps_ev_perc_norm,
#             dist2allpubserv,lights_sd,gpw_es_TOT,gpw_es_avg_dens,dist2water_r,
#             soil_prod_state,soil_carb,soil_luc,soil_prod_prf,soil_prod_state,soil_prod_trj,settlements))
```

```{r}
covariates=ehpm17_predictors_survey%>%
  select(-c(SEG_ID,municauto,n_obs,pobreza_extrema,pobreza_mod,literacy_rate,ingpe,DEPTO,
            COD_DEP,MPIO,COD_MUN,CANTON,COD_CAN,COD_ZON_CE,COD_SEC_CE,COD_SEG_CE,LZCODE,OBJECTID, SHAPE_Leng,AREA_KM2,dens_bus,dens_luxury))

# Create binary variables from the livelihood zone categories
covariates=covariates%>%
  mutate(AREA_ID=ifelse(AREA_ID=="U",1,0),
         LZ_BasicGrainLaborZone=ifelse(LZNAMEEN=="Basic Grain and Labor Zone",
                                       1,0),
         LZ_CentralIndusServ=ifelse(LZNAMEEN=="Central Free-Trade, Services and Industrial Labor Zone",
                                       1,0),
         LZ_CoffeeAgroIndus=ifelse(LZNAMEEN=="Coffee, Agro-Industrial and Labor Zone",
                                    1,0),
         LZ_LvstckGrainRem=ifelse(LZNAMEEN=="Eastern Basic Grain, Labor, Livestock and Remittance Zone",
                                   1,0),
         LZ_FishAgriTourism=ifelse(LZNAMEEN=="Fishing, Aquaculture and Tourism Zone",
                                  1,0))%>%
  select(-LZNAMEEN)
```

We write a function `myStd` to standardize the covariates by subtracting the mean from each observation and dividing them by the standard deviation. 

```{r}
myStd=function(x){ # Standardize a given covariate x with the myStd function
  mu_x=mean(x,na.rm = T)
  sd_x=sd(x,na.rm = T)
  x_std=(x-mu_x)/sd_x
  return(x_std)
}
```

We then apply `myStd` to each covariate.
```{r}
covariates_std=apply(covariates,2,myStd) # We then apply the function myStd to each covariate

```

The data frame `covariates_std` contains the standardized covariates.

Before removing highly correlated covariates, we compute the variance inflation factor (VIF) to determine the presence of multicollinearity.

```{r}
# Calculate the VIF before selecting covariates ####
covariates_std_df=as.data.frame(covariates_std)

data_model=covariates_std_df%>%
  bind_cols(ehpm17_predictors_survey%>%
              select(ingpe))
formula_glm= reformulate(paste(names(covariates_std_df),collapse="+"),
                         "ingpe")
glm_0=glm(formula_glm,
          data=data_model)

sort(car::vif(glm_0))
```
```{r}
dim(covariates_std_df)
vif_res=car::vif(glm_0)
length(which(c(vif_res)>5))
```

Values below 3 mean there are no multicollinearity issues. Values above 5 are indicative of multicollinearity issues. Of the 82 candidate covariates, 22 have a VIF higher than 5. Some covariate preselection is hence justified. This multicollinearity might be caused by the fact that 

*   Many predictors are an expression of the same raw data (e.g., distance to schools and distance to public services)
*   Some predictors cover similar physical features but come from different data sources (e.g., distance to coastline and distance to water bodies)
*   Some predictors are closely associated with each other (e.g., lights at night and percentage of the area classified as urban)

### Dimension Reduction Based on Distance Correlation
We start by computing our Distance correlation between all of the covariate pairs. Measuring correlation with this correlation better accounts for nonlinearity in the relationship between two variables than a standard Pearson correlation.

This can be illustrated by computing the Pearson and Distance correlations in the case of a linear or quadratic relationship. 
```{r cache=TRUE,eval=T}
x=seq(-10,10,length.out=1000)
y=2*x+rnorm(mean=2,sd=10,1000)

plotly::plot_ly(x=x,
        y=y,
        type="scatter",
        mode="markers")%>%
  plotly::layout(title=paste0("Pearson corr:",format(round(cor(x,y),4),scientific=F),
                 "\nDistance corr:",round(dcor(x,y),4)))
x=seq(-10,10,length.out=1000)
y=-x^2+rnorm(mean=2,sd=10,1000)
plotly::plot_ly(x=x,
        y=y,
        type="scatter",
        mode="markers")%>%
  plotly::layout(title=paste0("Pearson corr:",format(round(cor(x,y),4),scientific=F),
                 "\nDistance corr:",round(dcor(x,y),4)))
```
```{r linear, eval=F,echo=FALSE,out.width = '80%',fig.cap='Linear Relationship: Pearson Correlation Picks It Up'}
knitr::include_graphics("img/linear.PNG")
```

```{r quadra, eval=F,echo=FALSE,out.width = '80%',fig.cap='Quadratic Relationship: Only The Distance Correlation Picks It Up'}
knitr::include_graphics("img/quadra.PNG")
```

While the measure of association is similar for both correlation metrics when the dependence between the variables is linear, as shown in Figure  \@ref(fig:linear), the Distance correlation is much better at capturing a nonlinear relationship between $X$ and $Y$, as shown by the Distance correlation of 0.46 against -0.0128 for the Pearson correlation reported in Figure \@ref(fig:quadra).

The Distance correlation for all pairs of covariates is computed as follows. 

```{r cache=TRUE, eval=T}
# Compute the Distance correlation between all pairs of covariates
tic()
distMatrix=dcor.matrix(covariates_std)
toc()
diag(distMatrix)=NA # Replace diag with NA
```

We then randomly remove one of the two most-correlated covariates and repeat the process until only 50 covariates are left, which is 5 percent of the sample size of the training. The latter figure is a generally accepted rule of thumb to decide on an appropriate number of covariates to start the backward covariate selection process, as it limits the risk of chance correlation—finding a significant correlation by chance.

```{r eval=T}
# Randomly remove one of the two most-correlated covariates 
trainig_set_n=dim(covariates_std)[1]*0.6 # Size of the training set = 60%, validation set = 20%, test set=20%
target_n_cov=round(0.045*trainig_set_n) # Target number of covariates: 5% of training set= 50
n_cov=dim(covariates_std)[2] # Starting number of covariates = 58 
n_to_remove=n_cov-target_n_cov

i=1
distMatrix_reduced=distMatrix

for(i in 1:n_to_remove){ # Continue to remove the two most-correlated covariates until covariates number only 5% of training set 
  index_max <- arrayInd(which.max(distMatrix_reduced),
                        dim(distMatrix_reduced)) # Get index of columns and rows of the max d-corr cell
  
  set.seed(i+2) # Set seeds for replicability
  print(paste(colnames(distMatrix_reduced)[index_max]))
  print(distMatrix_reduced[index_max])

  remove_cov_pair=round(runif(1)+1) # Either the row (1) or column covariate (2)
  remove_cov=index_max[remove_cov_pair] # Select index of row or column
  print(paste("cov removed",colnames(distMatrix_reduced)[remove_cov])) # Print the names of the covariate removed
  print("")
  distMatrix_reduced=distMatrix_reduced[-remove_cov,-remove_cov] # Remove the corresponding covariate from distMatrix
  
}
```

Next, we recompute the VIF with the remaining 50 covariates. Median lights at night (named `lights_med` in the dataset), the rural-urban DYGESTIC binary segmentos classification (named `AREA_ID` in the dataset), population density (`pop_dens`), and slope (`slope`) are removed in the process, but they need to be reintroduced because we expect them to be important predictors of the development outcomes.
```{r eval=T}
# RE calculate vif ####
sort(colnames(distMatrix_reduced))

covariates_std_reduced=covariates_std[,c(colnames(distMatrix_reduced),"lights_med","AREA_ID","pop_dens","slope")]
covariates_std_reduced_df=as.data.frame(covariates_std_reduced)

data_model=covariates_std_reduced_df%>%
  bind_cols(ehpm17_predictors_survey%>%
              select(ingpe))
formula_glm_1= reformulate(paste(names(covariates_std_reduced_df),collapse="+"),
                         "ingpe")
glm_1=glm(formula_glm_1,
          data=data_model)
sort(car::vif(glm_1))
sort(names(covariates_std_reduced_df))
```
The VIF figures are much more acceptable now. The reduced number of covariates limits the risk of chance correlation. We write the covariates and their names in `.csv` files for later use.
```{r eval=T}
# Write the names of selected features into .csv ####
write.table(names(covariates_std_reduced_df),
            paste0(dir_data,
                   "out/distance_corr_var/selected_all.txt"),
            row.names = F)

# Write the selected features and outcome into .csv ####
covariates_std_reduced_df$SEG_ID=ehpm17_predictors_survey$SEG_ID
covariates_std_reduced_df$ingpe=ehpm17_predictors_survey$ingpe
covariates_std_reduced_df$literacy_rate=ehpm17_predictors_survey$literacy_rate
covariates_std_reduced_df$pobreza_extrema=ehpm17_predictors_survey$pobreza_extrema
covariates_std_reduced_df$pobreza_mod=ehpm17_predictors_survey$pobreza_mod
covariates_std_reduced_df$n_obs=ehpm17_predictors_survey$n_obs

write.csv(covariates_std_reduced_df,
            paste0(dir_data,
                   "out/all_covariates_and_outcomes.csv"),
            row.names = F)
```


## Correlations between the Predictors and the Outcome Variables
We now explore the relationship between the predictors and the outcome variables. This will give us a feel of the important covariate in the model.

### Correlation with Income
We compute the distance correlation for all covariates with a loop and store these correlations in a data frame.

```{r eval=T}
covariates_std_reduced_df$SEG_ID=ehpm17_predictors_survey$SEG_ID
covariates_std_reduced_df$ingpe=ehpm17_predictors_survey$ingpe
covariates_std_reduced_df$literacy_rate=ehpm17_predictors_survey$literacy_rate
covariates_std_reduced_df$pobreza_extrema=ehpm17_predictors_survey$pobreza_extrema
covariates_std_reduced_df$pobreza_mod=ehpm17_predictors_survey$pobreza_mod
covariates_std_reduced_df$n_obs=ehpm17_predictors_survey$n_obs
corr_re=list()
names_re=list()

for(i in 1:50){ # Where 50 is the number of 
  cor_test=energy::dcor(covariates_std_reduced_df$ingpe,covariates_std_reduced_df[,i])
  corr_re[[i]]=c(cor_test)
  names_re[[i]]=names(covariates_std_reduced_df)[i]
}
corr_df=do.call(rbind,corr_re)
names_df=do.call(rbind,names_re)

corr_df=as_tibble(corr_df)
corr_df$covariate = names_df 

names(corr_df)[1]="cor"

corr_df=corr_df %>% 
  arrange(desc(abs(cor)))
```

Lastly, we select only the variables with an absolute distance correlation with income higher than 20 percent and plot the correlation with income. The latter selection is only for illustrative purposes: we do not want to clutter the graph.  
```{r cache=TRUE, eval=T}
corr_df_top=corr_df%>%
    filter(!(covariate%in%c("pobreza_mod","literacy_rate","ingpe")))%>%
  filter(abs(cor)>=.2)

corr_df_top$covariate_f=factor(corr_df_top$covariate,
                               levels = corr_df_top$covariate)
plot_ly(x=corr_df_top$covariate_f,
       y=corr_df_top$cor,
       type="bar",
       name = corr_df_top$covariate)%>%
  layout(title="Covariate Correlations with Income",
         yaxis=list(tickformat = "%"))

```

```{r corr-ingpe, eval=F, Techo=FALSE,out.width = '80%',fig.cap='Correlation of Candidate Covariates with Income'}
knitr::include_graphics("img/corr_ingpe.PNG")
```

As expected, among the three most important covariates are those indicative of urban centers: light at night intensity, rural/urban classification of the segment, and distance to public amenities. 

To get a sense of the direction of the correlation and the associations between the variables, we plot a correlogram.

```{r cache=TRUE, eval=T}
covariates_std_reduced_df_selected=covariates_std_reduced_df%>%
  dplyr::select_(.dots =c("ingpe",corr_df_top$covariate))

qgraph::qgraph(cor(covariates_std_reduced_df_selected),
               minimum=0.20,
               layout="spring",
               # groups=gr,
               labels=names(covariates_std_reduced_df_selected),
               label.scale=F,
               title="Correlations: Income and Predictors")
```

```{r corrgram-ingpe,  eval=F,echo=FALSE,out.width = '80%',fig.cap='Correlogram of Income (ingpe) with Candidate Covariates'}
knitr::include_graphics("img/corrgram_ingpe.PNG")
```

The plot in Fig.\@ref(fig:corrgram-ingpe) clearly shows that the top predictors are highly correlated.     

### Correlation with Literacy
The process described in the previous subsection is repeated for literacy.
  
```{r eval=T,echo=FALSE, warning=FALSE,message=FALSE,cache=TRUE}
corr_re=list()
names_re=list()

for(i in 1:50){ # Where 40 is the number of 
  cor_test=energy::dcor(covariates_std_reduced_df$literacy_rate,covariates_std_reduced_df[,i])
  corr_re[[i]]=c(cor_test)
  names_re[[i]]=names(covariates_std_reduced_df)[i]
}
corr_df=do.call(rbind,corr_re)
names_df=do.call(rbind,names_re)

corr_df=as_tibble(corr_df)
corr_df$covariate = names_df 


names(corr_df)[1]="cor"

corr_df=corr_df%>%
  arrange(desc(abs(cor)))

corr_df_top=corr_df%>%
    filter(!(covariate%in%c("pobreza_mod","literacy_rate","ingpe")))%>%
  filter(abs(cor)>=.2)

corr_df_top$covariate_f=factor(corr_df_top$covariate,
                               levels = corr_df_top$covariate)
plot_ly(x=corr_df_top$covariate_f,
       y=corr_df_top$cor,
       type="bar",
       name = corr_df_top$covariate)%>%
  layout(title="Covariate Correlations with Literacy",
         yaxis=list(tickformat = "%"))

```

```{r corr-lit,  eval=F,echo=FALSE,out.width = '80%',fig.cap='Correlation of candidate covariates with literacy'}
knitr::include_graphics("img/corr_lit.PNG")
```

Light at night is again the best predictor of literacy, by distance, the rural-urban segmento identifier, and normalized difference vegetation index (NDVI).

### Correlation with Poverty
In examining this correlation, NDVI and distance to public amenities and hospitals, in particular, come first. We see that the level of correlation is lower than in the previous plot, suggesting that it might be harder to find good model fit for moderate poverty than for the other indicators.
```{r eval=T, echo=FALSE, warning=FALSE,message=FALSE,cache=TRUE}
corr_re=list()
names_re=list()

for(i in 1:40){ # Where 40 is the number of 
  cor_test=energy::dcor(covariates_std_reduced_df$pobreza_mod,covariates_std_reduced_df[,i])
  corr_re[[i]]=c(cor_test)
  names_re[[i]]=names(covariates_std_reduced_df)[i]
}
corr_df=do.call(rbind,corr_re)
names_df=do.call(rbind,names_re)

corr_df=as_tibble(corr_df)
corr_df$covariate = names_df 

names(corr_df)[1]="cor"

corr_df=corr_df%>%
  arrange(desc(abs(cor)))

corr_df_top=corr_df%>%
    filter(!(covariate%in%c("pobreza_mod","literacy_rate","ingpe")))%>%
  filter(abs(cor)>=.1)

corr_df_top$covariate_f=factor(corr_df_top$covariate,
                               levels = corr_df_top$covariate)
plot_ly(x=corr_df_top$covariate_f,
       y=corr_df_top$cor,
       type="bar",
       name = corr_df_top$covariate)%>%
  layout(title="Covariate Correlations with Moderate Poverty",
         yaxis=list(tickformat = "%"))
```

```{r corr-modpov,  eval=F,echo=FALSE,out.width = '80%',fig.cap='Correlation of Candidate Covariates with Moderate Poverty'}
knitr::include_graphics("img/corr_modpov.PNG")
```
