# Appendix 1: Primer on INLA {#theory}

This section provides an introduction to high resolution mapping based on Bayesian geostatistical techniques. The aim is to provides an intuitive understanding of the models rather than an in-depth derivation and explanation of the theory underpinning them. This section is largely based on the books by Zuur et al. [@zuur2017beginner] and Blangiardo and Cameletti [@blangiardo2015spatial].

## Area data vs point-referenced data
There are three types of spatial data [@blangiardo2015spatial]: 

*   *Area or lattice data*: the outcome variable to be modelled is a "random aggregate value over an aeral unit"[@blangiardo2015spatial]. The difference between area and lattice data is that the latter is aggregated over a regular grid while the former is aggregated over a set of irregular polygons such as administrative boudaries. For instance, income aggregated at the *segmento* level are *area* data. 
*   *Point-referenced (or geostatistical) data*: the outcome variable to be modelled is "a random outcome at a specific location" [@blangiardo2015spatial], such as households' income measured at the household location georeferenced with latitude and longitude coordinates.
*   *Spatial point patterns*: the outcome variable to be modelled represents the occurence or not of an event at a given location. While *point referenced data* are random outcomes at a *specific location*, the location of the occurence or not are themselve random in *spatial point patterns*. For instance, road traffic accidents could be modelled as a *spatial point patterns*.   

The EHPM data were collected at the household level. They were then aggregated at the *segmento* level, they are hence *area data*. Nevertheless, we will also explore a model assuming they are *point-referenced data*, using the centroids of the *segmentos* as the location.

### Bayesian approach
In  the frequentist approach, the interest lies in estimating in the probabilty of the data, say income, given the parameters, $P(data | \beta)$, i.e. one assumes the parameters exist. In a Bayesian approach, the interest lies in estimating  the probability of the parameters given the data, $P(\beta | data)$. 


The Bayes theorem states that:
$$
\begin{aligned}
P(A&B)=P(A|B)P(B)
\end{aligned}
$$

$$
\begin{aligned}
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{aligned}
$$
Replacing $A$ and $B$ with the quantity of interest, this gives:

$$
\begin{aligned}
P(\beta|data)=\frac{P(data|\beta)P(\beta)}{P(data)}
\end{aligned}
$$
where $P(\beta|data)$ is the posterior distribution of the parameter $\beta$, $P(data|\beta)$ is the likelihood function of the data (e.g. a *Gamma* distribution for income), $P(\beta)$ is the the prior distribution of the $\beta$ parameter.

This can be expressed as:
  
$$
\begin{aligned}
P(\beta|data)\propto P(data|\beta)P(\beta)
\end{aligned}
$$
which reads as the posterior distribution of $\beta$ is proportional to $P(data|\beta)P(\beta)$. 

There are various strategies for getting  $P(data|\beta)P(\beta)$: simulation with a Monte Carlo or Markov Chain Monte Carlo algorythm, or deterministic approach via  Integrated Nested Laplace Approximation (INLA) implemented in the INLA package.

## Model formulation
The aim is to model the relationship between the *outcome variable* $y$, for instance *segmento* median income, and a set of covariates $\mathbf{X}$, the bold notation denoting that $\mathbf{X}$ is matrix with each column corresponding to a covariate. 

### A simple model not accounting for spatial dependence
Ignoring for now the spatial dimension, this could be modeled as:
  
$$
\begin{aligned}
\mathbf{y}=\beta_{0}+\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon}
\end{aligned}
$$
  
$$
\begin{aligned}
\mathbf{\epsilon}\sim N(0,\mathbf\Omega)
\end{aligned}
$$
  
where $\mathbf{y}$ is a income vector of size $n$ for the $n$ *segmentos*, with each element $y_{i}$ of $i=1, ... ,n$ is the median income level in *segmemto* $i$, $\mathbf{X}$ is the matrix of covariates with $k$ columns for the $k$ covariates and $n$ rows for the $n$ *segmentos*, $\mathbf{\beta}$ is the vector of $k$ parameters indicating the effect of each covariate on $y$,  $\mathbf{\epsilon}$ is the vector of error term of size $n$, distributed normally with mean $0$ and variance-covariance defined by the matrix $\mathbf{\Omega}$:

$$
\begin{aligned}
\mathbf\Omega = \left[\begin{array}
                       {rrrr}
                       \sigma^2 & 0 & \dots & 0 \\\
                       0 & \sigma^2 & \dots & \vdots \\\
                       \vdots &  &\ddots & 0 \\\
                       0 & \dots & 0 & \sigma^2
                       \end{array}\right]=\sigma^2\mathbf{I}
\end{aligned}
$$

with each off-diagonal element of $\mathbf\Omega$ representing the covariance of $\epsilon_{i}$ and $\epsilon_{j}$, here set to $0$, and $\sigma^2$ is the variance, which can hence be more consisely written as $\sigma^2\mathbf{I}$, where $\mathbf{I}$ is an identity matrix where the diagonal elements are $1$ and the off-diagonal elements are $0$.

The strong assumption of this model is that the term $\epsilon_{i}$ is idependently and identically distributed, often abreviated as *iid* distributed. Let us unpack this expression. 

*Independence.* The independence assumption implies that $\epsilon_{i}$ and $\epsilon_{j}$ for any two pairs of *segmento* $i$ and $j$ will be independent, even if $i$ and $j$ are neighbouring *segmentos*. This is represented by the covariance between $\epsilon_{i}$ and $\epsilon_{j}$ for $i\neq j$ being set to zero in $\Omega$.

We can expect that it does not hold in the case of average income per *segmento*. For instance, let us imagine that a large company is recruiting its worksforce in *segmentos* $i$ and $j$ and that no data is avaible in $\mathbf{X}$ to take this into account. Hence, $\epsilon_{i}$ and $\epsilon_{j}$ will be highly correlated, violating the independence assumption.      

*Identical distribution.* The are no symbol $i$ in the normal distribution $N(\mu,\sigma^2)$, i.e. $\epsilon_{i}$ of all *segmento* are assumed to have a mean $\mu$ and a variance $\sigma^2$. However, both the mean and the variance might vary across locations. For instance, one can expected that in an areas where most of the workforce is employed in agriculture, weather shocks might implies larger deviation from model prediction than in areas where most of the workforce is employed in the service sector, leading to different variance. Similarly, the model might systematically over- or under-predict in some areas, leading to $\epsilon_{i}$ with $\mu$ higher or under zero. 

The violation of theses assumptions lead to incorrect estimate of the precison of the $\mathbf{\beta}$ parameters. 

Furthermore, modelling explicilty the spatial dependence increase the predictive performance of the model; not taking into account the location implies throwing away a imporant piece of information.


### A spatial model
In order to model explicitly the spatial dependence, an additional term is added to equation: 

$$
\begin{aligned}
\mathbf{y}=\beta_{0}+\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon}+\mathbf{\upsilon}
\end{aligned}
$$

where $\mathbf{\epsilon}$ is distributed according to equation presented in the previous sub-section and $\mathbf{\upsilon}$ is a spatially correlated random effect distributed according:
  
$$
\begin{aligned}
\mathbf{\upsilon}\sim N(0,\mathbf\Sigma)
\end{aligned}
$$

where $\Sigma$ is non-diagonal matrix given by:
  
$$
\begin{aligned}
\mathbf\Sigma = \left[\begin{array}
                       {rrrr}
                       \sigma_{u}^2 &\phi_{1,2}^2 & \dots & \phi_{1,n}^2  \\\
                       \vdots & \sigma_{u}^2 & \dots & \vdots \\\
                       \vdots &  &\ddots & \phi_{n-1,n-1}^2 \\\
                       \dots & \dots & \dots & \sigma_{u}^2
                       \end{array}\right]
\end{aligned}
$$

where $\sigma_{u}^2$ is the variance of the $\upsilon$ and the $\phi_{i,j}=corr(\upsilon_{i},\upsilon_{j})\neq0$ is the correlation between the spatial random effect at location $i$ and $j$.

The challenge now is to estimate the $\phi_{i,j}$ parameters. In the present case, the matrix $\mathbf\Sigma$ has a dimension of $1664*1664$, i.e 2'768'896 parameters to estimates. 


### Estimating the model with aeral data
When working with aearal data, quantiyfying proximity with an euclidean distance is not adequate. First, one would need to figure out which point of the area should be considered when measuring the distance. One option could be to take some measure of the centre of each *segmentos*, however larger *segmentos* would then appear more remote than smaller one. The prefered approach with aeral data is hence slightly different: proximity is defined in terms of which *segmentos* do share a border. 

For sack of simplicity, let us assume for the moment that the link function is a simple identity function such that $g(\mu(s_{i}))=\mu(s_{i})$. $\mu(s_{i})$ can hence be expressed as:
  
$$
\begin{aligned}
\mu(s_{i})=\eta_{i}=\mathbf{X(s_{i})}\mathbf{\beta}+\upsilon_{i}
\end{aligned}
$$

A conditional autoregressive model (CAR) correlation function is used to model the spatial random effect $\upsilon_{i}$ (when dealing with point reference data, a Matern correlation and its SDPE expression is used instead, see next sub-section).

The spatial dependency is assumed to be Markovian in nature, i.e. the spatial correlation can be summarized by the spatial correlation between direct neighbours. 

The distribution of each $\upsilon_{i}$ conditional on all other $\upsilon$ is expressed as:
  
$$
\begin{aligned}
\upsilon_{i}|\upsilon_{-i} \sim N(\sum_{j\neq i}^{N}c_{i,j}\upsilon_{j}, \sigma_{i}^{2}) 
\end{aligned}
$$

where ${-i}$ means all except $i$. In the simplest model, called the intrinsic CAR, $c_{i,j}$ is $1$ if $j$ is neigbour of $i$ and $0$ otherwise. The conditional mean of each $\upsilon_{i}$ is hence an average of its direct neighbour.

The joint distribution of all $\upsilon_{i}$ is also Gaussian and written as:

$$
\begin{aligned}
\mathbf{\upsilon} \sim N(0, (\mathbf{I-C})^{-1}\mathbf{M}) 
\end{aligned}
$$

where $\mathbf{I}$ is a matrix of $1$ and $0$, $\mathbf{C}$ contains the $c_{i,j}$ and $\mathbf{M}$ is the covariance matrix. Various CAR model specify various function for $\mathbf{C}$ and $\mathbf{M}$.

The homogeneous CAR model set:

$$
\begin{aligned}
\mathbf{C}=\phi\mathbf{A} \space \space \space \space \space \mathbf{M}=\mathbf{I}\sigma^{2}_{CAR}
\end{aligned}
$$

where $\mathbf{A}$ is $1$ if area are neighbour and $0$ otherwise and $\phi$ needs to be estimated: larger it is, more corellated are the random effects. The instric car model set $\phi$ to 1. 

One of the issues of CAR models is to lead to overfitting, i.e. model perform well on the training set of the data but poorly when doing out of sample prediction on the test set for validation purposes. 

A solution is to smooth the pattern of the spatial random effects. With a lower $\sigma^{2}_{CAR}$, the $\upsilon$ will vary less from neighbour to neigbour, leading to less overfit. In order to push the $\sigma^{2}_{CAR}$ to be lower, one can work on the priors. The standard way of doing this is by using penalized complexity priors whereby one specify the probability that $\sigma^{2}_{CAR}$ will be larger than a given number. To further limit the risk of overfitting. The standard way is to  decompose the spatial random effects into one part which is spatially correlated and one part which is pure noise. This is called the modified Besag-York-Mollie [@simpson2017penalising], which we will be using in the next section. 

### Estimating the model with point-referenced data
Let us now assume that the *segmentos* median income are point referenced data. 

We now reconsider $\mathbf{y}$ as a random variable distributed at $n$ locations $s_{1}, ...,s_{n}$. The income data can be considered as a sample of $y(s_{1}), ..., y(s_{n})$ from the random process $\mathbf{y(s)}$.

Assuming that the $y(s_{i})$ are normally distributed, we have:
  
$$
\begin{aligned}
y(s_{i})\sim N(\mu(s_{i}),\sigma^2)
\end{aligned}
$$

where $\mu(s_{i})$ is expressed as a function of a structured additive predictor $\eta_{i}$, such that $g(\mu(s_{i}))=\eta_{i}$. For sack of simplicity, let us assume for the moment that the link function is a simple identity function such that $g(\mu(s_{i}))=\mu(s_{i})$. $\mu(s_{i})$ can hence be expressed as:
  
$$
\begin{aligned}
\mu(s_{i})=\eta_{i}=\mathbf{X(s_{i})}\mathbf{\beta}+\upsilon_{i}+\epsilon_{i}
\end{aligned}
$$

Assuming the that $\upsilon_{i}$ is normally distributed, it makes it a Gaussian Field:

$$
\begin{aligned}
\upsilon_{i} \sim GF(\mathbf{0},\mathbf{\Sigma})
\end{aligned}
$$

where $GF$ stands for Gaussian fields. 

Assuming that the covariance is Markovian in nature, i.e. only the spatial correlation can be summarized by the spatial correlation between direct neighbours, then the $\upsilon_{i}$ is distributed acording to a Gaussian Markov Random Field (GMRF):

$$
\begin{aligned}
\upsilon_{i} \sim GRMF(\mathbf{0},\mathbf{\Sigma})
\end{aligned}
$$

As $\mathbf{\Sigma}$ can be large (a matrix with more than 2 millions element in case of the *segmentos*), a determininistic structure is imposed on it in order to speed up the estimation, 

When working with point referenced data, the variance-covariance $\mathbf{\Sigma}$ is expressed in terms of the Matern correlation function :

$$
\begin{aligned}
\mathbf{\Sigma}=\sigma_{\upsilon}^2 cor_{Matern}(\upsilon(s_{i}),\upsilon(s_{j}))
\end{aligned}
$$

The advantage of the Matern correlation function is that only a few parameters need to be estimated.

In order to further simplifiy the computation, the Matern correaltion can be re-expressed with a Stochastic Partial Differencial Equation (SPDE). Once the SPDE equation solved, one can use the SPDE parameters to solve the Matern correlation parameters. 

In order to solve the SPDE, a spatial mesh is created on the point reference data. For each node of the mesh, we get a  value $w_{k}$ via the finite element approach. The $w_{k}$ also form a GMRF. Once we have the $w_{k}$, we can calculate the $\upsilon_{i}$ as a weighted sum of $w_{k}$ time $a_{i,k}$s, where $a_{i,k}$ are the distance of $s{i}$ to node $k$. This allows us to obtain the posterior distribution of the $\upsilon_{i}$s.

